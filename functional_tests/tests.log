2022-04-02T08:19:31.239+0100 [ERROR] 2022-04-02T08:19:31.239+0100 [DEBUG] Generating TLS Certificates for Ingress: path=/home/nicj/.shipyard/certs
2022-04-02T08:19:33.361+0100 [ERROR] 2022-04-02T08:19:33.361+0100 [DEBUG] Starting Ingress
2022-04-02T08:19:33.362+0100 [ERROR] Running configuration from:  ./shipyard/kubernetes

2022-04-02T08:19:33.362+0100 [DEBUG] Statefile does not exist
2022-04-02T08:19:36.393+0100 [ERROR] 2022-04-02T08:19:36.393+0100 [INFO]  Creating resources from configuration: path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes
2022-04-02T08:19:36.393+0100 [DEBUG] Statefile does not exist
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=TEMPO_HTTP_ADDR
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=GRAFANA_USER
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=GRAFANA_PASSWORD
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=PROMETHEUS_HTTP_ADDR
2022-04-02T08:19:39.620+0100 [INFO]  Generating template: ref=consul_namespace output=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:19:39.620+0100 [DEBUG] Template content: ref=consul_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
  
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=CONSUL_CAKEY
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=GRAFANA_HTTP_ADDR
2022-04-02T08:19:39.620+0100 [INFO]  Generating template: ref=controller_values output=/home/nicj/.shipyard/data/kube_setup/helm-values.yaml
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [DEBUG] Template content: ref=controller_values
  source=
  | controller:
  |   enabled: "#{{ .Vars.controller_enabled }}"
  |   container_config:
  |     image:
  |       repository: "#{{ .Vars.controller_repo }}"
  |       tag: "#{{ .Vars.controller_version }}"
  | autoencrypt:
  |   enabled: #{{ .Vars.tls_enabled }}
  | acls:
  |   enabled: #{{ .Vars.acls_enabled }}
  | #{{- if eq .Vars.controller_enabled false }}
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | #{{ end }}
  
2022-04-02T08:19:39.620+0100 [INFO]  Generating template: ref=certs_script output=/home/nicj/.shipyard/data/kube_setup/fetch_certs.sh
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=CONSUL_CACERT
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=TLS_CERT
2022-04-02T08:19:39.620+0100 [INFO]  Generating template: ref=consul_values output=/home/nicj/.shipyard/data/consul_kubernetes/consul_values.yaml
2022-04-02T08:19:39.620+0100 [DEBUG] Template content: ref=consul_values
  source=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: #{{ .Vars.datacenter }}
  | 
  |   acls:
  |     manageSystemACLs: #{{ .Vars.acl_enabled }}
  |   tls:
  |     enabled: #{{ .Vars.tls_enabled }}
  |     enableAutoEncrypt: #{{ .Vars.tls_enabled }}
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: #{{ .Vars.federation_enabled }}
  |     createFederationSecret: #{{ .Vars.create_federation_secret }}
  | 
  |   image: #{{ .Vars.consul_image }}
  |   
  |   imageK8S: #{{ .Vars.consul_k8s_image }}
  |   
  |   imageEnvoy: #{{ .Vars.consul_envoy_image }}
  | 
  |   metrics:
  |     enabled: #{{ .Vars.metrics_enabled }}
  |     enableAgentMetrics: #{{ .Vars.metrics_enabled }}
  |     enableGatewayMetrics: #{{ .Vars.metrics_enabled }}
  |   
  |   logLevel: #{{ if eq .Vars.debug true }}"debug"#{{ else }}"info"#{{ end }}
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.#{{ .Vars.monitoring_namespace }}.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: #{{ if eq .Vars.debug true }}"--log-level debug"#{{ else }}null#{{ end }}
  | 
  |   transparentProxy:
  |     defaultEnabled: #{{ .Vars.transparent_proxy_enabled }}
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: #{{ .Vars.ingress_gateway_enabled }}
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       #{{ range .Vars.ingress_gateway_ports }}
  |         - port: #{{ . }}
  |           nodePort: null
  |       #{{ end }}
  | 
  | 
  | meshGateway:
  |   enabled: #{{ .Vars.mesh_gateway_enabled }}
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: #{{ .Vars.mesh_gateway_address }}
  |     port: 30443
  | 
  |   service:
  |     enabled: #{{ .Vars.mesh_gateway_enabled }}
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_ADDR
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=TLS_KEY
2022-04-02T08:19:39.620+0100 [INFO]  Generating template: ref=consul_proxy_defaults output=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:19:39.620+0100 [DEBUG] Template content: ref=consul_proxy_defaults
  source=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.#{{ .Vars.monitoring_namespace}}.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [DEBUG] Template output: ref=controller_values
  destination=
  | controller:
  |   enabled: "false"
  |   container_config:
  |     image:
  |       repository: "nicholasjackson/consul-release-controller"
  |       tag: ""
  | autoencrypt:
  |   enabled: true
  | acls:
  |   enabled: true
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | 
  
2022-04-02T08:19:39.620+0100 [DEBUG] Template content: ref=certs_script
  source=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [DEBUG] Template output: ref=certs_script
  destination=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
  
2022-04-02T08:19:39.620+0100 [DEBUG] Template output: ref=consul_proxy_defaults
  destination=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.monitoring.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
  
2022-04-02T08:19:39.620+0100 [DEBUG] Template output: ref=consul_values
  destination=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: dc1
  | 
  |   acls:
  |     manageSystemACLs: true
  |   tls:
  |     enabled: true
  |     enableAutoEncrypt: true
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: false
  |     createFederationSecret: false
  | 
  |   image: hashicorp/consul:1.11.3
  |   
  |   imageK8S: hashicorp/consul-k8s-control-plane:0.40.0
  |   
  |   imageEnvoy: envoyproxy/envoy:v1.20.1
  | 
  |   metrics:
  |     enabled: true
  |     enableAgentMetrics: true
  |     enableGatewayMetrics: true
  |   
  |   logLevel: "info"
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: null
  | 
  |   transparentProxy:
  |     defaultEnabled: false
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: true
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       
  |         - port: 18080
  |           nodePort: null
  |       
  |         - port: 18443
  |           nodePort: null
  |       
  | 
  | 
  | meshGateway:
  |   enabled: false
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: dc1.k8s-cluster.shipyard.run
  |     port: 30443
  | 
  |   service:
  |     enabled: false
  |     type: NodePort
  |     nodePort: 30443
  
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=UPSTREAMS
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [INFO]  Creating Network: ref=dc1
2022-04-02T08:19:39.620+0100 [ERROR] 2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_TOKEN_FILE
2022-04-02T08:19:39.620+0100 [INFO]  Creating Output: ref=KUBECONFIG
2022-04-02T08:19:39.620+0100 [DEBUG] Template output: ref=consul_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:19:39.621+0100 [ERROR] 2022-04-02T08:19:39.621+0100 [DEBUG] Attempting to create using bridge plugin: ref=dc1
2022-04-02T08:19:39.645+0100 [ERROR] 2022-04-02T08:19:39.645+0100 [INFO]  Creating ImageCache: ref=docker-cache
2022-04-02T08:19:39.648+0100 [ERROR] 2022-04-02T08:19:39.648+0100 [DEBUG] Connecting cache to network: name=network.dc1
2022-04-02T08:19:39.649+0100 [ERROR] 2022-04-02T08:19:39.649+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:19:39.668+0100 [ERROR] 2022-04-02T08:19:39.668+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:19:39.668+0100 [DEBUG] Creating Docker Container: ref=68585690-import
2022-04-02T08:19:42.346+0100 [ERROR] 2022-04-02T08:19:42.346+0100 [DEBUG] Forcefully remove: container=23d98bbe600018033d9fcf4d629a556bc8a786880feb4f1e3238952b36a59898
2022-04-02T08:19:42.757+0100 [ERROR] 2022-04-02T08:19:42.757+0100 [DEBUG] Image exists in local cache: image=shipyardrun/docker-registry-proxy:0.6.3
2022-04-02T08:19:42.757+0100 [DEBUG] Creating Docker Container: ref=docker-cache
2022-04-02T08:19:42.815+0100 [ERROR] 2022-04-02T08:19:42.814+0100 [DEBUG] Remove container from default networks: ref=docker-cache
2022-04-02T08:19:42.817+0100 [ERROR] 2022-04-02T08:19:42.817+0100 [DEBUG] Attaching container to network: ref=441440dce441a33d01b9b9e0263bcbb2cb9aec7740227a007dcd4dbceb001acf network=dc1
2022-04-02T08:19:42.823+0100 [ERROR] 2022-04-02T08:19:42.823+0100 [DEBUG] Disconnectng network: name=bridge ref=docker-cache
2022-04-02T08:19:43.379+0100 [ERROR] 2022-04-02T08:19:43.379+0100 [INFO]  dc1: Creating Cluster: ref=dc1
2022-04-02T08:19:43.402+0100 [ERROR] 2022-04-02T08:19:43.402+0100 [DEBUG] Image exists in local cache: image=shipyardrun/k3s:v1.22.4
2022-04-02T08:19:43.404+0100 [ERROR] 2022-04-02T08:19:43.403+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:19:43.404+0100 [ERROR] 2022-04-02T08:19:43.404+0100 [DEBUG] Creating Docker Container: ref=server.dc1
2022-04-02T08:19:43.470+0100 [ERROR] 2022-04-02T08:19:43.470+0100 [DEBUG] Remove container from default networks: ref=server.dc1
2022-04-02T08:19:43.473+0100 [ERROR] 2022-04-02T08:19:43.473+0100 [DEBUG] Attaching container to network: ref=3b13fb6fa8a9e03ca6efa9c47401857448a1eaf4dd000fc6c519e1b0192bd782 network=dc1
2022-04-02T08:19:43.479+0100 [ERROR] 2022-04-02T08:19:43.479+0100 [DEBUG] Disconnectng network: name=bridge ref=server.dc1
2022-04-02T08:19:46.103+0100 [ERROR] 2022-04-02T08:19:46.103+0100 [DEBUG] Copying file from: id=3b13fb6fa8a9e03ca6efa9c47401857448a1eaf4dd000fc6c519e1b0192bd782 src=/output/kubeconfig.yaml dst=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:19:46.143+0100 [ERROR] 2022-04-02T08:19:46.143+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:48.154+0100 [ERROR] 2022-04-02T08:19:48.154+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:50.157+0100 [ERROR] 2022-04-02T08:19:50.157+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:51.394+0100 [ERROR] 2022-04-02T08:19:51.394+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 15.001073]
2022-04-02T08:19:52.160+0100 [ERROR] 2022-04-02T08:19:52.160+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:54.165+0100 [ERROR] 2022-04-02T08:19:54.164+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:56.168+0100 [ERROR] 2022-04-02T08:19:56.168+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:19:58.172+0100 [ERROR] 2022-04-02T08:19:58.172+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:20:00.175+0100 [ERROR] 2022-04-02T08:20:00.175+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:20:02.179+0100 [ERROR] 2022-04-02T08:20:02.179+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-w5qs8 namespace=kube-system status=Pending
2022-04-02T08:20:04.184+0100 [ERROR] 2022-04-02T08:20:04.184+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-w5qs8 namespace=kube-system status=Pending
2022-04-02T08:20:06.187+0100 [ERROR] 2022-04-02T08:20:06.187+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:20:06.187+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:20:06.393+0100 [ERROR] 2022-04-02T08:20:06.393+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 30.000103]
2022-04-02T08:20:08.192+0100 [ERROR] 2022-04-02T08:20:08.192+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:20:08.192+0100 [DEBUG] Writing docker images to volume: images=[] volume=images.volume.shipyard.run
2022-04-02T08:20:08.210+0100 [ERROR] 2022-04-02T08:20:08.209+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:20:08.209+0100 [DEBUG] Creating Docker Container: ref=09975013-import
2022-04-02T08:20:10.738+0100 [ERROR] 2022-04-02T08:20:10.738+0100 [DEBUG] Forcefully remove: container=7988416f996af55736911ad527c3a17e68dfbe3e02fc1d95651689e172d80f65
2022-04-02T08:20:11.134+0100 [ERROR] 2022-04-02T08:20:11.134+0100 [DEBUG] dc1: Deploying connector
2022-04-02T08:20:13.020+0100 [ERROR] 2022-04-02T08:20:13.020+0100 [DEBUG] dc1: Writing namespace config: file=/tmp/2967719745/namespace.yaml
2022-04-02T08:20:13.020+0100 [DEBUG] dc1: Writing secret config: file=/tmp/2967719745/secret.yaml
2022-04-02T08:20:13.020+0100 [ERROR] 2022-04-02T08:20:13.020+0100 [DEBUG] dc1: Writing RBAC config: file=/tmp/2967719745/rbac.yaml
2022-04-02T08:20:13.020+0100 [ERROR] 2022-04-02T08:20:13.020+0100 [DEBUG] dc1: Writing deployment config: file=/tmp/2967719745/deployment.yaml
2022-04-02T08:20:13.020+0100 [ERROR] 2022-04-02T08:20:13.020+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2967719745/namespace.yaml
2022-04-02T08:20:13.569+0100 [ERROR] 2022-04-02T08:20:13.569+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2967719745/secret.yaml
2022-04-02T08:20:13.576+0100 [ERROR] 2022-04-02T08:20:13.576+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2967719745/rbac.yaml
2022-04-02T08:20:13.585+0100 [ERROR] 2022-04-02T08:20:13.585+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2967719745/deployment.yaml
2022-04-02T08:20:13.603+0100 [ERROR] 2022-04-02T08:20:13.603+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=controller-webhook
2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=consul
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=upstreams-proxy
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [DEBUG] Calling connector to expose local service: name=controller-webhook remote_port=19443 connector_addr=127.0.0.1:31205 local_addr=localhost:19443
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Applying Kubernetes configuration: ref=cert-manager-controller config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml"]
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Applying Kubernetes configuration: ref=consul_namespace config=["/home/nicj/.shipyard/data/consul/namespace.yaml"]
2022-04-02T08:20:15.607+0100 [DEBUG] Calling connector to expose remote service: name=upstreams-proxy local_port=28080 connector_addr=127.0.0.1:31205 local_addr=consul-release-controller.default.svc:8080
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=consul-rpc
2022-04-02T08:20:15.607+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=consul-lan-serf
2022-04-02T08:20:15.607+0100 [DEBUG] Calling connector to expose remote service: name=consul-rpc local_port=8300 connector_addr=127.0.0.1:31205 local_addr=consul-server.consul.svc:8300
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [DEBUG] Calling connector to expose remote service: name=consul-lan-serf local_port=8301 connector_addr=127.0.0.1:31205 local_addr=consul-server.consul.svc:8301
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-1
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.608+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-1 local_port=18080 connector_addr=127.0.0.1:31205 local_addr=consul-ingress-gateway.consul.svc:18080
2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=web
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.608+0100 [DEBUG] Calling connector to expose remote service: name=web local_port=9092 connector_addr=127.0.0.1:31205 local_addr=web.default.svc:9090
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [DEBUG] Calling connector to expose remote service: name=consul local_port=8501 connector_addr=127.0.0.1:31205 local_addr=consul-server.consul.svc:8501
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.607+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-2
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.608+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-2 local_port=18443 connector_addr=127.0.0.1:31205 local_addr=consul-ingress-gateway.consul.svc:18443
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.608+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml
2022-04-02T08:20:15.608+0100 [ERROR] 2022-04-02T08:20:15.608+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:20:15.629+0100 [ERROR] 2022-04-02T08:20:15.629+0100 [DEBUG] Successfully exposed service: id=f2215575-a230-405b-b077-9272a0e271e3
2022-04-02T08:20:15.629+0100 [DEBUG] Successfully exposed service: id=6be15526-d8ae-478f-a485-4a604913c892
2022-04-02T08:20:15.629+0100 [ERROR] 2022-04-02T08:20:15.629+0100 [DEBUG] Successfully exposed service: id=11e05ab2-2576-4335-b924-229fe61c0af6
2022-04-02T08:20:15.635+0100 [ERROR] 2022-04-02T08:20:15.635+0100 [DEBUG] Successfully exposed service: id=1ff3d65b-a305-4803-a05c-42d9bfc34537
2022-04-02T08:20:15.635+0100 [ERROR] 2022-04-02T08:20:15.635+0100 [DEBUG] Successfully exposed service: id=bbea0417-1f3c-43ed-93f8-222423849480
2022-04-02T08:20:15.635+0100 [ERROR] 2022-04-02T08:20:15.635+0100 [DEBUG] Successfully exposed service: id=2026a59d-b584-4883-9819-cb89d28defb6
2022-04-02T08:20:15.636+0100 [ERROR] 2022-04-02T08:20:15.636+0100 [DEBUG] Successfully exposed service: id=e013a702-b5b7-4abe-a771-b73a1c958af4
2022-04-02T08:20:15.637+0100 [ERROR] 2022-04-02T08:20:15.637+0100 [DEBUG] Successfully exposed service: id=2fd81567-fa1a-4702-93cb-b623ccb40b58
2022-04-02T08:20:15.680+0100 [ERROR] 2022-04-02T08:20:15.680+0100 [INFO]  Creating Helm chart: ref=consul
2022-04-02T08:20:15.680+0100 [DEBUG] Updating Helm chart repository: name=hashicorp url=https://helm.releases.hashicorp.com
2022-04-02T08:20:15.770+0100 [ERROR] 2022-04-02T08:20:15.770+0100 [DEBUG] Using Kubernetes config: ref=consul path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:20:15.770+0100 [ERROR] 2022-04-02T08:20:15.770+0100 [DEBUG] Creating chart from config: ref=consul chart=hashicorp/consul
2022-04-02T08:20:15.863+0100 [ERROR] 2022-04-02T08:20:15.863+0100 [DEBUG] Loading chart: ref=consul path=/home/nicj/.shipyard/helm_charts/cache/consul-0.40.0.tgz
2022-04-02T08:20:15.870+0100 [ERROR] 2022-04-02T08:20:15.870+0100 [DEBUG] Using Values: ref=consul
  values=
  | map[connectInject:map[centralConfig:map[enabled:true] default:false enabled:true envoyExtraArgs:<nil> failurePolicy:Ignore replicas:1 transparentProxy:map[defaultEnabled:false]] controller:map[enabled:true] global:map[acls:map[manageSystemACLs:true] datacenter:dc1 federation:map[createFederationSecret:false enabled:false] image:hashicorp/consul:1.11.3 imageEnvoy:envoyproxy/envoy:v1.20.1 imageK8S:hashicorp/consul-k8s-control-plane:0.40.0 logLevel:info metrics:map[enableAgentMetrics:true enableGatewayMetrics:true enabled:true] name:consul tls:map[enableAutoEncrypt:true enabled:true httpsOnly:false]] ingressGateways:map[defaults:map[replicas:1 service:map[ports:[map[nodePort:<nil> port:18080] map[nodePort:<nil> port:18443]]]] enabled:true] meshGateway:map[enabled:false replicas:1 service:map[enabled:false nodePort:30443 type:NodePort] wanAddress:map[port:30443 source:Static static:dc1.k8s-cluster.shipyard.run]] server:map[bootstrapExpect:1 extraConfig:{
  |   "ui_config": {
  |     "enabled": true,
  |     "metrics_provider": "prometheus",
  |     "metrics_proxy": {
  |       "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |     }
  |   }
  | }
  |  replicas:1 storage:128Mi] ui:map[enabled:true]]
  
2022-04-02T08:20:15.870+0100 [DEBUG] Validate chart: ref=consul
2022-04-02T08:20:15.870+0100 [DEBUG] Run chart: ref=consul
2022-04-02T08:20:16.286+0100 [ERROR] 2022-04-02T08:20:16.285+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:20:18.265+0100 [ERROR] 2022-04-02T08:20:18.265+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" ServiceAccount"
2022-04-02T08:20:18.269+0100 [ERROR] 2022-04-02T08:20:18.268+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="serviceaccounts \"consul-tls-init\" not found"
2022-04-02T08:20:18.291+0100 [ERROR] 2022-04-02T08:20:18.291+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager status=Pending
2022-04-02T08:20:18.323+0100 [ERROR] 2022-04-02T08:20:18.323+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:20:18.329+0100 [ERROR] 2022-04-02T08:20:18.329+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Role"
2022-04-02T08:20:18.332+0100 [ERROR] 2022-04-02T08:20:18.332+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="roles.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:20:18.386+0100 [ERROR] 2022-04-02T08:20:18.386+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:20:18.391+0100 [ERROR] 2022-04-02T08:20:18.391+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" RoleBinding"
2022-04-02T08:20:18.394+0100 [ERROR] 2022-04-02T08:20:18.393+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="rolebindings.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:20:18.452+0100 [ERROR] 2022-04-02T08:20:18.452+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:20:18.457+0100 [ERROR] 2022-04-02T08:20:18.457+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:20:18.460+0100 [ERROR] 2022-04-02T08:20:18.460+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="jobs.batch \"consul-tls-init\" not found"
2022-04-02T08:20:18.515+0100 [ERROR] 2022-04-02T08:20:18.515+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:20:18.523+0100 [ERROR] 2022-04-02T08:20:18.523+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-tls-init with timeout of 0s"
2022-04-02T08:20:18.528+0100 [ERROR] 2022-04-02T08:20:18.528+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: ADDED"
2022-04-02T08:20:18.528+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:18.541+0100 [ERROR] 2022-04-02T08:20:18.541+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:20:18.541+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:20.295+0100 [ERROR] 2022-04-02T08:20:20.295+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager type=Ready value=False
2022-04-02T08:20:20.602+0100 [ERROR] 2022-04-02T08:20:20.602+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:20:20.602+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:20.610+0100 [ERROR] 2022-04-02T08:20:20.609+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:20:20.611+0100 [ERROR] 2022-04-02T08:20:20.611+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:20:20.616+0100 [ERROR] 2022-04-02T08:20:20.616+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 58 resource(s)"
2022-04-02T08:20:21.018+0100 [ERROR] 2022-04-02T08:20:21.018+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:20:21.023+0100 [ERROR] 2022-04-02T08:20:21.022+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-server-acl-init-cleanup with timeout of 0s"
2022-04-02T08:20:21.025+0100 [ERROR] 2022-04-02T08:20:21.025+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: ADDED"
2022-04-02T08:20:21.025+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:21.032+0100 [ERROR] 2022-04-02T08:20:21.032+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:20:21.032+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:21.394+0100 [ERROR] 2022-04-02T08:20:21.394+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 45.000756]
2022-04-02T08:20:22.300+0100 [ERROR] 2022-04-02T08:20:22.300+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager type=Ready value=False
2022-04-02T08:20:24.306+0100 [ERROR] 2022-04-02T08:20:24.306+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager type=Ready value=False
2022-04-02T08:20:26.313+0100 [ERROR] 2022-04-02T08:20:26.313+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager type=Ready value=False
2022-04-02T08:20:28.319+0100 [ERROR] 2022-04-02T08:20:28.319+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-vs562 namespace=cert-manager type=Ready value=False
2022-04-02T08:20:30.325+0100 [ERROR] 2022-04-02T08:20:30.325+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:20:36.393+0100 [ERROR] 2022-04-02T08:20:36.393+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 60.000410]
2022-04-02T08:20:41.666+0100 [ERROR] 2022-04-02T08:20:41.666+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:20:41.666+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:20:41.673+0100 [ERROR] 2022-04-02T08:20:41.673+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:20:41.675+0100 [ERROR] 2022-04-02T08:20:41.675+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-server-acl-init-cleanup\" Job"
2022-04-02T08:20:41.736+0100 [ERROR] 2022-04-02T08:20:41.736+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:20:43.741+0100 [ERROR] 2022-04-02T08:20:43.741+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-9d879 namespace=consul status=Pending
2022-04-02T08:20:45.747+0100 [ERROR] 2022-04-02T08:20:45.747+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-9d879 namespace=consul type=Ready value=False
2022-04-02T08:20:47.752+0100 [ERROR] 2022-04-02T08:20:47.752+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:20:47.752+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:20:49.758+0100 [ERROR] 2022-04-02T08:20:49.758+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:20:49.758+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:20:51.394+0100 [ERROR] 2022-04-02T08:20:51.394+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 75.000667]
2022-04-02T08:20:51.762+0100 [ERROR] 2022-04-02T08:20:51.762+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:20:51.762+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:20:53.767+0100 [ERROR] 2022-04-02T08:20:53.767+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.767+0100 [INFO]  Generating template: ref=monitoring_namespace output=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:20:53.768+0100 [DEBUG] Template content: ref=monitoring_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Applying Kubernetes configuration: ref=consul_defaults config=["/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml"]
2022-04-02T08:20:53.768+0100 [INFO]  Generating template: ref=grafana_secret_template output=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:20:53.768+0100 [DEBUG] Template content: ref=grafana_secret_template
  source=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Template output: ref=monitoring_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Create Ingress: ref=zipkin
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Create Ingress: ref=tempo
2022-04-02T08:20:53.768+0100 [INFO]  Create Ingress: ref=grafana
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Generating template: ref=fetch_consul_resources output=/home/nicj/.shipyard/data/consul_kubernetes/fetch.sh
2022-04-02T08:20:53.768+0100 [DEBUG] Template content: ref=fetch_consul_resources
  source=
  |   #!/bin/sh -e
  | 
  |   echo "Port #{{ .Vars.port }}"
  |   echo "Fetching resources from running cluster, acls_enabled: #{{ .Vars.acl_enabled }}, tls_enabled #{{ .Vars.tls_enabled }}"
  | 
  |   #{{ if eq .Vars.acl_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   #{{end}}
  |   
  |   #{{ if eq .Vars.tls_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |   #{{end}}
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Creating Helm chart: ref=prometheus
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Updating Helm chart repository: name=prometheus url=https://prometheus-community.github.io/helm-charts
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Create Ingress: ref=prometheus
2022-04-02T08:20:53.768+0100 [DEBUG] Template output: ref=fetch_consul_resources
  destination=
  |   #!/bin/sh -e
  | 
  |   echo "Port 8501"
  |   echo "Fetching resources from running cluster, acls_enabled: true, tls_enabled true"
  | 
  |   
  |   kubectl get secret -n consul -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   
  |   
  |   
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Calling connector to expose remote service: name=zipkin local_port=9411 connector_addr=127.0.0.1:31205 local_addr=tempo.monitoring.svc:9411
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Calling connector to expose remote service: name=grafana local_port=8080 connector_addr=127.0.0.1:31205 local_addr=grafana.monitoring.svc:80
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Generating template: ref=prometheus_operator_template output=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Template content: ref=prometheus_operator_template
  source=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
  
2022-04-02T08:20:53.768+0100 [DEBUG] Calling connector to expose remote service: name=tempo local_port=3100 connector_addr=127.0.0.1:31205 local_addr=tempo.default.svc:3100
2022-04-02T08:20:53.768+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Template output: ref=prometheus_operator_template
  destination=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:20:53.769+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Template output: ref=grafana_secret_template
  destination=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: monitoring
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
  
2022-04-02T08:20:53.768+0100 [INFO]  Applying Kubernetes configuration: ref=monitoring_namespace config=["/home/nicj/.shipyard/data/monitoring/namespace.yaml"]
2022-04-02T08:20:53.769+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Calling connector to expose remote service: name=prometheus local_port=9090 connector_addr=127.0.0.1:31205 local_addr=prometheus-operated.monitoring.svc:9090
2022-04-02T08:20:53.769+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [INFO]  Remote executing command: ref=fetch_consul_resources command=sh args=["/data/fetch.sh"] image="&{shipyardrun/tools:v0.5.0  }"
2022-04-02T08:20:53.769+0100 [ERROR] 2022-04-02T08:20:53.768+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:20:53.769+0100 [ERROR] 2022-04-02T08:20:53.769+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:20:53.786+0100 [ERROR] 2022-04-02T08:20:53.786+0100 [DEBUG] Successfully exposed service: id=5a3ec616-10c1-419d-b7f7-5a6820708927
2022-04-02T08:20:53.788+0100 [ERROR] 2022-04-02T08:20:53.788+0100 [DEBUG] Successfully exposed service: id=20f268ab-2d34-45d2-affe-49cdf9ba3ad8
2022-04-02T08:20:53.788+0100 [ERROR] 2022-04-02T08:20:53.788+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.5.0
2022-04-02T08:20:53.788+0100 [DEBUG] Creating Docker Container: ref=fetch_consul_resources.remote_exec
2022-04-02T08:20:53.793+0100 [ERROR] 2022-04-02T08:20:53.793+0100 [DEBUG] Successfully exposed service: id=f9e4c42a-4e65-43d1-b630-167806567788
2022-04-02T08:20:53.794+0100 [ERROR] 2022-04-02T08:20:53.794+0100 [DEBUG] Successfully exposed service: id=8c0df1ad-3ad1-4e26-9975-f5b845de32d3
2022-04-02T08:20:53.840+0100 [ERROR] 2022-04-02T08:20:53.840+0100 [DEBUG] Remove container from default networks: ref=fetch_consul_resources.remote_exec
2022-04-02T08:20:53.844+0100 [ERROR] 2022-04-02T08:20:53.844+0100 [DEBUG] Attaching container to network: ref=418bcfd18b7907bb12aeab4af69a4f9b00cafb7f84325b00d4f554fa6119505f network=dc1
2022-04-02T08:20:53.851+0100 [ERROR] 2022-04-02T08:20:53.851+0100 [DEBUG] Disconnectng network: name=bridge ref=fetch_consul_resources.remote_exec
2022-04-02T08:20:54.331+0100 [ERROR] 2022-04-02T08:20:54.331+0100 [DEBUG] Using Kubernetes config: ref=prometheus path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:20:54.332+0100 [ERROR] 2022-04-02T08:20:54.332+0100 [DEBUG] Creating chart from config: ref=prometheus chart=prometheus/kube-prometheus-stack
2022-04-02T08:20:54.388+0100 [ERROR] 2022-04-02T08:20:54.388+0100 [INFO]  Applying Kubernetes configuration: ref=grafana_secret config=["/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml"]
2022-04-02T08:20:54.389+0100 [ERROR] 2022-04-02T08:20:54.389+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:20:54.708+0100 [ERROR] 2022-04-02T08:20:54.708+0100 [DEBUG] Port 8501
Fetching resources from running cluster, acls_enabled: true, tls_enabled true
2022-04-02T08:20:55.084+0100 [ERROR] 2022-04-02T08:20:55.084+0100 [DEBUG] Forcefully remove: container=418bcfd18b7907bb12aeab4af69a4f9b00cafb7f84325b00d4f554fa6119505f
2022-04-02T08:20:55.137+0100 [ERROR] 2022-04-02T08:20:55.137+0100 [DEBUG] Loading chart: ref=prometheus path=/home/nicj/.shipyard/helm_charts/cache/kube-prometheus-stack-32.0.0.tgz
2022-04-02T08:20:55.151+0100 [ERROR] 2022-04-02T08:20:55.151+0100 [DEBUG] Using Values: ref=prometheus values="map[alertmanager:map[enabled:false] defaultRules:map[create:false] grafana:map[enabled:false] serviceMonitor:map[enabled:false]]"
2022-04-02T08:20:55.151+0100 [DEBUG] Validate chart: ref=prometheus
2022-04-02T08:20:55.151+0100 [DEBUG] Run chart: ref=prometheus
2022-04-02T08:20:55.167+0100 [ERROR] 2022-04-02T08:20:55.167+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.215+0100 [ERROR] 2022-04-02T08:20:55.215+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.260+0100 [ERROR] 2022-04-02T08:20:55.259+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.275+0100 [ERROR] 2022-04-02T08:20:55.275+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.312+0100 [ERROR] 2022-04-02T08:20:55.312+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.403+0100 [ERROR] 2022-04-02T08:20:55.402+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.412+0100 [ERROR] 2022-04-02T08:20:55.412+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.445+0100 [ERROR] 2022-04-02T08:20:55.444+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:55.504+0100 [ERROR] 2022-04-02T08:20:55.504+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Clearing discovery cache"
2022-04-02T08:20:55.504+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="beginning wait for 8 resources with timeout of 1m0s"
2022-04-02T08:20:58.889+0100 [ERROR] 2022-04-02T08:20:58.889+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:59.178+0100 [ERROR] 2022-04-02T08:20:59.178+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:20:59.181+0100 [ERROR] 2022-04-02T08:20:59.181+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:20:59.461+0100 [ERROR] 2022-04-02T08:20:59.461+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:59.467+0100 [ERROR] 2022-04-02T08:20:59.466+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:20:59.469+0100 [ERROR] 2022-04-02T08:20:59.469+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:20:59.746+0100 [ERROR] 2022-04-02T08:20:59.746+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:20:59.751+0100 [ERROR] 2022-04-02T08:20:59.751+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:20:59.753+0100 [ERROR] 2022-04-02T08:20:59.753+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:00.056+0100 [ERROR] 2022-04-02T08:21:00.056+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:00.061+0100 [ERROR] 2022-04-02T08:21:00.061+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:21:00.064+0100 [ERROR] 2022-04-02T08:21:00.064+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:00.330+0100 [ERROR] 2022-04-02T08:21:00.330+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:00.335+0100 [ERROR] 2022-04-02T08:21:00.335+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:21:00.338+0100 [ERROR] 2022-04-02T08:21:00.338+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:00.610+0100 [ERROR] 2022-04-02T08:21:00.610+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:00.615+0100 [ERROR] 2022-04-02T08:21:00.615+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:21:00.618+0100 [ERROR] 2022-04-02T08:21:00.617+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-create\" not found"
2022-04-02T08:21:00.906+0100 [ERROR] 2022-04-02T08:21:00.906+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:00.911+0100 [ERROR] 2022-04-02T08:21:00.911+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-create with timeout of 0s"
2022-04-02T08:21:00.913+0100 [ERROR] 2022-04-02T08:21:00.913+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: ADDED"
2022-04-02T08:21:00.913+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:00.933+0100 [ERROR] 2022-04-02T08:21:00.933+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:21:00.933+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:02.716+0100 [ERROR] 2022-04-02T08:21:02.716+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:21:02.716+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:02.724+0100 [ERROR] 2022-04-02T08:21:02.724+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:21:02.725+0100 [ERROR] 2022-04-02T08:21:02.725+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:21:02.731+0100 [ERROR] 2022-04-02T08:21:02.731+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:21:02.738+0100 [ERROR] 2022-04-02T08:21:02.738+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:21:02.743+0100 [ERROR] 2022-04-02T08:21:02.743+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:21:02.748+0100 [ERROR] 2022-04-02T08:21:02.748+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:21:02.753+0100 [ERROR] 2022-04-02T08:21:02.753+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:21:02.757+0100 [ERROR] 2022-04-02T08:21:02.757+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 36 resource(s)"
2022-04-02T08:21:02.946+0100 [ERROR] 2022-04-02T08:21:02.946+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:21:02.948+0100 [ERROR] 2022-04-02T08:21:02.948+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:03.230+0100 [ERROR] 2022-04-02T08:21:03.230+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:03.237+0100 [ERROR] 2022-04-02T08:21:03.236+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:21:03.239+0100 [ERROR] 2022-04-02T08:21:03.239+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:03.521+0100 [ERROR] 2022-04-02T08:21:03.520+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:03.526+0100 [ERROR] 2022-04-02T08:21:03.526+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:21:03.528+0100 [ERROR] 2022-04-02T08:21:03.528+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:03.817+0100 [ERROR] 2022-04-02T08:21:03.817+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:03.822+0100 [ERROR] 2022-04-02T08:21:03.822+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:21:03.825+0100 [ERROR] 2022-04-02T08:21:03.825+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:04.134+0100 [ERROR] 2022-04-02T08:21:04.134+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:04.140+0100 [ERROR] 2022-04-02T08:21:04.140+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:21:04.143+0100 [ERROR] 2022-04-02T08:21:04.143+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:21:04.438+0100 [ERROR] 2022-04-02T08:21:04.438+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:04.443+0100 [ERROR] 2022-04-02T08:21:04.443+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:21:04.446+0100 [ERROR] 2022-04-02T08:21:04.446+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-patch\" not found"
2022-04-02T08:21:04.739+0100 [ERROR] 2022-04-02T08:21:04.739+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:21:04.744+0100 [ERROR] 2022-04-02T08:21:04.743+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-patch with timeout of 0s"
2022-04-02T08:21:04.746+0100 [ERROR] 2022-04-02T08:21:04.746+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: ADDED"
2022-04-02T08:21:04.746+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:04.759+0100 [ERROR] 2022-04-02T08:21:04.759+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:21:04.759+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:06.393+0100 [ERROR] 2022-04-02T08:21:06.393+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 90.000279]
2022-04-02T08:21:07.738+0100 [ERROR] 2022-04-02T08:21:07.737+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:21:07.737+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:21:07.746+0100 [ERROR] 2022-04-02T08:21:07.746+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:21:07.748+0100 [ERROR] 2022-04-02T08:21:07.748+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:21:07.754+0100 [ERROR] 2022-04-02T08:21:07.753+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:21:07.761+0100 [ERROR] 2022-04-02T08:21:07.761+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:21:07.770+0100 [ERROR] 2022-04-02T08:21:07.770+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:21:07.776+0100 [ERROR] 2022-04-02T08:21:07.776+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:21:07.781+0100 [ERROR] 2022-04-02T08:21:07.781+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:21:08.251+0100 [ERROR] 2022-04-02T08:21:08.251+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:21:10.256+0100 [ERROR] 2022-04-02T08:21:10.256+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-2hndz namespace=monitoring type=Ready value=False
2022-04-02T08:21:12.262+0100 [ERROR] 2022-04-02T08:21:12.262+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-2hndz namespace=monitoring type=Ready value=False
2022-04-02T08:21:14.269+0100 [ERROR] 2022-04-02T08:21:14.269+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:21:14.270+0100 [ERROR] 2022-04-02T08:21:14.270+0100 [INFO]  Creating Helm chart: ref=loki
2022-04-02T08:21:14.270+0100 [ERROR] 2022-04-02T08:21:14.270+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:21:14.270+0100 [ERROR] 2022-04-02T08:21:14.270+0100 [INFO]  Applying Kubernetes configuration: ref=prometheus config=["/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml"]
2022-04-02T08:21:14.271+0100 [ERROR] 2022-04-02T08:21:14.271+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:21:14.596+0100 [ERROR] 2022-04-02T08:21:14.596+0100 [DEBUG] Using Kubernetes config: ref=loki path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:21:14.598+0100 [ERROR] 2022-04-02T08:21:14.597+0100 [DEBUG] Creating chart from config: ref=loki chart=grafana/loki
2022-04-02T08:21:15.079+0100 [ERROR] 2022-04-02T08:21:15.079+0100 [DEBUG] Loading chart: ref=loki path=/home/nicj/.shipyard/helm_charts/cache/loki-2.9.1.tgz
2022-04-02T08:21:15.080+0100 [ERROR] 2022-04-02T08:21:15.080+0100 [DEBUG] Using Values: ref=loki values=map[]
2022-04-02T08:21:15.080+0100 [DEBUG] Validate chart: ref=loki
2022-04-02T08:21:15.080+0100 [DEBUG] Run chart: ref=loki
2022-04-02T08:21:15.309+0100 [ERROR] W0402 08:21:15.309301    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:15.325+0100 [ERROR] 2022-04-02T08:21:15.325+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 1 resource(s)"
2022-04-02T08:21:15.335+0100 [ERROR] 2022-04-02T08:21:15.335+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 8 resource(s)"
2022-04-02T08:21:15.340+0100 [ERROR] W0402 08:21:15.340198    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:15.369+0100 [ERROR] 2022-04-02T08:21:15.369+0100 [INFO]  Creating Helm chart: ref=promtail
2022-04-02T08:21:15.369+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:21:15.369+0100 [DEBUG] Using Kubernetes config: ref=promtail path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:21:15.370+0100 [ERROR] 2022-04-02T08:21:15.370+0100 [DEBUG] Creating chart from config: ref=promtail chart=grafana/promtail
2022-04-02T08:21:16.052+0100 [ERROR] 2022-04-02T08:21:16.052+0100 [DEBUG] Loading chart: ref=promtail path=/home/nicj/.shipyard/helm_charts/cache/promtail-3.11.0.tgz
2022-04-02T08:21:16.053+0100 [ERROR] 2022-04-02T08:21:16.053+0100 [DEBUG] Using Values: ref=promtail values=map[config:map[lokiAddress:http://loki:3100/loki/api/v1/push]]
2022-04-02T08:21:16.053+0100 [DEBUG] Validate chart: ref=promtail
2022-04-02T08:21:16.053+0100 [DEBUG] Run chart: ref=promtail
2022-04-02T08:21:16.353+0100 [ERROR] 2022-04-02T08:21:16.353+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 1 resource(s)"
2022-04-02T08:21:16.363+0100 [ERROR] 2022-04-02T08:21:16.363+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 5 resource(s)"
2022-04-02T08:21:16.390+0100 [ERROR] 2022-04-02T08:21:16.390+0100 [INFO]  Creating Helm chart: ref=tempo
2022-04-02T08:21:16.390+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:21:16.390+0100 [DEBUG] Using Kubernetes config: ref=tempo path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:21:16.391+0100 [ERROR] 2022-04-02T08:21:16.391+0100 [DEBUG] Creating chart from config: ref=tempo chart=grafana/tempo
2022-04-02T08:21:17.008+0100 [ERROR] 2022-04-02T08:21:17.008+0100 [DEBUG] Loading chart: ref=tempo path=/home/nicj/.shipyard/helm_charts/cache/tempo-0.13.1.tgz
2022-04-02T08:21:17.009+0100 [ERROR] 2022-04-02T08:21:17.009+0100 [DEBUG] Using Values: ref=tempo values="map[tempo:map[receivers:map[jaeger:map[protocols:map[grpc:map[endpoint:0.0.0.0:14250] thrift_binary:map[endpoint:0.0.0.0:6832] thrift_compact:map[endpoint:0.0.0.0:6831] thrift_http:map[endpoint:0.0.0.0:14268]]] zipkin:map[]]]]"
2022-04-02T08:21:17.009+0100 [DEBUG] Validate chart: ref=tempo
2022-04-02T08:21:17.009+0100 [DEBUG] Run chart: ref=tempo
2022-04-02T08:21:17.262+0100 [ERROR] 2022-04-02T08:21:17.262+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 1 resource(s)"
2022-04-02T08:21:17.272+0100 [ERROR] 2022-04-02T08:21:17.271+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 5 resource(s)"
2022-04-02T08:21:17.302+0100 [ERROR] 2022-04-02T08:21:17.302+0100 [INFO]  Creating Helm chart: ref=grafana
2022-04-02T08:21:17.302+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:21:17.302+0100 [DEBUG] Using Kubernetes config: ref=grafana path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:21:17.303+0100 [ERROR] 2022-04-02T08:21:17.303+0100 [DEBUG] Creating chart from config: ref=grafana chart=grafana/grafana
2022-04-02T08:21:17.883+0100 [ERROR] 2022-04-02T08:21:17.883+0100 [DEBUG] Loading chart: ref=grafana path=/home/nicj/.shipyard/helm_charts/cache/grafana-6.21.2.tgz
2022-04-02T08:21:17.885+0100 [ERROR] 2022-04-02T08:21:17.885+0100 [DEBUG] Using Values: ref=grafana values="map[admin:map[existingSecret:grafana-password] datasources:map[datasources.yaml:map[apiVersion:1 datasources:[map[isDefault:true name:Prometheus type:prometheus url:http://prometheus-kube-prometheus-prometheus:9090] map[isDefault:false jsonData:map[derivedFields:[map[datasourceUid:tempo_uid matcherRegex:trace_id=(\\w+) name:trace_id url:$${__value.raw}]] maxLines:1000] name:Loki type:loki uid:loki_uid url:http://loki:3100] map[isDefault:false name:Tempo type:tempo uid:tempo_uid url:http://tempo:3100]]]] sidecar:map[dashboards:map[enabled:true]]]"
2022-04-02T08:21:17.885+0100 [DEBUG] Validate chart: ref=grafana
2022-04-02T08:21:17.885+0100 [DEBUG] Run chart: ref=grafana
2022-04-02T08:21:18.222+0100 [ERROR] W0402 08:21:18.222552    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:18.225+0100 [ERROR] W0402 08:21:18.224951    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:18.255+0100 [ERROR] 2022-04-02T08:21:18.255+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 1 resource(s)"
2022-04-02T08:21:18.273+0100 [ERROR] 2022-04-02T08:21:18.273+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 15 resource(s)"
2022-04-02T08:21:18.277+0100 [ERROR] W0402 08:21:18.277209    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:18.277+0100 [ERROR] W0402 08:21:18.277320    1257 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:21:18.348+0100 [ERROR] 2022-04-02T08:21:18.348+0100 [INFO]  Generating template: ref=monitor_ingress_gateway output=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:21:18.348+0100 [DEBUG] Template content: ref=monitor_ingress_gateway
  source=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: #{{ .Vars.consul_namespace }}
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:21:18.348+0100 [ERROR] 2022-04-02T08:21:18.348+0100 [DEBUG] Template output: ref=monitor_ingress_gateway
  destination=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: monitoring
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: consul
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:21:18.349+0100 [ERROR] 2022-04-02T08:21:18.348+0100 [INFO]  Applying Kubernetes configuration: ref=monitor_ingress_gateway config=["/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml"]
2022-04-02T08:21:18.350+0100 [ERROR] 2022-04-02T08:21:18.349+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:21:18.442+0100 [ERROR] 2022-04-02T08:21:18.442+0100 [INFO]  Applying Kubernetes configuration: ref=application config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/../../example/kubernetes/"]
2022-04-02T08:21:18.442+0100 [INFO]  Creating Helm chart: ref=consul-release-controller
2022-04-02T08:21:18.442+0100 [DEBUG] Using Kubernetes config: ref=consul-release-controller path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:21:18.442+0100 [ERROR] 2022-04-02T08:21:18.442+0100 [INFO]  Applying Kubernetes configuration: ref=upstreams-proxy config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml"]
2022-04-02T08:21:18.443+0100 [ERROR] 2022-04-02T08:21:18.443+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/api.yaml
2022-04-02T08:21:18.443+0100 [ERROR] 2022-04-02T08:21:18.443+0100 [DEBUG] Creating chart from config: ref=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:21:18.443+0100 [DEBUG] Loading chart: ref=consul-release-controller path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:21:18.443+0100 [ERROR] 2022-04-02T08:21:18.443+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml
2022-04-02T08:21:18.444+0100 [ERROR] 2022-04-02T08:21:18.444+0100 [DEBUG] Using Values: ref=consul-release-controller values="map[acls:map[enabled:true] autoencrypt:map[enabled:true] controller:map[container_config:map[image:map[repository:nicholasjackson/consul-release-controller tag:]] enabled:false] webhook:map[namespace:shipyard service:controller-webhook]]"
2022-04-02T08:21:18.444+0100 [DEBUG] Validate chart: ref=consul-release-controller
2022-04-02T08:21:18.444+0100 [DEBUG] Run chart: ref=consul-release-controller
2022-04-02T08:21:18.539+0100 [ERROR] 2022-04-02T08:21:18.539+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/application-dashboard.yaml
2022-04-02T08:21:18.561+0100 [ERROR] 2022-04-02T08:21:18.561+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/consul-config.yaml
2022-04-02T08:21:18.649+0100 [ERROR] 2022-04-02T08:21:18.649+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest-dashboard.yaml
2022-04-02T08:21:18.703+0100 [ERROR] 2022-04-02T08:21:18.703+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest.yaml
2022-04-02T08:21:18.730+0100 [ERROR] 2022-04-02T08:21:18.730+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/metrics.yaml
2022-04-02T08:21:18.736+0100 [ERROR] 2022-04-02T08:21:18.736+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/web.yaml
2022-04-02T08:21:18.829+0100 [ERROR] 2022-04-02T08:21:18.829+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 1 resource(s)"
2022-04-02T08:21:18.840+0100 [ERROR] 2022-04-02T08:21:18.840+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 13 resource(s)"
2022-04-02T08:21:18.970+0100 [ERROR] 2022-04-02T08:21:18.970+0100 [INFO]  Remote executing command: ref=exec_standalone command=sh args=["/output/fetch_certs.sh"] image="&{shipyardrun/tools:v0.6.0  }"
2022-04-02T08:21:18.997+0100 [ERROR] 2022-04-02T08:21:18.997+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.6.0
2022-04-02T08:21:18.997+0100 [DEBUG] Creating Docker Container: ref=exec_standalone.remote_exec
2022-04-02T08:21:19.121+0100 [ERROR] 2022-04-02T08:21:19.121+0100 [DEBUG] Remove container from default networks: ref=exec_standalone.remote_exec
2022-04-02T08:21:19.125+0100 [ERROR] 2022-04-02T08:21:19.125+0100 [DEBUG] Attaching container to network: ref=45480788617bb1528fc64aa7ec16476ffce2165c7620ca4b731ef5eae054feec network=dc1
2022-04-02T08:21:19.133+0100 [ERROR] 2022-04-02T08:21:19.132+0100 [DEBUG] Disconnectng network: name=bridge ref=exec_standalone.remote_exec
2022-04-02T08:21:20.621+0100 [ERROR] 2022-04-02T08:21:20.621+0100 [DEBUG] Forcefully remove: container=45480788617bb1528fc64aa7ec16476ffce2165c7620ca4b731ef5eae054feec
2022-04-02T08:21:21.394+0100 [ERROR] 2022-04-02T08:21:21.394+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 105.001074]
2022-04-02T08:21:22.041+0100 [ERROR] 2022-04-02T08:21:22.041+0100 [DEBUG] Health check urls for browser windows: count=0
2022-04-02T08:21:22.041+0100 [DEBUG] Browser windows open

########################################################

Title Development setup
Author Nic Jackson
2022-04-02T08:21:22.041+0100 [ERROR] 
• Consul: https://localhost:8501
• Grafana: https://localhost:8080
• Application: http://localhost:18080

This blueprint defines 13 output variables.

You can set output variables as environment variables for your current terminal session using the following command:

eval $(shipyard env)

To list output variables use the command:

shipyard output
2022-04-02T08:21:22.657+0100 [INFO]  Starting controller
2022-04-02T08:21:27.227+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:21:27.230+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:27.233+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:21:27.233+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:28.233+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:28.236+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:28.236+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:29.237+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:29.240+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:29.240+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:30.241+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:30.244+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:30.244+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:31.244+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:31.247+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:31.247+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:32.248+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:32.251+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:32.251+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:33.251+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:33.254+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:33.254+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:34.255+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:34.258+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:34.258+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:35.258+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:35.261+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:35.261+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:36.262+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:36.265+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:36.265+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:37.265+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:37.269+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:37.269+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:38.269+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:38.272+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:38.272+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:39.272+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:39.276+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:39.276+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:40.276+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:40.279+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:40.279+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:41.279+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:41.283+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:41.283+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:42.283+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:42.286+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:42.286+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:43.287+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:43.290+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:43.290+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:44.290+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:44.293+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:44.293+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:45.294+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:45.297+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:45.297+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:46.298+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:46.301+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:21:46.301+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:21:47.302+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:47.306+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:21:47.306+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:21:47.317+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:21:47.320+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:21:47.320+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:22:22.386+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:22.389+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:22.392+0100 [DEBUG] statemachine: Handle event: event=event_configure state=state_start
2022-04-02T08:22:22.392+0100 [DEBUG] statemachine: Log state: event=event_configure state=state_start
2022-04-02T08:22:22.392+0100 [DEBUG] statemachine: Configure: state=state_configure
2022-04-02T08:22:22.392+0100 [DEBUG] statemachine: Log state: event=event_configure release=api state=state_configure
2022-04-02T08:22:22.392+0100 [INFO]  releaser-plugin-consul: Initializing deployment: service=api
2022-04-02T08:22:22.392+0100 [DEBUG] releaser-plugin-consul: Create service defaults: service=api
2022-04-02T08:22:23.390+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:23.393+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:24.394+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:24.397+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:25.398+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:25.400+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:25.419+0100 [DEBUG] releaser-plugin-consul: Create service resolver: service=api
2022-04-02T08:22:26.401+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:26.404+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:26.432+0100 [DEBUG] releaser-plugin-consul: Create service router: service=api
2022-04-02T08:22:27.405+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:27.408+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:27.439+0100 [DEBUG] releaser-plugin-consul: Create upstream service router: service=api
2022-04-02T08:22:28.409+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:28.411+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:28.445+0100 [DEBUG] releaser-plugin-consul: Create service intentions for the upstreams: service=api
2022-04-02T08:22:29.412+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:29.415+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:30.415+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:30.417+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:31.418+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:31.421+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:32.421+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:32.424+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:33.424+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:33.427+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:22:33.452+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-02T08:22:33.457+0100 [DEBUG] runtime-plugin-kubernetes: Cloning deployment: name=api-deployment namespace=default
2022-04-02T08:22:33.463+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment-primary namespaces=default
2022-04-02T08:22:33.467+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:33.470+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:22:33.470+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:34.428+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:34.431+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:34.431+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:34.470+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:34.473+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:34.473+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:35.431+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:35.434+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:35.434+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:35.473+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:35.476+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:35.476+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:36.435+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:36.438+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:36.438+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:36.477+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:36.480+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:36.480+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:37.438+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:37.441+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:37.441+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:37.481+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:37.484+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:37.484+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:38.441+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:38.445+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:38.445+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:38.485+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:38.488+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:38.488+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:39.445+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:39.448+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:39.448+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:39.488+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:39.491+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:39.491+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:40.448+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:40.451+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:40.451+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:40.491+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:40.494+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:40.494+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:41.452+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:41.455+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:41.455+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:41.495+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:41.498+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:41.498+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:42.456+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:42.459+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:42.459+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:42.499+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:42.502+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:42.502+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:43.459+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:43.462+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:43.462+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:43.502+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:43.505+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:43.505+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:44.463+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:44.466+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:44.466+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:44.505+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:44.508+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:44.508+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:45.467+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:45.470+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:45.470+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:45.508+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:45.511+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:45.511+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:46.470+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:46.474+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:46.474+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:46.512+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:46.515+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:46.515+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:47.474+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:47.477+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:47.477+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:47.515+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:47.518+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:47.518+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:48.477+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:48.480+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:48.480+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:48.519+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:48.522+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:48.522+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.480+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.483+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:22:49.483+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.522+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.525+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:22:49.525+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.525+0100 [DEBUG] runtime-plugin-kubernetes: Successfully cloned kubernetes deployment: name=api-deployment-primary namespace=default
2022-04-02T08:22:49.525+0100 [INFO]  runtime-plugin-kubernetes: Init primary complete: name=api-deployment namespace=default
2022-04-02T08:22:49.525+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:22:49.528+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:22:54.538+0100 [INFO]  runtime-plugin-kubernetes: Remove candidate deployment: name=api-deployment namespace=default
2022-04-02T08:22:54.547+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:22:54.547+0100 [DEBUG] kubernetes-webhook: Ignore deployment, resource was modified by the controller: name=api-deployment namespace=default labels="map[app:api_v2 consul-release-controller-version:2177]"
2022-04-02T08:22:54.551+0100 [DEBUG] statemachine: Configure completed successfully
2022-04-02T08:22:54.551+0100 [DEBUG] statemachine: Handle event: event=event_configured state=state_configure
2022-04-02T08:22:54.551+0100 [DEBUG] statemachine: Log state: event=event_configured state=state_configure
2022-04-02T08:22:54.551+0100 [DEBUG] statemachine: Log state: event=event_configured release=api state=state_idle
2022-04-02T08:22:55.537+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:22:55.554+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:22:55.554+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_idle
2022-04-02T08:22:55.554+0100 [DEBUG] statemachine: Handle event: event=event_deploy state=state_idle
2022-04-02T08:22:55.554+0100 [DEBUG] statemachine: Log state: event=event_deploy state=state_idle
2022-04-02T08:22:55.554+0100 [DEBUG] statemachine: Deploy: state=state_deploy
2022-04-02T08:22:55.554+0100 [DEBUG] statemachine: Log state: event=event_deploy release=api state=state_deploy
2022-04-02T08:22:55.557+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:22:55.560+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:22:55.560+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:22:56.560+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:22:56.563+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:56.563+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:22:57.563+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:22:57.566+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:57.566+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:22:58.567+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:22:58.570+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:58.570+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:22:59.570+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:22:59.573+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:22:59.573+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:00.555+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-02T08:23:00.558+0100 [DEBUG] runtime-plugin-kubernetes: Primary deployment already exists: name=api-deployment-primary namespace=default
2022-04-02T08:23:00.558+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:23:00.561+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:23:00.566+0100 [DEBUG] statemachine: Deploy completed, executing strategy
2022-04-02T08:23:00.566+0100 [DEBUG] statemachine: Handle event: event=event_deployed state=state_deploy
2022-04-02T08:23:00.566+0100 [DEBUG] statemachine: Log state: event=event_deployed state=state_deploy
2022-04-02T08:23:00.566+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:23:00.566+0100 [DEBUG] statemachine: Log state: event=event_deployed release=api state=state_monitor
2022-04-02T08:23:00.566+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=-1
2022-04-02T08:23:00.566+0100 [DEBUG] strategy-plugin-canary: Waiting for initial grace before starting rollout: type=canary delay=30
2022-04-02T08:23:00.573+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:00.577+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:00.577+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:01.577+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:01.579+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:01.579+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:02.580+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:02.583+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:02.583+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:03.583+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:03.586+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:03.586+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:04.586+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:04.589+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:04.589+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:05.590+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:05.593+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:05.593+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:06.594+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:06.597+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:06.597+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:07.597+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:07.600+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:07.600+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:08.600+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:08.603+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:08.603+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:09.603+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:09.606+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:23:09.606+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:10.608+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:10.617+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=1 desired_replicas=3
2022-04-02T08:23:10.617+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:23:11.617+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:11.620+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:23:11.620+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:23:11.633+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:23:11.636+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:23:11.636+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:23:11.645+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:23:11.648+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:23:11.648+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:23:11.650+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:16.651+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:21.652+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:26.653+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:30.567+0100 [DEBUG] strategy-plugin-canary: Strategy setup: type=canary traffic=10
2022-04-02T08:23:30.567+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:23:30.567+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:23:30.567+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:23:30.567+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:23:30.567+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:23:30.567+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=90 traffic_canary=10
2022-04-02T08:23:30.572+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:23:30.572+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:23:30.572+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:23:30.572+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:23:30.572+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:23:30.572+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=10
2022-04-02T08:23:31.654+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:36.655+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:41.656+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:46.657+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:51.659+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:23:56.660+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:00.574+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:24:00.574+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:24:00.578+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1648884240.574]"] value_type=model.Vector warnings=[]
2022-04-02T08:24:00.578+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-02T08:24:00.581+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1648884240.579]"] value_type=model.Vector warnings=[]
2022-04-02T08:24:00.581+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=30
2022-04-02T08:24:00.581+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:24:00.581+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:24:00.581+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:24:00.581+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:24:00.581+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:24:00.581+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=70 traffic_canary=30
2022-04-02T08:24:00.589+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:24:00.589+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:24:00.589+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:24:00.589+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:24:00.589+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:24:00.589+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=30
2022-04-02T08:24:01.660+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:06.661+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:11.662+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:16.664+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:21.665+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:26.665+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:30.590+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:24:30.590+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:24:30.593+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1648884270.591]"] value_type=model.Vector warnings=[]
2022-04-02T08:24:30.593+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-02T08:24:30.595+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1648884270.593]"] value_type=model.Vector warnings=[]
2022-04-02T08:24:30.595+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=50
2022-04-02T08:24:30.595+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:24:30.595+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:24:30.595+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:24:30.595+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:24:30.595+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:24:30.595+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=50 traffic_canary=50
2022-04-02T08:24:30.600+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:24:30.600+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:24:30.600+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:24:30.600+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:24:30.600+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:24:30.600+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=50
2022-04-02T08:24:31.666+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:36.667+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:41.668+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:46.669+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:51.669+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:24:56.670+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:00.601+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:25:00.601+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:25:00.603+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1648884300.602]"] value_type=model.Vector warnings=[]
2022-04-02T08:25:00.604+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-02T08:25:00.606+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1648884300.604]"] value_type=model.Vector warnings=[]
2022-04-02T08:25:00.606+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=70
2022-04-02T08:25:00.606+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:25:00.606+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:25:00.606+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:25:00.606+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:25:00.606+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:25:00.606+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=30 traffic_canary=70
2022-04-02T08:25:00.615+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:25:00.615+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:25:00.615+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:25:00.615+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:25:00.615+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:25:00.615+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=70
2022-04-02T08:25:01.671+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:06.673+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:11.673+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:16.675+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:21.675+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:26.676+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:30.615+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:25:30.615+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:25:30.617+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1648884330.615]"] value_type=model.Vector warnings=[]
2022-04-02T08:25:30.618+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-02T08:25:30.620+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1648884330.618]"] value_type=model.Vector warnings=[]
2022-04-02T08:25:30.620+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=90
2022-04-02T08:25:30.620+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:25:30.620+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:25:30.620+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:25:30.620+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:25:30.620+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:25:30.620+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=10 traffic_canary=90
2022-04-02T08:25:30.630+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:25:30.630+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:25:30.630+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:25:30.630+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:25:30.630+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:25:30.630+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=90
2022-04-02T08:25:31.678+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:36.679+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:41.679+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:46.680+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:51.680+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:25:56.681+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:00.631+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:26:00.631+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:26:00.633+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1648884360.631]"] value_type=model.Vector warnings=[]
2022-04-02T08:26:00.633+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-02T08:26:00.636+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1648884360.634]"] value_type=model.Vector warnings=[]
2022-04-02T08:26:00.636+0100 [DEBUG] strategy-plugin-canary: Strategy complete: type=canary traffic=110
2022-04-02T08:26:00.636+0100 [DEBUG] statemachine: Monitor checks completed, strategy complete
2022-04-02T08:26:00.636+0100 [DEBUG] statemachine: Handle event: event=event_complete state=state_monitor
2022-04-02T08:26:00.636+0100 [DEBUG] statemachine: Log state: event=event_complete state=state_monitor
2022-04-02T08:26:00.636+0100 [DEBUG] statemachine: Promote: state=state_promote
2022-04-02T08:26:00.636+0100 [DEBUG] statemachine: Log state: event=event_complete release=api state=state_promote
2022-04-02T08:26:00.636+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=0 traffic_canary=100
2022-04-02T08:26:01.682+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:05.641+0100 [INFO]  runtime-plugin-kubernetes: Promote deployment: name=api-deployment namespace=default
2022-04-02T08:26:05.641+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:05.646+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:26:05.646+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:26:05.646+0100 [DEBUG] runtime-plugin-kubernetes: Delete existing primary deployment: name=api-deployment-primary namespace=default
2022-04-02T08:26:05.650+0100 [DEBUG] runtime-plugin-kubernetes: Creating primary deployment from: name=api-deployment namespace=default
2022-04-02T08:26:05.659+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment-primary namespaces=default
2022-04-02T08:26:05.665+0100 [DEBUG] runtime-plugin-kubernetes: Successfully created new Primary deployment: name=api-deployment-primary namespace=default
2022-04-02T08:26:05.665+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:05.673+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:26:05.673+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:06.673+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:06.676+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:06.676+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:06.683+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:07.677+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:07.680+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:07.680+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:08.680+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:08.683+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:08.684+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:09.684+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:09.687+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:09.687+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:10.687+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:10.690+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:10.690+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:11.684+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:11.691+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:11.694+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:11.694+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:12.694+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:12.697+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:12.697+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:13.698+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:13.701+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:13.701+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:14.702+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:14.705+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:14.705+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:15.705+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:15.708+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:15.708+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:16.685+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:16.709+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:16.712+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:16.712+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:17.713+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:17.716+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:17.716+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:18.716+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:18.719+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:18.719+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:19.720+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:19.723+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:19.723+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:20.724+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:20.727+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:20.727+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:21.686+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:21.728+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:21.732+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:21.732+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:22.733+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:22.736+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:22.736+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:23.737+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:23.740+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:23.740+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:24.740+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:24.743+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:24.743+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:25.744+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:25.747+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:25.747+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:26.687+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:26.748+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:26:26.751+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:26:26.751+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:26:26.751+0100 [INFO]  runtime-plugin-kubernetes: Promote complete: name=api-deployment namespace=default
2022-04-02T08:26:26.751+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:26:26.754+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:26:31.689+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:31.760+0100 [INFO]  runtime-plugin-kubernetes: Remove candidate deployment: name=api-deployment namespace=default
2022-04-02T08:26:31.771+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:26:31.771+0100 [DEBUG] kubernetes-webhook: Ignore deployment, resource was modified by the controller: name=api-deployment namespace=default labels="map[app:api_v2 consul-release-controller-version:2631]"
2022-04-02T08:26:31.774+0100 [DEBUG] statemachine: Handle event: event=event_promoted state=state_promote
2022-04-02T08:26:31.774+0100 [DEBUG] statemachine: Log state: event=event_promoted state=state_promote
2022-04-02T08:26:31.774+0100 [DEBUG] statemachine: Log state: event=event_promoted release=api state=state_idle
2022-04-02T08:26:36.690+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:26:36.727+0100 [DEBUG] statemachine: Handle event: event=event_destroy state=state_idle
2022-04-02T08:26:36.727+0100 [DEBUG] statemachine: Log state: event=event_destroy state=state_idle
2022-04-02T08:26:36.727+0100 [DEBUG] statemachine: Destroy: state=state_destroy
2022-04-02T08:26:36.727+0100 [DEBUG] statemachine: Log state: event=event_destroy release=api state=state_destroy
2022-04-02T08:26:36.727+0100 [INFO]  runtime-plugin-kubernetes: Restore original deployment: name=api-deployment namespace=default
2022-04-02T08:26:36.730+0100 [DEBUG] runtime-plugin-kubernetes: Delete existing candidate deployment: name=api-deployment namespace=default
2022-04-02T08:26:36.736+0100 [DEBUG] runtime-plugin-kubernetes: Clone primary to create original deployment: name=api-deployment namespace=default
2022-04-02T08:26:36.742+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:26:36.748+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:36.752+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:26:36.752+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:37.752+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:37.756+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:37.756+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:38.756+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:38.759+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:38.759+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:39.759+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:39.762+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:39.762+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:40.763+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:40.766+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:40.766+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:41.767+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:41.769+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:41.769+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:42.770+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:42.773+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:42.773+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:43.773+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:43.777+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:43.777+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:44.777+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:44.781+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:44.781+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:45.781+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:45.784+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:45.784+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:46.785+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:46.788+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:46.788+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:47.789+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:47.792+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:47.792+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:48.793+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:48.796+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:48.796+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:49.797+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:49.800+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:49.800+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:50.801+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:50.804+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:50.804+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:51.805+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:51.808+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:26:51.808+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:26:52.809+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:26:52.812+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:26:52.812+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:26:52.812+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:26:52.815+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=0 traffic_canary=100
2022-04-02T08:26:57.825+0100 [INFO]  runtime-plugin-kubernetes: Remove primary deployment: name=api-deployment namespace=default
2022-04-02T08:26:57.829+0100 [INFO]  releaser-plugin-consul: Remove Consul config: name=api
2022-04-02T08:26:57.829+0100 [DEBUG] releaser-plugin-consul: Delete splitter: name=api
2022-04-02T08:26:58.834+0100 [DEBUG] releaser-plugin-consul: Cleanup router: name=api
2022-04-02T08:26:59.845+0100 [DEBUG] releaser-plugin-consul: Cleanup upstream router: name=api
2022-04-02T08:27:00.852+0100 [DEBUG] releaser-plugin-consul: Cleanup resolver: name=api
2022-04-02T08:27:01.858+0100 [DEBUG] releaser-plugin-consul: Cleanup service intentions: name=api
2022-04-02T08:27:02.869+0100 [DEBUG] releaser-plugin-consul: Cleanup defaults: name=api
2022-04-02T08:27:02.871+0100 [DEBUG] statemachine: Handle event: event=event_complete state=state_destroy
2022-04-02T08:27:02.871+0100 [DEBUG] statemachine: Log state: event=event_complete state=state_destroy
2022-04-02T08:27:02.871+0100 [DEBUG] statemachine: Log state: event=event_complete release=api state=state_idle
2022-04-02T08:27:08.770+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:27:08.773+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:27:08.773+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:27:08.789+0100 [INFO]  Shutting down server gracefully
2022-04-02T08:27:08.790+0100 [INFO]  Shutting down listener
2022-04-02T08:27:08.790+0100 [INFO]  Shutting down metrics
2022-04-02T08:27:08.791+0100 [INFO]  Shutting down kubernetes controller
2022-04-02T08:27:08.791+0100 [INFO]  kubernetes-controller: Stopping Kubernetes controller
2022-04-02T08:27:53.825+0100 [ERROR] 2022-04-02T08:27:53.825+0100 [DEBUG] Generating TLS Certificates for Ingress: path=/home/nicj/.shipyard/certs
2022-04-02T08:27:55.385+0100 [ERROR] 2022-04-02T08:27:55.385+0100 [DEBUG] Starting Ingress
2022-04-02T08:27:55.386+0100 [ERROR] Running configuration from:  ./shipyard/kubernetes

2022-04-02T08:27:55.386+0100 [DEBUG] Statefile does not exist
2022-04-02T08:27:58.282+0100 [ERROR] 2022-04-02T08:27:58.282+0100 [INFO]  Creating resources from configuration: path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes
2022-04-02T08:27:58.282+0100 [DEBUG] Statefile does not exist
2022-04-02T08:28:03.034+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=UPSTREAMS
2022-04-02T08:28:03.034+0100 [INFO]  Generating template: ref=controller_values output=/home/nicj/.shipyard/data/kube_setup/helm-values.yaml
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=GRAFANA_HTTP_ADDR
2022-04-02T08:28:03.034+0100 [DEBUG] Template content: ref=controller_values
  source=
  | controller:
  |   enabled: "#{{ .Vars.controller_enabled }}"
  |   container_config:
  |     image:
  |       repository: "#{{ .Vars.controller_repo }}"
  |       tag: "#{{ .Vars.controller_version }}"
  | autoencrypt:
  |   enabled: #{{ .Vars.tls_enabled }}
  | acls:
  |   enabled: #{{ .Vars.acls_enabled }}
  | #{{- if eq .Vars.controller_enabled false }}
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | #{{ end }}
  
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=GRAFANA_PASSWORD
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=CONSUL_CAKEY
2022-04-02T08:28:03.034+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_TOKEN_FILE
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=PROMETHEUS_HTTP_ADDR
2022-04-02T08:28:03.034+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Generating template: ref=consul_values output=/home/nicj/.shipyard/data/consul_kubernetes/consul_values.yaml
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [DEBUG] Template content: ref=consul_values
  source=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: #{{ .Vars.datacenter }}
  | 
  |   acls:
  |     manageSystemACLs: #{{ .Vars.acl_enabled }}
  |   tls:
  |     enabled: #{{ .Vars.tls_enabled }}
  |     enableAutoEncrypt: #{{ .Vars.tls_enabled }}
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: #{{ .Vars.federation_enabled }}
  |     createFederationSecret: #{{ .Vars.create_federation_secret }}
  | 
  |   image: #{{ .Vars.consul_image }}
  |   
  |   imageK8S: #{{ .Vars.consul_k8s_image }}
  |   
  |   imageEnvoy: #{{ .Vars.consul_envoy_image }}
  | 
  |   metrics:
  |     enabled: #{{ .Vars.metrics_enabled }}
  |     enableAgentMetrics: #{{ .Vars.metrics_enabled }}
  |     enableGatewayMetrics: #{{ .Vars.metrics_enabled }}
  |   
  |   logLevel: #{{ if eq .Vars.debug true }}"debug"#{{ else }}"info"#{{ end }}
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.#{{ .Vars.monitoring_namespace }}.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: #{{ if eq .Vars.debug true }}"--log-level debug"#{{ else }}null#{{ end }}
  | 
  |   transparentProxy:
  |     defaultEnabled: #{{ .Vars.transparent_proxy_enabled }}
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: #{{ .Vars.ingress_gateway_enabled }}
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       #{{ range .Vars.ingress_gateway_ports }}
  |         - port: #{{ . }}
  |           nodePort: null
  |       #{{ end }}
  | 
  | 
  | meshGateway:
  |   enabled: #{{ .Vars.mesh_gateway_enabled }}
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: #{{ .Vars.mesh_gateway_address }}
  |     port: 30443
  | 
  |   service:
  |     enabled: #{{ .Vars.mesh_gateway_enabled }}
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_ADDR
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=KUBECONFIG
2022-04-02T08:28:03.034+0100 [INFO]  Generating template: ref=consul_proxy_defaults output=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:28:03.035+0100 [DEBUG] Template content: ref=consul_proxy_defaults
  source=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.#{{ .Vars.monitoring_namespace}}.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [DEBUG] Template output: ref=controller_values
  destination=
  | controller:
  |   enabled: "false"
  |   container_config:
  |     image:
  |       repository: "nicholasjackson/consul-release-controller"
  |       tag: ""
  | autoencrypt:
  |   enabled: true
  | acls:
  |   enabled: true
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  |
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=TEMPO_HTTP_ADDR
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=TLS_KEY
2022-04-02T08:28:03.035+0100 [DEBUG] Template output: ref=consul_proxy_defaults
  destination=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.monitoring.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Generating template: ref=consul_namespace output=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:28:03.035+0100 [DEBUG] Template content: ref=consul_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [DEBUG] Template output: ref=consul_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [DEBUG] Template output: ref=consul_values
  destination=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: dc1
  | 
  |   acls:
  |     manageSystemACLs: true
  |   tls:
  |     enabled: true
  |     enableAutoEncrypt: true
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: false
  |     createFederationSecret: false
  | 
  |   image: hashicorp/consul:1.11.3
  |   
  |   imageK8S: hashicorp/consul-k8s-control-plane:0.40.0
  |   
  |   imageEnvoy: envoyproxy/envoy:v1.20.1
  | 
  |   metrics:
  |     enabled: true
  |     enableAgentMetrics: true
  |     enableGatewayMetrics: true
  |   
  |   logLevel: "info"
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: null
  | 
  |   transparentProxy:
  |     defaultEnabled: false
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: true
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       
  |         - port: 18080
  |           nodePort: null
  |       
  |         - port: 18443
  |           nodePort: null
  |       
  | 
  | 
  | meshGateway:
  |   enabled: false
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: dc1.k8s-cluster.shipyard.run
  |     port: 30443
  | 
  |   service:
  |     enabled: false
  |     type: NodePort
  |     nodePort: 30443
  
2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=TLS_CERT
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=GRAFANA_USER
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [INFO]  Generating template: ref=certs_script output=/home/nicj/.shipyard/data/kube_setup/fetch_certs.sh
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [DEBUG] Template content: ref=certs_script
  source=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.035+0100 [DEBUG] Template output: ref=certs_script
  destination=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
  
2022-04-02T08:28:03.034+0100 [INFO]  Creating Network: ref=dc1
2022-04-02T08:28:03.035+0100 [ERROR] 2022-04-02T08:28:03.034+0100 [INFO]  Creating Output: ref=CONSUL_CACERT
2022-04-02T08:28:03.036+0100 [ERROR] 2022-04-02T08:28:03.036+0100 [DEBUG] Attempting to create using bridge plugin: ref=dc1
2022-04-02T08:28:03.061+0100 [ERROR] 2022-04-02T08:28:03.061+0100 [INFO]  Creating ImageCache: ref=docker-cache
2022-04-02T08:28:03.063+0100 [ERROR] 2022-04-02T08:28:03.063+0100 [DEBUG] Connecting cache to network: name=network.dc1
2022-04-02T08:28:03.064+0100 [ERROR] 2022-04-02T08:28:03.064+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:28:03.083+0100 [ERROR] 2022-04-02T08:28:03.083+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:28:03.083+0100 [DEBUG] Creating Docker Container: ref=83478282-import
2022-04-02T08:28:05.721+0100 [ERROR] 2022-04-02T08:28:05.721+0100 [DEBUG] Forcefully remove: container=b700875f0affc3fa711eb1054b972288f7cd4bedef834ca229890d5693a55df2
2022-04-02T08:28:06.156+0100 [ERROR] 2022-04-02T08:28:06.155+0100 [DEBUG] Image exists in local cache: image=shipyardrun/docker-registry-proxy:0.6.3
2022-04-02T08:28:06.156+0100 [ERROR] 2022-04-02T08:28:06.156+0100 [DEBUG] Creating Docker Container: ref=docker-cache
2022-04-02T08:28:06.206+0100 [ERROR] 2022-04-02T08:28:06.206+0100 [DEBUG] Remove container from default networks: ref=docker-cache
2022-04-02T08:28:06.210+0100 [ERROR] 2022-04-02T08:28:06.210+0100 [DEBUG] Attaching container to network: ref=48e6b97be378c16cf340c5775d18f3fe8af9939424320b8f90b6d7a42e3cd1d2 network=dc1
2022-04-02T08:28:06.216+0100 [ERROR] 2022-04-02T08:28:06.216+0100 [DEBUG] Disconnectng network: name=bridge ref=docker-cache
2022-04-02T08:28:06.796+0100 [ERROR] 2022-04-02T08:28:06.796+0100 [INFO]  dc1: Creating Cluster: ref=dc1
2022-04-02T08:28:06.817+0100 [ERROR] 2022-04-02T08:28:06.817+0100 [DEBUG] Image exists in local cache: image=shipyardrun/k3s:v1.22.4
2022-04-02T08:28:06.818+0100 [ERROR] 2022-04-02T08:28:06.818+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:28:06.818+0100 [ERROR] 2022-04-02T08:28:06.818+0100 [DEBUG] Creating Docker Container: ref=server.dc1
2022-04-02T08:28:06.875+0100 [ERROR] 2022-04-02T08:28:06.875+0100 [DEBUG] Remove container from default networks: ref=server.dc1
2022-04-02T08:28:06.878+0100 [ERROR] 2022-04-02T08:28:06.878+0100 [DEBUG] Attaching container to network: ref=98822f1c9a6a4150191cd142f0e9f7ed365b3d10a43c67dc56e75343c0414158 network=dc1
2022-04-02T08:28:06.883+0100 [ERROR] 2022-04-02T08:28:06.883+0100 [DEBUG] Disconnectng network: name=bridge ref=server.dc1
2022-04-02T08:28:09.544+0100 [ERROR] 2022-04-02T08:28:09.544+0100 [DEBUG] Copying file from: id=98822f1c9a6a4150191cd142f0e9f7ed365b3d10a43c67dc56e75343c0414158 src=/output/kubeconfig.yaml dst=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:28:09.589+0100 [ERROR] 2022-04-02T08:28:09.589+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:11.599+0100 [ERROR] 2022-04-02T08:28:11.599+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:13.283+0100 [ERROR] 2022-04-02T08:28:13.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 15.000693]
2022-04-02T08:28:13.602+0100 [ERROR] 2022-04-02T08:28:13.602+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:15.606+0100 [ERROR] 2022-04-02T08:28:15.606+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:17.610+0100 [ERROR] 2022-04-02T08:28:17.610+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:19.613+0100 [ERROR] 2022-04-02T08:28:19.613+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:21.617+0100 [ERROR] 2022-04-02T08:28:21.617+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:23.620+0100 [ERROR] 2022-04-02T08:28:23.620+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:25.629+0100 [ERROR] 2022-04-02T08:28:25.629+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-z7wnn namespace=kube-system status=Pending
2022-04-02T08:28:27.635+0100 [ERROR] 2022-04-02T08:28:27.635+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-z7wnn namespace=kube-system status=Pending
2022-04-02T08:28:28.283+0100 [ERROR] 2022-04-02T08:28:28.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 30.001036]
2022-04-02T08:28:29.639+0100 [ERROR] 2022-04-02T08:28:29.639+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:28:29.639+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:28:31.643+0100 [ERROR] 2022-04-02T08:28:31.643+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:28:31.643+0100 [DEBUG] Writing docker images to volume: images=[] volume=images.volume.shipyard.run
2022-04-02T08:28:31.663+0100 [ERROR] 2022-04-02T08:28:31.663+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:28:31.663+0100 [DEBUG] Creating Docker Container: ref=63845971-import
2022-04-02T08:28:34.238+0100 [ERROR] 2022-04-02T08:28:34.238+0100 [DEBUG] Forcefully remove: container=aeb8011e95eef1d38d1518933cfa4192f78fd1a76507a4ff0512fcae13732054
2022-04-02T08:28:34.615+0100 [ERROR] 2022-04-02T08:28:34.615+0100 [DEBUG] dc1: Deploying connector
2022-04-02T08:28:36.234+0100 [ERROR] 2022-04-02T08:28:36.234+0100 [DEBUG] dc1: Writing namespace config: file=/tmp/4089621970/namespace.yaml
2022-04-02T08:28:36.234+0100 [DEBUG] dc1: Writing secret config: file=/tmp/4089621970/secret.yaml
2022-04-02T08:28:36.235+0100 [ERROR] 2022-04-02T08:28:36.234+0100 [DEBUG] dc1: Writing RBAC config: file=/tmp/4089621970/rbac.yaml
2022-04-02T08:28:36.235+0100 [ERROR] 2022-04-02T08:28:36.235+0100 [DEBUG] dc1: Writing deployment config: file=/tmp/4089621970/deployment.yaml
2022-04-02T08:28:36.235+0100 [ERROR] 2022-04-02T08:28:36.235+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/4089621970/namespace.yaml
2022-04-02T08:28:36.774+0100 [ERROR] 2022-04-02T08:28:36.774+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/4089621970/secret.yaml
2022-04-02T08:28:36.779+0100 [ERROR] 2022-04-02T08:28:36.779+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/4089621970/rbac.yaml
2022-04-02T08:28:36.787+0100 [ERROR] 2022-04-02T08:28:36.787+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/4089621970/deployment.yaml
2022-04-02T08:28:36.804+0100 [ERROR] 2022-04-02T08:28:36.804+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.807+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-2
2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=consul
2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=consul-rpc
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-2 local_port=18443 connector_addr=127.0.0.1:32577 local_addr=consul-ingress-gateway.consul.svc:18443
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=consul-lan-serf
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-1
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=web
2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-1 local_port=18080 connector_addr=127.0.0.1:32577 local_addr=consul-ingress-gateway.consul.svc:18080
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=controller-webhook
2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose local service: name=controller-webhook remote_port=19443 connector_addr=127.0.0.1:32577 local_addr=localhost:19443
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=consul local_port=8501 connector_addr=127.0.0.1:32577 local_addr=consul-server.consul.svc:8501
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Applying Kubernetes configuration: ref=cert-manager-controller config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml"]
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=consul-lan-serf local_port=8301 connector_addr=127.0.0.1:32577 local_addr=consul-server.consul.svc:8301
2022-04-02T08:28:38.808+0100 [INFO]  Create Ingress: ref=upstreams-proxy
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=upstreams-proxy local_port=28080 connector_addr=127.0.0.1:32577 local_addr=consul-release-controller.default.svc:8080
2022-04-02T08:28:38.808+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [INFO]  Applying Kubernetes configuration: ref=consul_namespace config=["/home/nicj/.shipyard/data/consul/namespace.yaml"]
2022-04-02T08:28:38.809+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=web local_port=9092 connector_addr=127.0.0.1:32577 local_addr=web.default.svc:9090
2022-04-02T08:28:38.809+0100 [ERROR] 2022-04-02T08:28:38.808+0100 [DEBUG] Calling connector to expose remote service: name=consul-rpc local_port=8300 connector_addr=127.0.0.1:32577 local_addr=consul-server.consul.svc:8300
2022-04-02T08:28:38.809+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml
2022-04-02T08:28:38.809+0100 [ERROR] 2022-04-02T08:28:38.809+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:28:38.829+0100 [ERROR] 2022-04-02T08:28:38.828+0100 [DEBUG] Successfully exposed service: id=7849ba34-548c-453c-9b2f-042635cc1304
2022-04-02T08:28:38.828+0100 [DEBUG] Successfully exposed service: id=d60e01b7-d23f-4532-8c80-688a904a7daf
2022-04-02T08:28:38.829+0100 [DEBUG] Successfully exposed service: id=4176dc31-e7b4-47f4-8e71-0ae0484bac51
2022-04-02T08:28:38.829+0100 [ERROR] 2022-04-02T08:28:38.829+0100 [DEBUG] Successfully exposed service: id=0badc0dd-07e1-4cd1-bbeb-c3a99f441308
2022-04-02T08:28:38.836+0100 [ERROR] 2022-04-02T08:28:38.835+0100 [DEBUG] Successfully exposed service: id=fb781663-b542-4218-9074-d462fb177920
2022-04-02T08:28:38.836+0100 [ERROR] 2022-04-02T08:28:38.836+0100 [DEBUG] Successfully exposed service: id=9d0c4aaf-c9b2-44ce-9317-a58ff83cc827
2022-04-02T08:28:38.836+0100 [ERROR] 2022-04-02T08:28:38.836+0100 [DEBUG] Successfully exposed service: id=2f5d92af-06dc-4674-9a02-dd93a64117c9
2022-04-02T08:28:38.836+0100 [ERROR] 2022-04-02T08:28:38.836+0100 [DEBUG] Successfully exposed service: id=a6562142-652d-4cd0-8b93-0eb7d7cda760
2022-04-02T08:28:38.873+0100 [ERROR] 2022-04-02T08:28:38.873+0100 [INFO]  Creating Helm chart: ref=consul
2022-04-02T08:28:38.873+0100 [DEBUG] Updating Helm chart repository: name=hashicorp url=https://helm.releases.hashicorp.com
2022-04-02T08:28:39.027+0100 [ERROR] 2022-04-02T08:28:39.027+0100 [DEBUG] Using Kubernetes config: ref=consul path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:28:39.027+0100 [ERROR] 2022-04-02T08:28:39.027+0100 [DEBUG] Creating chart from config: ref=consul chart=hashicorp/consul
2022-04-02T08:28:39.153+0100 [ERROR] 2022-04-02T08:28:39.153+0100 [DEBUG] Loading chart: ref=consul path=/home/nicj/.shipyard/helm_charts/cache/consul-0.40.0.tgz
2022-04-02T08:28:39.158+0100 [ERROR] 2022-04-02T08:28:39.158+0100 [DEBUG] Using Values: ref=consul
  values=
  | map[connectInject:map[centralConfig:map[enabled:true] default:false enabled:true envoyExtraArgs:<nil> failurePolicy:Ignore replicas:1 transparentProxy:map[defaultEnabled:false]] controller:map[enabled:true] global:map[acls:map[manageSystemACLs:true] datacenter:dc1 federation:map[createFederationSecret:false enabled:false] image:hashicorp/consul:1.11.3 imageEnvoy:envoyproxy/envoy:v1.20.1 imageK8S:hashicorp/consul-k8s-control-plane:0.40.0 logLevel:info metrics:map[enableAgentMetrics:true enableGatewayMetrics:true enabled:true] name:consul tls:map[enableAutoEncrypt:true enabled:true httpsOnly:false]] ingressGateways:map[defaults:map[replicas:1 service:map[ports:[map[nodePort:<nil> port:18080] map[nodePort:<nil> port:18443]]]] enabled:true] meshGateway:map[enabled:false replicas:1 service:map[enabled:false nodePort:30443 type:NodePort] wanAddress:map[port:30443 source:Static static:dc1.k8s-cluster.shipyard.run]] server:map[bootstrapExpect:1 extraConfig:{
  |   "ui_config": {
  |     "enabled": true,
  |     "metrics_provider": "prometheus",
  |     "metrics_proxy": {
  |       "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |     }
  |   }
  | }
  |  replicas:1 storage:128Mi] ui:map[enabled:true]]
  
2022-04-02T08:28:39.158+0100 [DEBUG] Validate chart: ref=consul
2022-04-02T08:28:39.158+0100 [DEBUG] Run chart: ref=consul
2022-04-02T08:28:39.204+0100 [ERROR] 2022-04-02T08:28:39.204+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:28:39.758+0100 [ERROR] 2022-04-02T08:28:39.758+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" ServiceAccount"
2022-04-02T08:28:39.761+0100 [ERROR] 2022-04-02T08:28:39.761+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="serviceaccounts \"consul-tls-init\" not found"
2022-04-02T08:28:39.813+0100 [ERROR] 2022-04-02T08:28:39.813+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:28:39.818+0100 [ERROR] 2022-04-02T08:28:39.818+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Role"
2022-04-02T08:28:39.820+0100 [ERROR] 2022-04-02T08:28:39.820+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="roles.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:28:39.870+0100 [ERROR] 2022-04-02T08:28:39.870+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:28:39.875+0100 [ERROR] 2022-04-02T08:28:39.875+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" RoleBinding"
2022-04-02T08:28:39.877+0100 [ERROR] 2022-04-02T08:28:39.877+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="rolebindings.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:28:39.937+0100 [ERROR] 2022-04-02T08:28:39.937+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:28:39.941+0100 [ERROR] 2022-04-02T08:28:39.941+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:28:39.943+0100 [ERROR] 2022-04-02T08:28:39.943+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="jobs.batch \"consul-tls-init\" not found"
2022-04-02T08:28:39.995+0100 [ERROR] 2022-04-02T08:28:39.995+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:28:40.003+0100 [ERROR] 2022-04-02T08:28:40.003+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-tls-init with timeout of 0s"
2022-04-02T08:28:40.008+0100 [ERROR] 2022-04-02T08:28:40.008+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: ADDED"
2022-04-02T08:28:40.008+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:28:40.019+0100 [ERROR] 2022-04-02T08:28:40.018+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:28:40.018+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:28:41.209+0100 [ERROR] 2022-04-02T08:28:41.209+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager status=Pending
2022-04-02T08:28:42.167+0100 [ERROR] 2022-04-02T08:28:42.167+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:28:42.167+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:28:42.180+0100 [ERROR] 2022-04-02T08:28:42.180+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:28:42.183+0100 [ERROR] 2022-04-02T08:28:42.183+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:28:42.189+0100 [ERROR] 2022-04-02T08:28:42.189+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 58 resource(s)"
2022-04-02T08:28:42.578+0100 [ERROR] 2022-04-02T08:28:42.578+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:28:42.584+0100 [ERROR] 2022-04-02T08:28:42.584+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-server-acl-init-cleanup with timeout of 0s"
2022-04-02T08:28:42.586+0100 [ERROR] 2022-04-02T08:28:42.586+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: ADDED"
2022-04-02T08:28:42.586+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:28:42.590+0100 [ERROR] 2022-04-02T08:28:42.590+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:28:42.590+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:28:43.214+0100 [ERROR] 2022-04-02T08:28:43.214+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager type=Ready value=False
2022-04-02T08:28:43.283+0100 [ERROR] 2022-04-02T08:28:43.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 45.000563]
2022-04-02T08:28:45.221+0100 [ERROR] 2022-04-02T08:28:45.221+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager type=Ready value=False
2022-04-02T08:28:47.227+0100 [ERROR] 2022-04-02T08:28:47.227+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager type=Ready value=False
2022-04-02T08:28:49.231+0100 [ERROR] 2022-04-02T08:28:49.231+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager type=Ready value=False
2022-04-02T08:28:51.236+0100 [ERROR] 2022-04-02T08:28:51.236+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-zcf8s namespace=cert-manager type=Ready value=False
2022-04-02T08:28:53.242+0100 [ERROR] 2022-04-02T08:28:53.242+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:28:58.283+0100 [ERROR] 2022-04-02T08:28:58.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 60.001061]
2022-04-02T08:29:01.396+0100 [ERROR] 2022-04-02T08:29:01.396+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:29:01.396+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:01.404+0100 [ERROR] 2022-04-02T08:29:01.404+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:29:01.406+0100 [ERROR] 2022-04-02T08:29:01.406+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-server-acl-init-cleanup\" Job"
2022-04-02T08:29:01.461+0100 [ERROR] 2022-04-02T08:29:01.461+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:29:03.467+0100 [ERROR] 2022-04-02T08:29:03.466+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-5w9d9 namespace=consul type=Ready value=False
2022-04-02T08:29:05.472+0100 [ERROR] 2022-04-02T08:29:05.472+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:29:05.472+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:29:07.477+0100 [ERROR] 2022-04-02T08:29:07.477+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:29:07.477+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:29:09.481+0100 [ERROR] 2022-04-02T08:29:09.481+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:29:09.481+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:29:11.485+0100 [ERROR] 2022-04-02T08:29:11.485+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Generating template: ref=grafana_secret_template output=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:29:11.486+0100 [DEBUG] Template content: ref=grafana_secret_template
  source=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
  
2022-04-02T08:29:11.486+0100 [INFO]  Create Ingress: ref=tempo
2022-04-02T08:29:11.486+0100 [INFO]  Generating template: ref=prometheus_operator_template output=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [DEBUG] Template content: ref=prometheus_operator_template
  source=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Generating template: ref=monitoring_namespace output=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:29:11.486+0100 [DEBUG] Template content: ref=monitoring_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
  
2022-04-02T08:29:11.486+0100 [DEBUG] Calling connector to expose remote service: name=tempo local_port=3100 connector_addr=127.0.0.1:32577 local_addr=tempo.default.svc:3100
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Creating Helm chart: ref=prometheus
2022-04-02T08:29:11.486+0100 [DEBUG] Template output: ref=grafana_secret_template
  destination=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: monitoring
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
  
2022-04-02T08:29:11.486+0100 [INFO]  Applying Kubernetes configuration: ref=consul_defaults config=["/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml"]
2022-04-02T08:29:11.486+0100 [INFO]  Create Ingress: ref=prometheus
2022-04-02T08:29:11.486+0100 [DEBUG] Template output: ref=monitoring_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [DEBUG] Updating Helm chart repository: name=prometheus url=https://prometheus-community.github.io/helm-charts
2022-04-02T08:29:11.486+0100 [INFO]  Applying Kubernetes configuration: ref=monitoring_namespace config=["/home/nicj/.shipyard/data/monitoring/namespace.yaml"]
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [DEBUG] Calling connector to expose remote service: name=prometheus local_port=9090 connector_addr=127.0.0.1:32577 local_addr=prometheus-operated.monitoring.svc:9090
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Create Ingress: ref=grafana
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [DEBUG] Calling connector to expose remote service: name=grafana local_port=8080 connector_addr=127.0.0.1:32577 local_addr=grafana.monitoring.svc:80
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [DEBUG] Template output: ref=prometheus_operator_template
  destination=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:29:11.486+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Generating template: ref=fetch_consul_resources output=/home/nicj/.shipyard/data/consul_kubernetes/fetch.sh
2022-04-02T08:29:11.486+0100 [DEBUG] Template content: ref=fetch_consul_resources
  source=
  |   #!/bin/sh -e
  | 
  |   echo "Port #{{ .Vars.port }}"
  |   echo "Fetching resources from running cluster, acls_enabled: #{{ .Vars.acl_enabled }}, tls_enabled #{{ .Vars.tls_enabled }}"
  | 
  |   #{{ if eq .Vars.acl_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   #{{end}}
  |   
  |   #{{ if eq .Vars.tls_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |   #{{end}}
2022-04-02T08:29:11.487+0100 [ERROR] 2022-04-02T08:29:11.486+0100 [INFO]  Create Ingress: ref=zipkin
2022-04-02T08:29:11.487+0100 [ERROR] 2022-04-02T08:29:11.487+0100 [DEBUG] Calling connector to expose remote service: name=zipkin local_port=9411 connector_addr=127.0.0.1:32577 local_addr=tempo.monitoring.svc:9411
2022-04-02T08:29:11.487+0100 [ERROR] 2022-04-02T08:29:11.487+0100 [DEBUG] Template output: ref=fetch_consul_resources
  destination=
  |   #!/bin/sh -e
  | 
  |   echo "Port 8501"
  |   echo "Fetching resources from running cluster, acls_enabled: true, tls_enabled true"
  | 
  |   
  |   kubectl get secret -n consul -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   
  |   
  |   
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |
2022-04-02T08:29:11.487+0100 [ERROR] 2022-04-02T08:29:11.487+0100 [INFO]  Remote executing command: ref=fetch_consul_resources command=sh args=["/data/fetch.sh"] image="&{shipyardrun/tools:v0.5.0  }"
2022-04-02T08:29:11.487+0100 [ERROR] 2022-04-02T08:29:11.487+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:29:11.488+0100 [ERROR] 2022-04-02T08:29:11.487+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:29:11.506+0100 [ERROR] 2022-04-02T08:29:11.506+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.5.0
2022-04-02T08:29:11.506+0100 [DEBUG] Creating Docker Container: ref=fetch_consul_resources.remote_exec
2022-04-02T08:29:11.509+0100 [ERROR] 2022-04-02T08:29:11.509+0100 [DEBUG] Successfully exposed service: id=8b652934-a4a2-415c-a36f-2b174ed2c944
2022-04-02T08:29:11.517+0100 [ERROR] 2022-04-02T08:29:11.517+0100 [DEBUG] Successfully exposed service: id=8716f023-e206-4731-ae8b-6afde65709bc
2022-04-02T08:29:11.517+0100 [ERROR] 2022-04-02T08:29:11.517+0100 [DEBUG] Successfully exposed service: id=37e8b5ce-cc2f-40c7-b07e-8da29488a100
2022-04-02T08:29:11.518+0100 [ERROR] 2022-04-02T08:29:11.518+0100 [DEBUG] Successfully exposed service: id=9ca4de8e-a6a9-4722-add2-cd56e3550f65
2022-04-02T08:29:11.563+0100 [ERROR] 2022-04-02T08:29:11.563+0100 [DEBUG] Remove container from default networks: ref=fetch_consul_resources.remote_exec
2022-04-02T08:29:11.566+0100 [ERROR] 2022-04-02T08:29:11.566+0100 [DEBUG] Attaching container to network: ref=5883402fa0ca508e895b64ed2555bcd9fb9dab62665f096f10cb657f1163f863 network=dc1
2022-04-02T08:29:11.572+0100 [ERROR] 2022-04-02T08:29:11.572+0100 [DEBUG] Disconnectng network: name=bridge ref=fetch_consul_resources.remote_exec
2022-04-02T08:29:11.981+0100 [ERROR] 2022-04-02T08:29:11.981+0100 [DEBUG] Using Kubernetes config: ref=prometheus path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:11.981+0100 [ERROR] 2022-04-02T08:29:11.981+0100 [DEBUG] Creating chart from config: ref=prometheus chart=prometheus/kube-prometheus-stack
2022-04-02T08:29:12.090+0100 [ERROR] 2022-04-02T08:29:12.090+0100 [INFO]  Applying Kubernetes configuration: ref=grafana_secret config=["/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml"]
2022-04-02T08:29:12.091+0100 [ERROR] 2022-04-02T08:29:12.090+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:29:12.571+0100 [ERROR] 2022-04-02T08:29:12.571+0100 [DEBUG] Port 8501
Fetching resources from running cluster, acls_enabled: true, tls_enabled true
2022-04-02T08:29:12.778+0100 [ERROR] 2022-04-02T08:29:12.778+0100 [DEBUG] Loading chart: ref=prometheus path=/home/nicj/.shipyard/helm_charts/cache/kube-prometheus-stack-32.0.0.tgz
2022-04-02T08:29:12.794+0100 [ERROR] 2022-04-02T08:29:12.794+0100 [DEBUG] Using Values: ref=prometheus values="map[alertmanager:map[enabled:false] defaultRules:map[create:false] grafana:map[enabled:false] serviceMonitor:map[enabled:false]]"
2022-04-02T08:29:12.794+0100 [DEBUG] Validate chart: ref=prometheus
2022-04-02T08:29:12.794+0100 [DEBUG] Run chart: ref=prometheus
2022-04-02T08:29:12.811+0100 [ERROR] 2022-04-02T08:29:12.811+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:12.860+0100 [ERROR] 2022-04-02T08:29:12.860+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:12.910+0100 [ERROR] 2022-04-02T08:29:12.909+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:12.929+0100 [ERROR] 2022-04-02T08:29:12.929+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:12.972+0100 [ERROR] 2022-04-02T08:29:12.972+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:13.058+0100 [ERROR] 2022-04-02T08:29:13.058+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:13.062+0100 [ERROR] 2022-04-02T08:29:13.062+0100 [DEBUG] Forcefully remove: container=5883402fa0ca508e895b64ed2555bcd9fb9dab62665f096f10cb657f1163f863
2022-04-02T08:29:13.068+0100 [ERROR] 2022-04-02T08:29:13.068+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:13.099+0100 [ERROR] 2022-04-02T08:29:13.099+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:13.162+0100 [ERROR] 2022-04-02T08:29:13.162+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Clearing discovery cache"
2022-04-02T08:29:13.162+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="beginning wait for 8 resources with timeout of 1m0s"
2022-04-02T08:29:13.283+0100 [ERROR] 2022-04-02T08:29:13.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 75.000695]
2022-04-02T08:29:16.570+0100 [ERROR] 2022-04-02T08:29:16.569+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:16.843+0100 [ERROR] 2022-04-02T08:29:16.843+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:29:16.846+0100 [ERROR] 2022-04-02T08:29:16.846+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:17.125+0100 [ERROR] 2022-04-02T08:29:17.125+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:17.131+0100 [ERROR] 2022-04-02T08:29:17.130+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:29:17.133+0100 [ERROR] 2022-04-02T08:29:17.133+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:17.424+0100 [ERROR] 2022-04-02T08:29:17.424+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:17.430+0100 [ERROR] 2022-04-02T08:29:17.430+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:29:17.432+0100 [ERROR] 2022-04-02T08:29:17.432+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:17.709+0100 [ERROR] 2022-04-02T08:29:17.709+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:17.715+0100 [ERROR] 2022-04-02T08:29:17.715+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:29:17.717+0100 [ERROR] 2022-04-02T08:29:17.717+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:17.999+0100 [ERROR] 2022-04-02T08:29:17.999+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:18.005+0100 [ERROR] 2022-04-02T08:29:18.004+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:29:18.007+0100 [ERROR] 2022-04-02T08:29:18.007+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:18.294+0100 [ERROR] 2022-04-02T08:29:18.294+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:18.300+0100 [ERROR] 2022-04-02T08:29:18.299+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:29:18.302+0100 [ERROR] 2022-04-02T08:29:18.302+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-create\" not found"
2022-04-02T08:29:18.578+0100 [ERROR] 2022-04-02T08:29:18.578+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:18.582+0100 [ERROR] 2022-04-02T08:29:18.582+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-create with timeout of 0s"
2022-04-02T08:29:18.584+0100 [ERROR] 2022-04-02T08:29:18.584+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: ADDED"
2022-04-02T08:29:18.584+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:18.602+0100 [ERROR] 2022-04-02T08:29:18.602+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:29:18.602+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:20.271+0100 [ERROR] 2022-04-02T08:29:20.271+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:29:20.271+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:20.279+0100 [ERROR] 2022-04-02T08:29:20.279+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:29:20.281+0100 [ERROR] 2022-04-02T08:29:20.281+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:29:20.287+0100 [ERROR] 2022-04-02T08:29:20.287+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:29:20.296+0100 [ERROR] 2022-04-02T08:29:20.295+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:29:20.301+0100 [ERROR] 2022-04-02T08:29:20.300+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:29:20.306+0100 [ERROR] 2022-04-02T08:29:20.306+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:29:20.310+0100 [ERROR] 2022-04-02T08:29:20.310+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:29:20.314+0100 [ERROR] 2022-04-02T08:29:20.314+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 36 resource(s)"
2022-04-02T08:29:20.508+0100 [ERROR] 2022-04-02T08:29:20.508+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:29:20.511+0100 [ERROR] 2022-04-02T08:29:20.511+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:20.791+0100 [ERROR] 2022-04-02T08:29:20.791+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:20.797+0100 [ERROR] 2022-04-02T08:29:20.797+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:29:20.800+0100 [ERROR] 2022-04-02T08:29:20.800+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:21.080+0100 [ERROR] 2022-04-02T08:29:21.080+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:21.085+0100 [ERROR] 2022-04-02T08:29:21.085+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:29:21.087+0100 [ERROR] 2022-04-02T08:29:21.087+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:21.375+0100 [ERROR] 2022-04-02T08:29:21.375+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:21.380+0100 [ERROR] 2022-04-02T08:29:21.380+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:29:21.384+0100 [ERROR] 2022-04-02T08:29:21.383+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:21.692+0100 [ERROR] 2022-04-02T08:29:21.691+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:21.697+0100 [ERROR] 2022-04-02T08:29:21.697+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:29:21.699+0100 [ERROR] 2022-04-02T08:29:21.699+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:29:21.987+0100 [ERROR] 2022-04-02T08:29:21.987+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:21.993+0100 [ERROR] 2022-04-02T08:29:21.993+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:29:21.997+0100 [ERROR] 2022-04-02T08:29:21.997+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-patch\" not found"
2022-04-02T08:29:22.289+0100 [ERROR] 2022-04-02T08:29:22.289+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:29:22.297+0100 [ERROR] 2022-04-02T08:29:22.297+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-patch with timeout of 0s"
2022-04-02T08:29:22.299+0100 [ERROR] 2022-04-02T08:29:22.299+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: ADDED"
2022-04-02T08:29:22.299+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:22.310+0100 [ERROR] 2022-04-02T08:29:22.310+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:29:22.310+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:25.290+0100 [ERROR] 2022-04-02T08:29:25.290+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:29:25.290+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:29:25.300+0100 [ERROR] 2022-04-02T08:29:25.300+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:29:25.302+0100 [ERROR] 2022-04-02T08:29:25.302+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:29:25.308+0100 [ERROR] 2022-04-02T08:29:25.308+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:29:25.318+0100 [ERROR] 2022-04-02T08:29:25.318+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:29:25.323+0100 [ERROR] 2022-04-02T08:29:25.323+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:29:25.328+0100 [ERROR] 2022-04-02T08:29:25.328+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:29:25.334+0100 [ERROR] 2022-04-02T08:29:25.334+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:29:25.685+0100 [ERROR] 2022-04-02T08:29:25.685+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:29:27.690+0100 [ERROR] 2022-04-02T08:29:27.690+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-nkkrw namespace=monitoring type=Ready value=False
2022-04-02T08:29:28.283+0100 [ERROR] 2022-04-02T08:29:28.283+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 90.000978]
2022-04-02T08:29:29.696+0100 [ERROR] 2022-04-02T08:29:29.696+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-nkkrw namespace=monitoring type=Ready value=False
2022-04-02T08:29:31.701+0100 [ERROR] 2022-04-02T08:29:31.701+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:29:31.701+0100 [ERROR] 2022-04-02T08:29:31.701+0100 [INFO]  Applying Kubernetes configuration: ref=prometheus config=["/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml"]
2022-04-02T08:29:31.701+0100 [ERROR] 2022-04-02T08:29:31.701+0100 [INFO]  Creating Helm chart: ref=loki
2022-04-02T08:29:31.701+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:29:31.702+0100 [ERROR] 2022-04-02T08:29:31.702+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:29:32.025+0100 [ERROR] 2022-04-02T08:29:32.025+0100 [DEBUG] Using Kubernetes config: ref=loki path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:32.025+0100 [ERROR] 2022-04-02T08:29:32.025+0100 [DEBUG] Creating chart from config: ref=loki chart=grafana/loki
2022-04-02T08:29:32.914+0100 [ERROR] 2022-04-02T08:29:32.913+0100 [DEBUG] Loading chart: ref=loki path=/home/nicj/.shipyard/helm_charts/cache/loki-2.9.1.tgz
2022-04-02T08:29:32.914+0100 [ERROR] 2022-04-02T08:29:32.914+0100 [DEBUG] Using Values: ref=loki values=map[]
2022-04-02T08:29:32.914+0100 [DEBUG] Validate chart: ref=loki
2022-04-02T08:29:32.914+0100 [DEBUG] Run chart: ref=loki
2022-04-02T08:29:33.148+0100 [ERROR] W0402 08:29:33.148759    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:33.162+0100 [ERROR] 2022-04-02T08:29:33.162+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 1 resource(s)"
2022-04-02T08:29:33.170+0100 [ERROR] 2022-04-02T08:29:33.170+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 8 resource(s)"
2022-04-02T08:29:33.174+0100 [ERROR] W0402 08:29:33.174431    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:33.203+0100 [ERROR] 2022-04-02T08:29:33.203+0100 [INFO]  Creating Helm chart: ref=promtail
2022-04-02T08:29:33.203+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:29:33.203+0100 [DEBUG] Using Kubernetes config: ref=promtail path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:33.204+0100 [ERROR] 2022-04-02T08:29:33.204+0100 [DEBUG] Creating chart from config: ref=promtail chart=grafana/promtail
2022-04-02T08:29:33.856+0100 [ERROR] 2022-04-02T08:29:33.856+0100 [DEBUG] Loading chart: ref=promtail path=/home/nicj/.shipyard/helm_charts/cache/promtail-3.11.0.tgz
2022-04-02T08:29:33.857+0100 [ERROR] 2022-04-02T08:29:33.857+0100 [DEBUG] Using Values: ref=promtail values=map[config:map[lokiAddress:http://loki:3100/loki/api/v1/push]]
2022-04-02T08:29:33.857+0100 [DEBUG] Validate chart: ref=promtail
2022-04-02T08:29:33.857+0100 [DEBUG] Run chart: ref=promtail
2022-04-02T08:29:34.276+0100 [ERROR] 2022-04-02T08:29:34.276+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 1 resource(s)"
2022-04-02T08:29:34.286+0100 [ERROR] 2022-04-02T08:29:34.285+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 5 resource(s)"
2022-04-02T08:29:34.310+0100 [ERROR] 2022-04-02T08:29:34.310+0100 [INFO]  Creating Helm chart: ref=tempo
2022-04-02T08:29:34.310+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:29:34.310+0100 [DEBUG] Using Kubernetes config: ref=tempo path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:34.311+0100 [ERROR] 2022-04-02T08:29:34.311+0100 [DEBUG] Creating chart from config: ref=tempo chart=grafana/tempo
2022-04-02T08:29:34.902+0100 [ERROR] 2022-04-02T08:29:34.902+0100 [DEBUG] Loading chart: ref=tempo path=/home/nicj/.shipyard/helm_charts/cache/tempo-0.13.1.tgz
2022-04-02T08:29:34.903+0100 [ERROR] 2022-04-02T08:29:34.903+0100 [DEBUG] Using Values: ref=tempo values="map[tempo:map[receivers:map[jaeger:map[protocols:map[grpc:map[endpoint:0.0.0.0:14250] thrift_binary:map[endpoint:0.0.0.0:6832] thrift_compact:map[endpoint:0.0.0.0:6831] thrift_http:map[endpoint:0.0.0.0:14268]]] zipkin:map[]]]]"
2022-04-02T08:29:34.903+0100 [DEBUG] Validate chart: ref=tempo
2022-04-02T08:29:34.903+0100 [DEBUG] Run chart: ref=tempo
2022-04-02T08:29:35.144+0100 [ERROR] 2022-04-02T08:29:35.144+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 1 resource(s)"
2022-04-02T08:29:35.153+0100 [ERROR] 2022-04-02T08:29:35.153+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 5 resource(s)"
2022-04-02T08:29:35.185+0100 [ERROR] 2022-04-02T08:29:35.184+0100 [INFO]  Creating Helm chart: ref=grafana
2022-04-02T08:29:35.185+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:29:35.185+0100 [ERROR] 2022-04-02T08:29:35.185+0100 [DEBUG] Using Kubernetes config: ref=grafana path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:35.185+0100 [ERROR] 2022-04-02T08:29:35.185+0100 [DEBUG] Creating chart from config: ref=grafana chart=grafana/grafana
2022-04-02T08:29:35.528+0100 [ERROR] 2022-04-02T08:29:35.528+0100 [DEBUG] Loading chart: ref=grafana path=/home/nicj/.shipyard/helm_charts/cache/grafana-6.21.2.tgz
2022-04-02T08:29:35.530+0100 [ERROR] 2022-04-02T08:29:35.529+0100 [DEBUG] Using Values: ref=grafana values="map[admin:map[existingSecret:grafana-password] datasources:map[datasources.yaml:map[apiVersion:1 datasources:[map[isDefault:true name:Prometheus type:prometheus url:http://prometheus-kube-prometheus-prometheus:9090] map[isDefault:false jsonData:map[derivedFields:[map[datasourceUid:tempo_uid matcherRegex:trace_id=(\\w+) name:trace_id url:$${__value.raw}]] maxLines:1000] name:Loki type:loki uid:loki_uid url:http://loki:3100] map[isDefault:false name:Tempo type:tempo uid:tempo_uid url:http://tempo:3100]]]] sidecar:map[dashboards:map[enabled:true]]]"
2022-04-02T08:29:35.530+0100 [DEBUG] Validate chart: ref=grafana
2022-04-02T08:29:35.530+0100 [DEBUG] Run chart: ref=grafana
2022-04-02T08:29:35.852+0100 [ERROR] W0402 08:29:35.852165    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:35.854+0100 [ERROR] W0402 08:29:35.854231    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:35.884+0100 [ERROR] 2022-04-02T08:29:35.884+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 1 resource(s)"
2022-04-02T08:29:35.903+0100 [ERROR] 2022-04-02T08:29:35.902+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 15 resource(s)"
2022-04-02T08:29:35.906+0100 [ERROR] W0402 08:29:35.906253    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:35.906+0100 [ERROR] W0402 08:29:35.906415    7537 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:29:35.964+0100 [ERROR] 2022-04-02T08:29:35.964+0100 [INFO]  Generating template: ref=monitor_ingress_gateway output=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:29:35.964+0100 [DEBUG] Template content: ref=monitor_ingress_gateway
  source=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: #{{ .Vars.consul_namespace }}
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:29:35.964+0100 [ERROR] 2022-04-02T08:29:35.964+0100 [DEBUG] Template output: ref=monitor_ingress_gateway
  destination=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: monitoring
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: consul
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:29:35.964+0100 [ERROR] 2022-04-02T08:29:35.964+0100 [INFO]  Applying Kubernetes configuration: ref=monitor_ingress_gateway config=["/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml"]
2022-04-02T08:29:35.965+0100 [ERROR] 2022-04-02T08:29:35.965+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:29:36.045+0100 [ERROR] 2022-04-02T08:29:36.045+0100 [INFO]  Applying Kubernetes configuration: ref=application config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/../../example/kubernetes/"]
2022-04-02T08:29:36.045+0100 [ERROR] 2022-04-02T08:29:36.045+0100 [INFO]  Creating Helm chart: ref=consul-release-controller
2022-04-02T08:29:36.045+0100 [INFO]  Applying Kubernetes configuration: ref=upstreams-proxy config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml"]
2022-04-02T08:29:36.045+0100 [ERROR] 2022-04-02T08:29:36.045+0100 [DEBUG] Using Kubernetes config: ref=consul-release-controller path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:29:36.045+0100 [ERROR] 2022-04-02T08:29:36.045+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml
2022-04-02T08:29:36.045+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/api.yaml
2022-04-02T08:29:36.046+0100 [ERROR] 2022-04-02T08:29:36.045+0100 [DEBUG] Creating chart from config: ref=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:29:36.045+0100 [DEBUG] Loading chart: ref=consul-release-controller path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:29:36.047+0100 [ERROR] 2022-04-02T08:29:36.046+0100 [DEBUG] Using Values: ref=consul-release-controller values="map[acls:map[enabled:true] autoencrypt:map[enabled:true] controller:map[container_config:map[image:map[repository:nicholasjackson/consul-release-controller tag:]] enabled:false] webhook:map[namespace:shipyard service:controller-webhook]]"
2022-04-02T08:29:36.047+0100 [DEBUG] Validate chart: ref=consul-release-controller
2022-04-02T08:29:36.047+0100 [DEBUG] Run chart: ref=consul-release-controller
2022-04-02T08:29:36.168+0100 [ERROR] 2022-04-02T08:29:36.168+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/application-dashboard.yaml
2022-04-02T08:29:36.194+0100 [ERROR] 2022-04-02T08:29:36.194+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/consul-config.yaml
2022-04-02T08:29:36.266+0100 [ERROR] 2022-04-02T08:29:36.265+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest-dashboard.yaml
2022-04-02T08:29:36.293+0100 [ERROR] 2022-04-02T08:29:36.293+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest.yaml
2022-04-02T08:29:36.351+0100 [ERROR] 2022-04-02T08:29:36.350+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/metrics.yaml
2022-04-02T08:29:36.357+0100 [ERROR] 2022-04-02T08:29:36.357+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/web.yaml
2022-04-02T08:29:36.448+0100 [ERROR] 2022-04-02T08:29:36.448+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 1 resource(s)"
2022-04-02T08:29:36.461+0100 [ERROR] 2022-04-02T08:29:36.461+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 13 resource(s)"
2022-04-02T08:29:36.600+0100 [ERROR] 2022-04-02T08:29:36.600+0100 [INFO]  Remote executing command: ref=exec_standalone command=sh args=["/output/fetch_certs.sh"] image="&{shipyardrun/tools:v0.6.0  }"
2022-04-02T08:29:36.623+0100 [ERROR] 2022-04-02T08:29:36.623+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.6.0
2022-04-02T08:29:36.623+0100 [DEBUG] Creating Docker Container: ref=exec_standalone.remote_exec
2022-04-02T08:29:37.048+0100 [ERROR] 2022-04-02T08:29:37.048+0100 [DEBUG] Remove container from default networks: ref=exec_standalone.remote_exec
2022-04-02T08:29:37.052+0100 [ERROR] 2022-04-02T08:29:37.052+0100 [DEBUG] Attaching container to network: ref=7f054585b5c6de149a4fd558847b805022365300fb29756bbd0f59a3c1560f2e network=dc1
2022-04-02T08:29:37.066+0100 [ERROR] 2022-04-02T08:29:37.066+0100 [DEBUG] Disconnectng network: name=bridge ref=exec_standalone.remote_exec
2022-04-02T08:29:40.323+0100 [ERROR] 2022-04-02T08:29:40.323+0100 [DEBUG] Forcefully remove: container=7f054585b5c6de149a4fd558847b805022365300fb29756bbd0f59a3c1560f2e
2022-04-02T08:29:40.857+0100 [ERROR] 2022-04-02T08:29:40.857+0100 [DEBUG] Health check urls for browser windows: count=0
2022-04-02T08:29:40.857+0100 [DEBUG] Browser windows open

########################################################

Title Development setup
Author Nic Jackson
2022-04-02T08:29:40.857+0100 [ERROR] 
• Consul: https://localhost:8501
• Grafana: https://localhost:8080
• Application: http://localhost:18080

This blueprint defines 13 output variables.

You can set output variables as environment variables for your current terminal session using the following command:

eval $(shipyard env)

To list output variables use the command:

shipyard output
2022-04-02T08:29:41.478+0100 [INFO]  Starting controller
2022-04-02T08:29:46.062+0100 [DEBUG] statemachine: Handle event: event=event_configure state=state_start
2022-04-02T08:29:46.062+0100 [DEBUG] statemachine: Log state: event=event_configure state=state_start
2022-04-02T08:29:46.062+0100 [DEBUG] statemachine: Configure: state=state_configure
2022-04-02T08:29:46.062+0100 [DEBUG] statemachine: Log state: event=event_configure release=api state=state_configure
2022-04-02T08:29:46.062+0100 [INFO]  releaser-plugin-consul: Initializing deployment: service=api
2022-04-02T08:29:46.062+0100 [DEBUG] releaser-plugin-consul: Create service defaults: service=api
2022-04-02T08:29:46.090+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:29:46.090+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_configure
2022-04-02T08:29:46.090+0100 [DEBUG] kubernetes-webhook: Reject deployment, there is currently an active release for this deployment: name=api-deployment namespace=default state=state_configure
2022-04-02T08:29:46.092+0100 [INFO]  Shutting down server gracefully
2022-04-02T08:29:46.093+0100 [INFO]  Shutting down listener
2022-04-02T08:29:46.093+0100 [INFO]  Shutting down metrics
2022-04-02T08:29:46.095+0100 [INFO]  Shutting down kubernetes controller
2022-04-02T08:29:46.095+0100 [INFO]  kubernetes-controller: Stopping Kubernetes controller
2022-04-02T08:29:47.071+0100 [ERROR] releaser-plugin-consul: Unable to create Consul ServiceDefaults: name=consul-release-controller error="Get \"https://127.0.0.1:8501/v1/config/service-defaults/consul-release-controller\": dial tcp 127.0.0.1:8501: connect: connection refused"
2022-04-02T08:29:47.071+0100 [ERROR] statemachine: Configure completed with error: error="Get \"https://127.0.0.1:8501/v1/config/service-defaults/consul-release-controller\": dial tcp 127.0.0.1:8501: connect: connection refused"
2022-04-02T08:29:47.071+0100 [DEBUG] statemachine: Handle event: event=event_fail state=state_configure
2022-04-02T08:29:47.071+0100 [DEBUG] statemachine: Log state: event=event_fail state=state_configure
2022-04-02T08:29:47.071+0100 [DEBUG] statemachine: Log state: event=event_fail release=api state=state_fail
2022-04-02T08:30:31.299+0100 [ERROR] 2022-04-02T08:30:31.299+0100 [DEBUG] Generating TLS Certificates for Ingress: path=/home/nicj/.shipyard/certs
2022-04-02T08:30:33.676+0100 [ERROR] 2022-04-02T08:30:33.676+0100 [DEBUG] Starting Ingress
2022-04-02T08:30:33.676+0100 [ERROR] Running configuration from:  ./shipyard/kubernetes

2022-04-02T08:30:33.676+0100 [DEBUG] Statefile does not exist
2022-04-02T08:30:36.894+0100 [ERROR] 2022-04-02T08:30:36.894+0100 [INFO]  Creating resources from configuration: path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes
2022-04-02T08:30:36.894+0100 [DEBUG] Statefile does not exist
2022-04-02T08:30:39.982+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=TEMPO_HTTP_ADDR
2022-04-02T08:30:39.982+0100 [INFO]  Generating template: ref=consul_namespace output=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=CONSUL_CACERT
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=TLS_KEY
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=GRAFANA_USER
2022-04-02T08:30:39.982+0100 [INFO]  Generating template: ref=consul_proxy_defaults output=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:30:39.982+0100 [DEBUG] Template content: ref=consul_proxy_defaults
  source=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.#{{ .Vars.monitoring_namespace}}.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-02T08:30:39.982+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Network: ref=dc1
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [DEBUG] Template content: ref=consul_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Generating template: ref=certs_script output=/home/nicj/.shipyard/data/kube_setup/fetch_certs.sh
2022-04-02T08:30:39.983+0100 [DEBUG] Template content: ref=certs_script
  source=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.983+0100 [DEBUG] Template output: ref=consul_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
  
2022-04-02T08:30:39.983+0100 [DEBUG] Template output: ref=certs_script
  destination=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Generating template: ref=controller_values output=/home/nicj/.shipyard/data/kube_setup/helm-values.yaml
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.983+0100 [DEBUG] Template content: ref=controller_values
  source=
  | controller:
  |   enabled: "#{{ .Vars.controller_enabled }}"
  |   container_config:
  |     image:
  |       repository: "#{{ .Vars.controller_repo }}"
  |       tag: "#{{ .Vars.controller_version }}"
  | autoencrypt:
  |   enabled: #{{ .Vars.tls_enabled }}
  | acls:
  |   enabled: #{{ .Vars.acls_enabled }}
  | #{{- if eq .Vars.controller_enabled false }}
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | #{{ end }}
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=TLS_CERT
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=CONSUL_CAKEY
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=UPSTREAMS
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=GRAFANA_HTTP_ADDR
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_TOKEN_FILE
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=PROMETHEUS_HTTP_ADDR
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=KUBECONFIG
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=GRAFANA_PASSWORD
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [INFO]  Generating template: ref=consul_values output=/home/nicj/.shipyard/data/consul_kubernetes/consul_values.yaml
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.983+0100 [DEBUG] Template content: ref=consul_values
  source=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: #{{ .Vars.datacenter }}
  | 
  |   acls:
  |     manageSystemACLs: #{{ .Vars.acl_enabled }}
  |   tls:
  |     enabled: #{{ .Vars.tls_enabled }}
  |     enableAutoEncrypt: #{{ .Vars.tls_enabled }}
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: #{{ .Vars.federation_enabled }}
  |     createFederationSecret: #{{ .Vars.create_federation_secret }}
  | 
  |   image: #{{ .Vars.consul_image }}
  |   
  |   imageK8S: #{{ .Vars.consul_k8s_image }}
  |   
  |   imageEnvoy: #{{ .Vars.consul_envoy_image }}
  | 
  |   metrics:
  |     enabled: #{{ .Vars.metrics_enabled }}
  |     enableAgentMetrics: #{{ .Vars.metrics_enabled }}
  |     enableGatewayMetrics: #{{ .Vars.metrics_enabled }}
  |   
  |   logLevel: #{{ if eq .Vars.debug true }}"debug"#{{ else }}"info"#{{ end }}
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.#{{ .Vars.monitoring_namespace }}.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: #{{ if eq .Vars.debug true }}"--log-level debug"#{{ else }}null#{{ end }}
  | 
  |   transparentProxy:
  |     defaultEnabled: #{{ .Vars.transparent_proxy_enabled }}
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: #{{ .Vars.ingress_gateway_enabled }}
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       #{{ range .Vars.ingress_gateway_ports }}
  |         - port: #{{ . }}
  |           nodePort: null
  |       #{{ end }}
  | 
  | 
  | meshGateway:
  |   enabled: #{{ .Vars.mesh_gateway_enabled }}
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: #{{ .Vars.mesh_gateway_address }}
  |     port: 30443
  | 
  |   service:
  |     enabled: #{{ .Vars.mesh_gateway_enabled }}
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.983+0100 [DEBUG] Template output: ref=controller_values
  destination=
  | controller:
  |   enabled: "false"
  |   container_config:
  |     image:
  |       repository: "nicholasjackson/consul-release-controller"
  |       tag: ""
  | autoencrypt:
  |   enabled: true
  | acls:
  |   enabled: true
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  |
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.982+0100 [DEBUG] Template output: ref=consul_proxy_defaults
  destination=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.monitoring.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
  
2022-04-02T08:30:39.982+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_ADDR
2022-04-02T08:30:39.983+0100 [ERROR] 2022-04-02T08:30:39.983+0100 [DEBUG] Template output: ref=consul_values
  destination=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: dc1
  | 
  |   acls:
  |     manageSystemACLs: true
  |   tls:
  |     enabled: true
  |     enableAutoEncrypt: true
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: false
  |     createFederationSecret: false
  | 
  |   image: hashicorp/consul:1.11.3
  |   
  |   imageK8S: hashicorp/consul-k8s-control-plane:0.40.0
  |   
  |   imageEnvoy: envoyproxy/envoy:v1.20.1
  | 
  |   metrics:
  |     enabled: true
  |     enableAgentMetrics: true
  |     enableGatewayMetrics: true
  |   
  |   logLevel: "info"
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: null
  | 
  |   transparentProxy:
  |     defaultEnabled: false
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: true
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       
  |         - port: 18080
  |           nodePort: null
  |       
  |         - port: 18443
  |           nodePort: null
  |       
  | 
  | 
  | meshGateway:
  |   enabled: false
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: dc1.k8s-cluster.shipyard.run
  |     port: 30443
  | 
  |   service:
  |     enabled: false
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:30:39.984+0100 [ERROR] 2022-04-02T08:30:39.984+0100 [DEBUG] Attempting to create using bridge plugin: ref=dc1
2022-04-02T08:30:40.007+0100 [ERROR] 2022-04-02T08:30:40.007+0100 [INFO]  Creating ImageCache: ref=docker-cache
2022-04-02T08:30:40.010+0100 [ERROR] 2022-04-02T08:30:40.010+0100 [DEBUG] Connecting cache to network: name=network.dc1
2022-04-02T08:30:40.011+0100 [ERROR] 2022-04-02T08:30:40.011+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:30:40.029+0100 [ERROR] 2022-04-02T08:30:40.029+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:30:40.029+0100 [DEBUG] Creating Docker Container: ref=29925218-import
2022-04-02T08:30:42.697+0100 [ERROR] 2022-04-02T08:30:42.697+0100 [DEBUG] Forcefully remove: container=1a96b777bdae3b860cce347673c73d3f3d70fe9e009e4257bcc5817acf7f775b
2022-04-02T08:30:43.107+0100 [ERROR] 2022-04-02T08:30:43.107+0100 [DEBUG] Image exists in local cache: image=shipyardrun/docker-registry-proxy:0.6.3
2022-04-02T08:30:43.107+0100 [ERROR] 2022-04-02T08:30:43.107+0100 [DEBUG] Creating Docker Container: ref=docker-cache
2022-04-02T08:30:43.159+0100 [ERROR] 2022-04-02T08:30:43.159+0100 [DEBUG] Remove container from default networks: ref=docker-cache
2022-04-02T08:30:43.162+0100 [ERROR] 2022-04-02T08:30:43.162+0100 [DEBUG] Attaching container to network: ref=d6e52b47c162800478704bcb36db2e9991f1b72fc4ac8388540bb8ae49b2acb5 network=dc1
2022-04-02T08:30:43.169+0100 [ERROR] 2022-04-02T08:30:43.169+0100 [DEBUG] Disconnectng network: name=bridge ref=docker-cache
2022-04-02T08:30:43.837+0100 [ERROR] 2022-04-02T08:30:43.837+0100 [INFO]  dc1: Creating Cluster: ref=dc1
2022-04-02T08:30:43.859+0100 [ERROR] 2022-04-02T08:30:43.859+0100 [DEBUG] Image exists in local cache: image=shipyardrun/k3s:v1.22.4
2022-04-02T08:30:43.860+0100 [ERROR] 2022-04-02T08:30:43.860+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:30:43.860+0100 [ERROR] 2022-04-02T08:30:43.860+0100 [DEBUG] Creating Docker Container: ref=server.dc1
2022-04-02T08:30:43.916+0100 [ERROR] 2022-04-02T08:30:43.916+0100 [DEBUG] Remove container from default networks: ref=server.dc1
2022-04-02T08:30:43.919+0100 [ERROR] 2022-04-02T08:30:43.919+0100 [DEBUG] Attaching container to network: ref=02863c59bf438c3069014412f24e105baf08258f0205b50ab210c46162bdd358 network=dc1
2022-04-02T08:30:43.929+0100 [ERROR] 2022-04-02T08:30:43.929+0100 [DEBUG] Disconnectng network: name=bridge ref=server.dc1
2022-04-02T08:30:46.534+0100 [ERROR] 2022-04-02T08:30:46.534+0100 [DEBUG] Copying file from: id=02863c59bf438c3069014412f24e105baf08258f0205b50ab210c46162bdd358 src=/output/kubeconfig.yaml dst=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:30:46.574+0100 [ERROR] 2022-04-02T08:30:46.573+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:48.584+0100 [ERROR] 2022-04-02T08:30:48.584+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:50.587+0100 [ERROR] 2022-04-02T08:30:50.587+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:51.895+0100 [ERROR] 2022-04-02T08:30:51.895+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 15.000301]
2022-04-02T08:30:52.590+0100 [ERROR] 2022-04-02T08:30:52.590+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:54.594+0100 [ERROR] 2022-04-02T08:30:54.594+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:56.597+0100 [ERROR] 2022-04-02T08:30:56.597+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:30:58.602+0100 [ERROR] 2022-04-02T08:30:58.602+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:31:00.606+0100 [ERROR] 2022-04-02T08:31:00.606+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:31:02.611+0100 [ERROR] 2022-04-02T08:31:02.610+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-9j4kj namespace=kube-system status=Pending
2022-04-02T08:31:04.616+0100 [ERROR] 2022-04-02T08:31:04.616+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-9j4kj namespace=kube-system status=Pending
2022-04-02T08:31:06.620+0100 [ERROR] 2022-04-02T08:31:06.620+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:31:06.620+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:31:06.895+0100 [ERROR] 2022-04-02T08:31:06.895+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 30.000659]
2022-04-02T08:31:08.624+0100 [ERROR] 2022-04-02T08:31:08.624+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:31:08.624+0100 [DEBUG] Writing docker images to volume: images=[] volume=images.volume.shipyard.run
2022-04-02T08:31:08.641+0100 [ERROR] 2022-04-02T08:31:08.641+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:31:08.641+0100 [DEBUG] Creating Docker Container: ref=41887903-import
2022-04-02T08:31:11.174+0100 [ERROR] 2022-04-02T08:31:11.174+0100 [DEBUG] Forcefully remove: container=16f5ef710361f211aa41a4e0ca31daf0647a0da07d4fdc99d72d17bc37e8e3c2
2022-04-02T08:31:11.516+0100 [ERROR] 2022-04-02T08:31:11.516+0100 [DEBUG] dc1: Deploying connector
2022-04-02T08:31:12.791+0100 [ERROR] 2022-04-02T08:31:12.791+0100 [DEBUG] dc1: Writing namespace config: file=/tmp/1072507525/namespace.yaml
2022-04-02T08:31:12.791+0100 [DEBUG] dc1: Writing secret config: file=/tmp/1072507525/secret.yaml
2022-04-02T08:31:12.792+0100 [ERROR] 2022-04-02T08:31:12.792+0100 [DEBUG] dc1: Writing RBAC config: file=/tmp/1072507525/rbac.yaml
2022-04-02T08:31:12.792+0100 [ERROR] 2022-04-02T08:31:12.792+0100 [DEBUG] dc1: Writing deployment config: file=/tmp/1072507525/deployment.yaml
2022-04-02T08:31:12.792+0100 [ERROR] 2022-04-02T08:31:12.792+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/1072507525/namespace.yaml
2022-04-02T08:31:13.335+0100 [ERROR] 2022-04-02T08:31:13.335+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/1072507525/secret.yaml
2022-04-02T08:31:13.340+0100 [ERROR] 2022-04-02T08:31:13.340+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/1072507525/rbac.yaml
2022-04-02T08:31:13.349+0100 [ERROR] 2022-04-02T08:31:13.349+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/1072507525/deployment.yaml
2022-04-02T08:31:13.366+0100 [ERROR] 2022-04-02T08:31:13.366+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:31:15.370+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:31:15.370+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [INFO]  Create Ingress: ref=web
2022-04-02T08:31:15.370+0100 [INFO]  Create Ingress: ref=consul-lan-serf
2022-04-02T08:31:15.370+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [INFO]  Create Ingress: ref=controller-webhook
2022-04-02T08:31:15.370+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [DEBUG] Calling connector to expose remote service: name=web local_port=9092 connector_addr=127.0.0.1:32633 local_addr=web.default.svc:9090
2022-04-02T08:31:15.370+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [DEBUG] Calling connector to expose remote service: name=consul-lan-serf local_port=8301 connector_addr=127.0.0.1:32633 local_addr=consul-server.consul.svc:8301
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [INFO]  Create Ingress: ref=upstreams-proxy
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [INFO]  Create Ingress: ref=consul-rpc
2022-04-02T08:31:15.371+0100 [INFO]  Create Ingress: ref=consul
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [DEBUG] Calling connector to expose remote service: name=consul-rpc local_port=8300 connector_addr=127.0.0.1:32633 local_addr=consul-server.consul.svc:8300
2022-04-02T08:31:15.371+0100 [DEBUG] Calling connector to expose remote service: name=consul local_port=8501 connector_addr=127.0.0.1:32633 local_addr=consul-server.consul.svc:8501
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [DEBUG] Calling connector to expose remote service: name=upstreams-proxy local_port=28080 connector_addr=127.0.0.1:32633 local_addr=consul-release-controller.default.svc:8080
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-1
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-1 local_port=18080 connector_addr=127.0.0.1:32633 local_addr=consul-ingress-gateway.consul.svc:18080
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [DEBUG] Calling connector to expose local service: name=controller-webhook remote_port=19443 connector_addr=127.0.0.1:32633 local_addr=localhost:19443
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.370+0100 [INFO]  Applying Kubernetes configuration: ref=consul_namespace config=["/home/nicj/.shipyard/data/consul/namespace.yaml"]
2022-04-02T08:31:15.370+0100 [INFO]  Applying Kubernetes configuration: ref=cert-manager-controller config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml"]
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-2
2022-04-02T08:31:15.371+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-2 local_port=18443 connector_addr=127.0.0.1:32633 local_addr=consul-ingress-gateway.consul.svc:18443
2022-04-02T08:31:15.372+0100 [ERROR] 2022-04-02T08:31:15.371+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:31:15.372+0100 [ERROR] 2022-04-02T08:31:15.372+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml
2022-04-02T08:31:15.392+0100 [ERROR] 2022-04-02T08:31:15.392+0100 [DEBUG] Successfully exposed service: id=c4bff055-8fd0-46bf-8cb5-7787f96a3772
2022-04-02T08:31:15.392+0100 [DEBUG] Successfully exposed service: id=44c19ea1-1b6a-4b0a-be29-9283b735100e
2022-04-02T08:31:15.392+0100 [DEBUG] Successfully exposed service: id=50ef2855-746f-46aa-8f89-71c56c2c7c3f
2022-04-02T08:31:15.393+0100 [ERROR] 2022-04-02T08:31:15.393+0100 [DEBUG] Successfully exposed service: id=d9c5944a-fed4-45a9-947c-8ba09c344433
2022-04-02T08:31:15.393+0100 [DEBUG] Successfully exposed service: id=2c265d3f-1dd7-44da-8382-3fba078e4643
2022-04-02T08:31:15.394+0100 [ERROR] 2022-04-02T08:31:15.394+0100 [DEBUG] Successfully exposed service: id=865e314c-8a34-4980-b6d3-97dfe035f7b9
2022-04-02T08:31:15.394+0100 [ERROR] 2022-04-02T08:31:15.394+0100 [DEBUG] Successfully exposed service: id=1444b229-e58a-4259-b750-bd184bcb6678
2022-04-02T08:31:15.394+0100 [ERROR] 2022-04-02T08:31:15.394+0100 [DEBUG] Successfully exposed service: id=faa09547-d6be-4c53-9b5b-6466a248686b
2022-04-02T08:31:15.424+0100 [ERROR] 2022-04-02T08:31:15.424+0100 [INFO]  Creating Helm chart: ref=consul
2022-04-02T08:31:15.424+0100 [DEBUG] Updating Helm chart repository: name=hashicorp url=https://helm.releases.hashicorp.com
2022-04-02T08:31:15.538+0100 [ERROR] 2022-04-02T08:31:15.538+0100 [DEBUG] Using Kubernetes config: ref=consul path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:31:15.539+0100 [ERROR] 2022-04-02T08:31:15.538+0100 [DEBUG] Creating chart from config: ref=consul chart=hashicorp/consul
2022-04-02T08:31:15.650+0100 [ERROR] 2022-04-02T08:31:15.650+0100 [DEBUG] Loading chart: ref=consul path=/home/nicj/.shipyard/helm_charts/cache/consul-0.40.0.tgz
2022-04-02T08:31:15.655+0100 [ERROR] 2022-04-02T08:31:15.655+0100 [DEBUG] Using Values: ref=consul
  values=
  | map[connectInject:map[centralConfig:map[enabled:true] default:false enabled:true envoyExtraArgs:<nil> failurePolicy:Ignore replicas:1 transparentProxy:map[defaultEnabled:false]] controller:map[enabled:true] global:map[acls:map[manageSystemACLs:true] datacenter:dc1 federation:map[createFederationSecret:false enabled:false] image:hashicorp/consul:1.11.3 imageEnvoy:envoyproxy/envoy:v1.20.1 imageK8S:hashicorp/consul-k8s-control-plane:0.40.0 logLevel:info metrics:map[enableAgentMetrics:true enableGatewayMetrics:true enabled:true] name:consul tls:map[enableAutoEncrypt:true enabled:true httpsOnly:false]] ingressGateways:map[defaults:map[replicas:1 service:map[ports:[map[nodePort:<nil> port:18080] map[nodePort:<nil> port:18443]]]] enabled:true] meshGateway:map[enabled:false replicas:1 service:map[enabled:false nodePort:30443 type:NodePort] wanAddress:map[port:30443 source:Static static:dc1.k8s-cluster.shipyard.run]] server:map[bootstrapExpect:1 extraConfig:{
  |   "ui_config": {
  |     "enabled": true,
  |     "metrics_provider": "prometheus",
  |     "metrics_proxy": {
  |       "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |     }
  |   }
  | }
  |  replicas:1 storage:128Mi] ui:map[enabled:true]]
  
2022-04-02T08:31:15.655+0100 [DEBUG] Validate chart: ref=consul
2022-04-02T08:31:15.655+0100 [DEBUG] Run chart: ref=consul
2022-04-02T08:31:15.767+0100 [ERROR] 2022-04-02T08:31:15.767+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:31:16.311+0100 [ERROR] 2022-04-02T08:31:16.311+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" ServiceAccount"
2022-04-02T08:31:16.314+0100 [ERROR] 2022-04-02T08:31:16.314+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="serviceaccounts \"consul-tls-init\" not found"
2022-04-02T08:31:16.373+0100 [ERROR] 2022-04-02T08:31:16.373+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:31:16.378+0100 [ERROR] 2022-04-02T08:31:16.377+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Role"
2022-04-02T08:31:16.380+0100 [ERROR] 2022-04-02T08:31:16.380+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="roles.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:31:16.431+0100 [ERROR] 2022-04-02T08:31:16.431+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:31:16.436+0100 [ERROR] 2022-04-02T08:31:16.436+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" RoleBinding"
2022-04-02T08:31:16.438+0100 [ERROR] 2022-04-02T08:31:16.438+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="rolebindings.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:31:16.491+0100 [ERROR] 2022-04-02T08:31:16.490+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:31:16.495+0100 [ERROR] 2022-04-02T08:31:16.495+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:31:16.497+0100 [ERROR] 2022-04-02T08:31:16.497+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="jobs.batch \"consul-tls-init\" not found"
2022-04-02T08:31:16.548+0100 [ERROR] 2022-04-02T08:31:16.548+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:31:16.555+0100 [ERROR] 2022-04-02T08:31:16.555+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-tls-init with timeout of 0s"
2022-04-02T08:31:16.559+0100 [ERROR] 2022-04-02T08:31:16.559+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: ADDED"
2022-04-02T08:31:16.559+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:16.569+0100 [ERROR] 2022-04-02T08:31:16.569+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:31:16.569+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:17.772+0100 [ERROR] 2022-04-02T08:31:17.772+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-cainjector-7974c84449-zw4v2 namespace=cert-manager status=Pending
2022-04-02T08:31:19.044+0100 [ERROR] 2022-04-02T08:31:19.044+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:31:19.044+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:19.053+0100 [ERROR] 2022-04-02T08:31:19.053+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:31:19.055+0100 [ERROR] 2022-04-02T08:31:19.055+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:31:19.059+0100 [ERROR] 2022-04-02T08:31:19.059+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 58 resource(s)"
2022-04-02T08:31:19.517+0100 [ERROR] 2022-04-02T08:31:19.517+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:31:19.537+0100 [ERROR] 2022-04-02T08:31:19.537+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-server-acl-init-cleanup with timeout of 0s"
2022-04-02T08:31:19.541+0100 [ERROR] 2022-04-02T08:31:19.541+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: ADDED"
2022-04-02T08:31:19.541+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:19.571+0100 [ERROR] 2022-04-02T08:31:19.571+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:31:19.571+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:19.776+0100 [ERROR] 2022-04-02T08:31:19.776+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-4mdh5 namespace=cert-manager type=Ready value=False
2022-04-02T08:31:21.781+0100 [ERROR] 2022-04-02T08:31:21.781+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-4mdh5 namespace=cert-manager type=Ready value=False
2022-04-02T08:31:21.894+0100 [ERROR] 2022-04-02T08:31:21.894+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 45.000098]
2022-04-02T08:31:23.786+0100 [ERROR] 2022-04-02T08:31:23.786+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-4mdh5 namespace=cert-manager type=Ready value=False
2022-04-02T08:31:25.791+0100 [ERROR] 2022-04-02T08:31:25.791+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-4mdh5 namespace=cert-manager type=Ready value=False
2022-04-02T08:31:27.796+0100 [ERROR] 2022-04-02T08:31:27.796+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-4mdh5 namespace=cert-manager type=Ready value=False
2022-04-02T08:31:29.802+0100 [ERROR] 2022-04-02T08:31:29.802+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:31:36.895+0100 [ERROR] 2022-04-02T08:31:36.895+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 60.000289]
2022-04-02T08:31:38.440+0100 [ERROR] 2022-04-02T08:31:38.440+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:31:38.440+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:38.447+0100 [ERROR] 2022-04-02T08:31:38.447+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:31:38.449+0100 [ERROR] 2022-04-02T08:31:38.449+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-server-acl-init-cleanup\" Job"
2022-04-02T08:31:38.504+0100 [ERROR] 2022-04-02T08:31:38.504+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:31:40.509+0100 [ERROR] 2022-04-02T08:31:40.509+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-bhkhs namespace=consul status=Pending
2022-04-02T08:31:42.514+0100 [ERROR] 2022-04-02T08:31:42.514+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-bhkhs namespace=consul type=Ready value=False
2022-04-02T08:31:44.519+0100 [ERROR] 2022-04-02T08:31:44.519+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:31:44.519+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:31:46.523+0100 [ERROR] 2022-04-02T08:31:46.523+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:31:46.523+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:31:48.529+0100 [ERROR] 2022-04-02T08:31:48.529+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:31:48.529+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [INFO]  Creating Helm chart: ref=prometheus
2022-04-02T08:31:50.533+0100 [INFO]  Create Ingress: ref=zipkin
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [DEBUG] Updating Helm chart repository: name=prometheus url=https://prometheus-community.github.io/helm-charts
2022-04-02T08:31:50.533+0100 [INFO]  Generating template: ref=fetch_consul_resources output=/home/nicj/.shipyard/data/consul_kubernetes/fetch.sh
2022-04-02T08:31:50.533+0100 [INFO]  Generating template: ref=monitoring_namespace output=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:31:50.533+0100 [DEBUG] Template content: ref=monitoring_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [DEBUG] Template content: ref=fetch_consul_resources
  source=
  |   #!/bin/sh -e
  | 
  |   echo "Port #{{ .Vars.port }}"
  |   echo "Fetching resources from running cluster, acls_enabled: #{{ .Vars.acl_enabled }}, tls_enabled #{{ .Vars.tls_enabled }}"
  | 
  |   #{{ if eq .Vars.acl_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   #{{end}}
  |   
  |   #{{ if eq .Vars.tls_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |   #{{end}}
  
2022-04-02T08:31:50.533+0100 [INFO]  Create Ingress: ref=prometheus
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [INFO]  Generating template: ref=grafana_secret_template output=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:31:50.533+0100 [INFO]  Applying Kubernetes configuration: ref=consul_defaults config=["/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml"]
2022-04-02T08:31:50.533+0100 [DEBUG] Calling connector to expose remote service: name=prometheus local_port=9090 connector_addr=127.0.0.1:32633 local_addr=prometheus-operated.monitoring.svc:9090
2022-04-02T08:31:50.533+0100 [DEBUG] Calling connector to expose remote service: name=zipkin local_port=9411 connector_addr=127.0.0.1:32633 local_addr=tempo.monitoring.svc:9411
2022-04-02T08:31:50.533+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [INFO]  Create Ingress: ref=grafana
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [INFO]  Generating template: ref=prometheus_operator_template output=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:31:50.534+0100 [DEBUG] Template content: ref=prometheus_operator_template
  source=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [DEBUG] Template output: ref=monitoring_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [DEBUG] Template output: ref=fetch_consul_resources
  destination=
  |   #!/bin/sh -e
  | 
  |   echo "Port 8501"
  |   echo "Fetching resources from running cluster, acls_enabled: true, tls_enabled true"
  | 
  |   
  |   kubectl get secret -n consul -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   
  |   
  |   
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [INFO]  Applying Kubernetes configuration: ref=monitoring_namespace config=["/home/nicj/.shipyard/data/monitoring/namespace.yaml"]
2022-04-02T08:31:50.534+0100 [INFO]  Remote executing command: ref=fetch_consul_resources command=sh args=["/data/fetch.sh"] image="&{shipyardrun/tools:v0.5.0  }"
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [INFO]  Create Ingress: ref=tempo
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [DEBUG] Calling connector to expose remote service: name=tempo local_port=3100 connector_addr=127.0.0.1:32633 local_addr=tempo.default.svc:3100
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.533+0100 [DEBUG] Template content: ref=grafana_secret_template
  source=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [DEBUG] Calling connector to expose remote service: name=grafana local_port=8080 connector_addr=127.0.0.1:32633 local_addr=grafana.monitoring.svc:80
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [DEBUG] Template output: ref=grafana_secret_template
  destination=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: monitoring
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
  
2022-04-02T08:31:50.534+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:31:50.534+0100 [DEBUG] Template output: ref=prometheus_operator_template
  destination=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:31:50.534+0100 [ERROR] 2022-04-02T08:31:50.534+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:31:50.551+0100 [ERROR] 2022-04-02T08:31:50.551+0100 [DEBUG] Successfully exposed service: id=5b3be5cd-d3e8-477c-a5e0-7ae51aabcc06
2022-04-02T08:31:50.551+0100 [ERROR] 2022-04-02T08:31:50.551+0100 [DEBUG] Successfully exposed service: id=81268bfc-ed90-4b13-9694-d8a69a89c8e3
2022-04-02T08:31:50.556+0100 [ERROR] 2022-04-02T08:31:50.556+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.5.0
2022-04-02T08:31:50.556+0100 [DEBUG] Creating Docker Container: ref=fetch_consul_resources.remote_exec
2022-04-02T08:31:50.559+0100 [ERROR] 2022-04-02T08:31:50.558+0100 [DEBUG] Successfully exposed service: id=e3a0bcf3-5aaf-40d9-b5eb-c079afdbfbd0
2022-04-02T08:31:50.561+0100 [ERROR] 2022-04-02T08:31:50.561+0100 [DEBUG] Successfully exposed service: id=15a3045e-533a-40e6-a6b4-2b5303849bb3
2022-04-02T08:31:50.609+0100 [ERROR] 2022-04-02T08:31:50.609+0100 [DEBUG] Remove container from default networks: ref=fetch_consul_resources.remote_exec
2022-04-02T08:31:50.612+0100 [ERROR] 2022-04-02T08:31:50.612+0100 [DEBUG] Attaching container to network: ref=5a408b458acc1a3a690a8b2b43107fcfa9feb1ab8be03017917c36bdf04403a2 network=dc1
2022-04-02T08:31:50.620+0100 [ERROR] 2022-04-02T08:31:50.620+0100 [DEBUG] Disconnectng network: name=bridge ref=fetch_consul_resources.remote_exec
2022-04-02T08:31:50.943+0100 [ERROR] 2022-04-02T08:31:50.943+0100 [DEBUG] Using Kubernetes config: ref=prometheus path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:31:50.943+0100 [ERROR] 2022-04-02T08:31:50.943+0100 [DEBUG] Creating chart from config: ref=prometheus chart=prometheus/kube-prometheus-stack
2022-04-02T08:31:51.112+0100 [ERROR] 2022-04-02T08:31:51.111+0100 [INFO]  Applying Kubernetes configuration: ref=grafana_secret config=["/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml"]
2022-04-02T08:31:51.112+0100 [ERROR] 2022-04-02T08:31:51.112+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:31:51.370+0100 [ERROR] 2022-04-02T08:31:51.370+0100 [DEBUG] Port 8501
Fetching resources from running cluster, acls_enabled: true, tls_enabled true
2022-04-02T08:31:51.755+0100 [ERROR] 2022-04-02T08:31:51.754+0100 [DEBUG] Forcefully remove: container=5a408b458acc1a3a690a8b2b43107fcfa9feb1ab8be03017917c36bdf04403a2
2022-04-02T08:31:51.781+0100 [ERROR] 2022-04-02T08:31:51.781+0100 [DEBUG] Loading chart: ref=prometheus path=/home/nicj/.shipyard/helm_charts/cache/kube-prometheus-stack-32.0.0.tgz
2022-04-02T08:31:51.796+0100 [ERROR] 2022-04-02T08:31:51.796+0100 [DEBUG] Using Values: ref=prometheus values="map[alertmanager:map[enabled:false] defaultRules:map[create:false] grafana:map[enabled:false] serviceMonitor:map[enabled:false]]"
2022-04-02T08:31:51.796+0100 [DEBUG] Validate chart: ref=prometheus
2022-04-02T08:31:51.796+0100 [DEBUG] Run chart: ref=prometheus
2022-04-02T08:31:51.813+0100 [ERROR] 2022-04-02T08:31:51.813+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:51.863+0100 [ERROR] 2022-04-02T08:31:51.863+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:51.895+0100 [ERROR] 2022-04-02T08:31:51.895+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 75.000272]
2022-04-02T08:31:51.909+0100 [ERROR] 2022-04-02T08:31:51.909+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:51.922+0100 [ERROR] 2022-04-02T08:31:51.921+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:51.957+0100 [ERROR] 2022-04-02T08:31:51.957+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:52.044+0100 [ERROR] 2022-04-02T08:31:52.044+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:52.055+0100 [ERROR] 2022-04-02T08:31:52.055+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:52.089+0100 [ERROR] 2022-04-02T08:31:52.089+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:52.162+0100 [ERROR] 2022-04-02T08:31:52.162+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Clearing discovery cache"
2022-04-02T08:31:52.162+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="beginning wait for 8 resources with timeout of 1m0s"
2022-04-02T08:31:53.367+0100 [ERROR] 2022-04-02T08:31:53.367+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:53.661+0100 [ERROR] 2022-04-02T08:31:53.661+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:31:53.680+0100 [ERROR] 2022-04-02T08:31:53.680+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:53.964+0100 [ERROR] 2022-04-02T08:31:53.964+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:53.968+0100 [ERROR] 2022-04-02T08:31:53.968+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:31:53.971+0100 [ERROR] 2022-04-02T08:31:53.971+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:54.251+0100 [ERROR] 2022-04-02T08:31:54.251+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:54.256+0100 [ERROR] 2022-04-02T08:31:54.256+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:31:54.259+0100 [ERROR] 2022-04-02T08:31:54.258+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:54.543+0100 [ERROR] 2022-04-02T08:31:54.543+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:54.549+0100 [ERROR] 2022-04-02T08:31:54.549+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:31:54.551+0100 [ERROR] 2022-04-02T08:31:54.551+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:54.831+0100 [ERROR] 2022-04-02T08:31:54.831+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:54.836+0100 [ERROR] 2022-04-02T08:31:54.836+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:31:54.839+0100 [ERROR] 2022-04-02T08:31:54.839+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:55.122+0100 [ERROR] 2022-04-02T08:31:55.122+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:55.127+0100 [ERROR] 2022-04-02T08:31:55.127+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:31:55.130+0100 [ERROR] 2022-04-02T08:31:55.130+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-create\" not found"
2022-04-02T08:31:55.420+0100 [ERROR] 2022-04-02T08:31:55.420+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:55.425+0100 [ERROR] 2022-04-02T08:31:55.425+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-create with timeout of 0s"
2022-04-02T08:31:55.428+0100 [ERROR] 2022-04-02T08:31:55.427+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: ADDED"
2022-04-02T08:31:55.427+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:55.445+0100 [ERROR] 2022-04-02T08:31:55.445+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:31:55.445+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:57.170+0100 [ERROR] 2022-04-02T08:31:57.170+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:31:57.170+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:57.178+0100 [ERROR] 2022-04-02T08:31:57.177+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:31:57.179+0100 [ERROR] 2022-04-02T08:31:57.179+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:31:57.184+0100 [ERROR] 2022-04-02T08:31:57.184+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:31:57.193+0100 [ERROR] 2022-04-02T08:31:57.193+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:31:57.198+0100 [ERROR] 2022-04-02T08:31:57.198+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:31:57.203+0100 [ERROR] 2022-04-02T08:31:57.203+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:31:57.209+0100 [ERROR] 2022-04-02T08:31:57.209+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:31:57.213+0100 [ERROR] 2022-04-02T08:31:57.213+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 36 resource(s)"
2022-04-02T08:31:57.402+0100 [ERROR] 2022-04-02T08:31:57.402+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:31:57.404+0100 [ERROR] 2022-04-02T08:31:57.404+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:57.680+0100 [ERROR] 2022-04-02T08:31:57.680+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:57.685+0100 [ERROR] 2022-04-02T08:31:57.685+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:31:57.688+0100 [ERROR] 2022-04-02T08:31:57.688+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:57.969+0100 [ERROR] 2022-04-02T08:31:57.969+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:57.974+0100 [ERROR] 2022-04-02T08:31:57.974+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:31:57.977+0100 [ERROR] 2022-04-02T08:31:57.977+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:58.281+0100 [ERROR] 2022-04-02T08:31:58.281+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:58.288+0100 [ERROR] 2022-04-02T08:31:58.288+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:31:58.290+0100 [ERROR] 2022-04-02T08:31:58.290+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:58.573+0100 [ERROR] 2022-04-02T08:31:58.573+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:58.578+0100 [ERROR] 2022-04-02T08:31:58.578+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:31:58.581+0100 [ERROR] 2022-04-02T08:31:58.581+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:31:58.867+0100 [ERROR] 2022-04-02T08:31:58.867+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:58.873+0100 [ERROR] 2022-04-02T08:31:58.873+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:31:58.876+0100 [ERROR] 2022-04-02T08:31:58.876+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-patch\" not found"
2022-04-02T08:31:59.173+0100 [ERROR] 2022-04-02T08:31:59.173+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:31:59.177+0100 [ERROR] 2022-04-02T08:31:59.177+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-patch with timeout of 0s"
2022-04-02T08:31:59.180+0100 [ERROR] 2022-04-02T08:31:59.180+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: ADDED"
2022-04-02T08:31:59.180+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:31:59.199+0100 [ERROR] 2022-04-02T08:31:59.199+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:31:59.199+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:32:02.368+0100 [ERROR] 2022-04-02T08:32:02.368+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:32:02.368+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:32:02.375+0100 [ERROR] 2022-04-02T08:32:02.375+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:32:02.377+0100 [ERROR] 2022-04-02T08:32:02.377+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:32:02.383+0100 [ERROR] 2022-04-02T08:32:02.383+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:32:02.394+0100 [ERROR] 2022-04-02T08:32:02.394+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:32:02.400+0100 [ERROR] 2022-04-02T08:32:02.400+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:32:02.405+0100 [ERROR] 2022-04-02T08:32:02.405+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:32:02.410+0100 [ERROR] 2022-04-02T08:32:02.410+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:32:02.696+0100 [ERROR] 2022-04-02T08:32:02.696+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:32:04.703+0100 [ERROR] 2022-04-02T08:32:04.702+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-s58cf namespace=monitoring type=Ready value=False
2022-04-02T08:32:06.709+0100 [ERROR] 2022-04-02T08:32:06.708+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-s58cf namespace=monitoring type=Ready value=False
2022-04-02T08:32:06.895+0100 [ERROR] 2022-04-02T08:32:06.895+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 90.001048]
2022-04-02T08:32:08.714+0100 [ERROR] 2022-04-02T08:32:08.714+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:32:08.714+0100 [ERROR] 2022-04-02T08:32:08.714+0100 [INFO]  Creating Helm chart: ref=loki
2022-04-02T08:32:08.714+0100 [ERROR] 2022-04-02T08:32:08.714+0100 [INFO]  Applying Kubernetes configuration: ref=prometheus config=["/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml"]
2022-04-02T08:32:08.715+0100 [ERROR] 2022-04-02T08:32:08.714+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:32:08.715+0100 [ERROR] 2022-04-02T08:32:08.715+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:32:08.937+0100 [ERROR] 2022-04-02T08:32:08.937+0100 [DEBUG] Using Kubernetes config: ref=loki path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:32:08.937+0100 [ERROR] 2022-04-02T08:32:08.937+0100 [DEBUG] Creating chart from config: ref=loki chart=grafana/loki
2022-04-02T08:32:09.608+0100 [ERROR] 2022-04-02T08:32:09.608+0100 [DEBUG] Loading chart: ref=loki path=/home/nicj/.shipyard/helm_charts/cache/loki-2.9.1.tgz
2022-04-02T08:32:09.609+0100 [ERROR] 2022-04-02T08:32:09.609+0100 [DEBUG] Using Values: ref=loki values=map[]
2022-04-02T08:32:09.609+0100 [DEBUG] Validate chart: ref=loki
2022-04-02T08:32:09.609+0100 [DEBUG] Run chart: ref=loki
2022-04-02T08:32:09.827+0100 [ERROR] W0402 08:32:09.827488    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:32:09.841+0100 [ERROR] 2022-04-02T08:32:09.841+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 1 resource(s)"
2022-04-02T08:32:09.850+0100 [ERROR] 2022-04-02T08:32:09.850+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 8 resource(s)"
2022-04-02T08:32:09.854+0100 [ERROR] W0402 08:32:09.854588    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:32:09.884+0100 [ERROR] 2022-04-02T08:32:09.884+0100 [INFO]  Creating Helm chart: ref=promtail
2022-04-02T08:32:09.884+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:32:09.884+0100 [DEBUG] Using Kubernetes config: ref=promtail path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:32:09.884+0100 [ERROR] 2022-04-02T08:32:09.884+0100 [DEBUG] Creating chart from config: ref=promtail chart=grafana/promtail
2022-04-02T08:32:10.539+0100 [ERROR] 2022-04-02T08:32:10.539+0100 [DEBUG] Loading chart: ref=promtail path=/home/nicj/.shipyard/helm_charts/cache/promtail-3.11.0.tgz
2022-04-02T08:32:10.540+0100 [ERROR] 2022-04-02T08:32:10.540+0100 [DEBUG] Using Values: ref=promtail values=map[config:map[lokiAddress:http://loki:3100/loki/api/v1/push]]
2022-04-02T08:32:10.540+0100 [DEBUG] Validate chart: ref=promtail
2022-04-02T08:32:10.540+0100 [DEBUG] Run chart: ref=promtail
2022-04-02T08:32:10.826+0100 [ERROR] 2022-04-02T08:32:10.825+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 1 resource(s)"
2022-04-02T08:32:10.835+0100 [ERROR] 2022-04-02T08:32:10.835+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 5 resource(s)"
2022-04-02T08:32:10.859+0100 [ERROR] 2022-04-02T08:32:10.859+0100 [INFO]  Creating Helm chart: ref=tempo
2022-04-02T08:32:10.859+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:32:10.859+0100 [ERROR] 2022-04-02T08:32:10.859+0100 [DEBUG] Using Kubernetes config: ref=tempo path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:32:10.860+0100 [ERROR] 2022-04-02T08:32:10.860+0100 [DEBUG] Creating chart from config: ref=tempo chart=grafana/tempo
2022-04-02T08:32:11.665+0100 [ERROR] 2022-04-02T08:32:11.664+0100 [DEBUG] Loading chart: ref=tempo path=/home/nicj/.shipyard/helm_charts/cache/tempo-0.13.1.tgz
2022-04-02T08:32:11.665+0100 [ERROR] 2022-04-02T08:32:11.665+0100 [DEBUG] Using Values: ref=tempo values="map[tempo:map[receivers:map[jaeger:map[protocols:map[grpc:map[endpoint:0.0.0.0:14250] thrift_binary:map[endpoint:0.0.0.0:6832] thrift_compact:map[endpoint:0.0.0.0:6831] thrift_http:map[endpoint:0.0.0.0:14268]]] zipkin:map[]]]]"
2022-04-02T08:32:11.665+0100 [DEBUG] Validate chart: ref=tempo
2022-04-02T08:32:11.665+0100 [DEBUG] Run chart: ref=tempo
2022-04-02T08:32:11.940+0100 [ERROR] 2022-04-02T08:32:11.940+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 1 resource(s)"
2022-04-02T08:32:11.951+0100 [ERROR] 2022-04-02T08:32:11.951+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 5 resource(s)"
2022-04-02T08:32:11.991+0100 [ERROR] 2022-04-02T08:32:11.991+0100 [INFO]  Creating Helm chart: ref=grafana
2022-04-02T08:32:11.991+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:32:11.991+0100 [DEBUG] Using Kubernetes config: ref=grafana path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:32:11.992+0100 [ERROR] 2022-04-02T08:32:11.991+0100 [DEBUG] Creating chart from config: ref=grafana chart=grafana/grafana
2022-04-02T08:32:12.327+0100 [ERROR] 2022-04-02T08:32:12.327+0100 [DEBUG] Loading chart: ref=grafana path=/home/nicj/.shipyard/helm_charts/cache/grafana-6.21.2.tgz
2022-04-02T08:32:12.329+0100 [ERROR] 2022-04-02T08:32:12.329+0100 [DEBUG] Using Values: ref=grafana values="map[admin:map[existingSecret:grafana-password] datasources:map[datasources.yaml:map[apiVersion:1 datasources:[map[isDefault:true name:Prometheus type:prometheus url:http://prometheus-kube-prometheus-prometheus:9090] map[isDefault:false jsonData:map[derivedFields:[map[datasourceUid:tempo_uid matcherRegex:trace_id=(\\w+) name:trace_id url:$${__value.raw}]] maxLines:1000] name:Loki type:loki uid:loki_uid url:http://loki:3100] map[isDefault:false name:Tempo type:tempo uid:tempo_uid url:http://tempo:3100]]]] sidecar:map[dashboards:map[enabled:true]]]"
2022-04-02T08:32:12.329+0100 [DEBUG] Validate chart: ref=grafana
2022-04-02T08:32:12.329+0100 [DEBUG] Run chart: ref=grafana
2022-04-02T08:32:12.644+0100 [ERROR] W0402 08:32:12.644421    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:32:12.647+0100 [ERROR] W0402 08:32:12.646942    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:32:12.675+0100 [ERROR] 2022-04-02T08:32:12.675+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 1 resource(s)"
2022-04-02T08:32:12.694+0100 [ERROR] 2022-04-02T08:32:12.693+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 15 resource(s)"
2022-04-02T08:32:12.698+0100 [ERROR] W0402 08:32:12.698355    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0402 08:32:12.698384    9887 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:32:12.770+0100 [ERROR] 2022-04-02T08:32:12.770+0100 [INFO]  Generating template: ref=monitor_ingress_gateway output=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:32:12.770+0100 [DEBUG] Template content: ref=monitor_ingress_gateway
  source=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: #{{ .Vars.consul_namespace }}
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:32:12.771+0100 [ERROR] 2022-04-02T08:32:12.771+0100 [DEBUG] Template output: ref=monitor_ingress_gateway
  destination=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: monitoring
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: consul
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:32:12.771+0100 [ERROR] 2022-04-02T08:32:12.771+0100 [INFO]  Applying Kubernetes configuration: ref=monitor_ingress_gateway config=["/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml"]
2022-04-02T08:32:12.771+0100 [ERROR] 2022-04-02T08:32:12.771+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:32:12.853+0100 [ERROR] 2022-04-02T08:32:12.853+0100 [INFO]  Applying Kubernetes configuration: ref=application config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/../../example/kubernetes/"]
2022-04-02T08:32:12.853+0100 [INFO]  Applying Kubernetes configuration: ref=upstreams-proxy config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml"]
2022-04-02T08:32:12.853+0100 [INFO]  Creating Helm chart: ref=consul-release-controller
2022-04-02T08:32:12.853+0100 [ERROR] 2022-04-02T08:32:12.853+0100 [DEBUG] Using Kubernetes config: ref=consul-release-controller path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:32:12.854+0100 [ERROR] 2022-04-02T08:32:12.854+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml
2022-04-02T08:32:12.854+0100 [DEBUG] Creating chart from config: ref=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:32:12.854+0100 [DEBUG] Loading chart: ref=consul-release-controller path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:32:12.854+0100 [ERROR] 2022-04-02T08:32:12.854+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/api.yaml
2022-04-02T08:32:12.855+0100 [ERROR] 2022-04-02T08:32:12.855+0100 [DEBUG] Using Values: ref=consul-release-controller values="map[acls:map[enabled:true] autoencrypt:map[enabled:true] controller:map[container_config:map[image:map[repository:nicholasjackson/consul-release-controller tag:]] enabled:false] webhook:map[namespace:shipyard service:controller-webhook]]"
2022-04-02T08:32:12.855+0100 [DEBUG] Validate chart: ref=consul-release-controller
2022-04-02T08:32:12.855+0100 [DEBUG] Run chart: ref=consul-release-controller
2022-04-02T08:32:12.978+0100 [ERROR] 2022-04-02T08:32:12.977+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/application-dashboard.yaml
2022-04-02T08:32:12.993+0100 [ERROR] 2022-04-02T08:32:12.993+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/consul-config.yaml
2022-04-02T08:32:13.049+0100 [ERROR] 2022-04-02T08:32:13.049+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest-dashboard.yaml
2022-04-02T08:32:13.064+0100 [ERROR] 2022-04-02T08:32:13.064+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest.yaml
2022-04-02T08:32:13.148+0100 [ERROR] 2022-04-02T08:32:13.148+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/metrics.yaml
2022-04-02T08:32:13.156+0100 [ERROR] 2022-04-02T08:32:13.156+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/web.yaml
2022-04-02T08:32:13.236+0100 [ERROR] 2022-04-02T08:32:13.236+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 1 resource(s)"
2022-04-02T08:32:13.247+0100 [ERROR] 2022-04-02T08:32:13.247+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 13 resource(s)"
2022-04-02T08:32:13.376+0100 [ERROR] 2022-04-02T08:32:13.375+0100 [INFO]  Remote executing command: ref=exec_standalone command=sh args=["/output/fetch_certs.sh"] image="&{shipyardrun/tools:v0.6.0  }"
2022-04-02T08:32:13.404+0100 [ERROR] 2022-04-02T08:32:13.404+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.6.0
2022-04-02T08:32:13.404+0100 [DEBUG] Creating Docker Container: ref=exec_standalone.remote_exec
2022-04-02T08:32:13.674+0100 [ERROR] 2022-04-02T08:32:13.674+0100 [DEBUG] Remove container from default networks: ref=exec_standalone.remote_exec
2022-04-02T08:32:13.678+0100 [ERROR] 2022-04-02T08:32:13.678+0100 [DEBUG] Attaching container to network: ref=6a3e025fc3c613189828d689766e3dcce0db016edd7ba4ad7490a81ab03920af network=dc1
2022-04-02T08:32:13.686+0100 [ERROR] 2022-04-02T08:32:13.686+0100 [DEBUG] Disconnectng network: name=bridge ref=exec_standalone.remote_exec
2022-04-02T08:32:14.813+0100 [ERROR] 2022-04-02T08:32:14.812+0100 [DEBUG] Forcefully remove: container=6a3e025fc3c613189828d689766e3dcce0db016edd7ba4ad7490a81ab03920af
2022-04-02T08:32:15.499+0100 [ERROR] 2022-04-02T08:32:15.499+0100 [DEBUG] Health check urls for browser windows: count=0
2022-04-02T08:32:15.499+0100 [DEBUG] Browser windows open

########################################################

Title Development setup
Author Nic Jackson
2022-04-02T08:32:15.499+0100 [ERROR] 
• Consul: https://localhost:8501
• Grafana: https://localhost:8080
• Application: http://localhost:18080
2022-04-02T08:32:15.499+0100 [ERROR] 
2022-04-02T08:32:15.499+0100 [ERROR] This blueprint defines 13 output variables.
2022-04-02T08:32:15.499+0100 [ERROR] 
You can set output variables as environment variables for your current terminal session using the following command:
2022-04-02T08:32:15.499+0100 [ERROR] eval $(shipyard env)

To list output variables use the command:

shipyard output
2022-04-02T08:32:16.122+0100 [INFO]  Starting controller
2022-04-02T08:32:20.658+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:32:20.658+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_fail
2022-04-02T08:32:20.658+0100 [DEBUG] statemachine: Handle event: event=event_deploy state=state_fail
2022-04-02T08:32:20.658+0100 [DEBUG] statemachine: Log state: event=event_deploy state=state_fail
2022-04-02T08:32:20.658+0100 [DEBUG] statemachine: Deploy: state=state_deploy
2022-04-02T08:32:20.658+0100 [DEBUG] statemachine: Log state: event=event_deploy release=api state=state_deploy
2022-04-02T08:32:20.667+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:20.671+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:32:20.671+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:21.672+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:21.675+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:21.675+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:22.676+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:22.679+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:22.679+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:23.679+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:23.682+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:23.682+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:24.683+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:24.685+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:24.685+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:25.658+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-02T08:32:25.659+0100 [DEBUG] runtime-plugin-kubernetes: No candidate deployment, nothing to do
2022-04-02T08:32:25.659+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:32:25.662+0100 [DEBUG] releaser-plugin-consul: Service not healthy, retrying: name=api
2022-04-02T08:32:25.662+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:32:25.665+0100 [ERROR] releaser-plugin-consul: Unable to create Consul ServiceSplitter: name=api error="Put \"https://127.0.0.1:8501/v1/config\": x509: certificate signed by unknown authority (possibly because of \"x509: ECDSA verification failure\" while trying to verify candidate authority certificate \"Consul Agent CA\")"
2022-04-02T08:32:25.665+0100 [ERROR] statemachine: Deploy completed with error: error="Put \"https://127.0.0.1:8501/v1/config\": x509: certificate signed by unknown authority (possibly because of \"x509: ECDSA verification failure\" while trying to verify candidate authority certificate \"Consul Agent CA\")"
2022-04-02T08:32:25.665+0100 [DEBUG] statemachine: Handle event: event=event_fail state=state_deploy
2022-04-02T08:32:25.665+0100 [DEBUG] statemachine: Log state: event=event_fail state=state_deploy
2022-04-02T08:32:25.665+0100 [DEBUG] statemachine: Log state: event=event_fail release=api state=state_fail
2022-04-02T08:32:25.685+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:25.689+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:25.689+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:26.689+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:26.692+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:26.692+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:27.692+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:27.695+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:27.695+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:28.695+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:28.698+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:28.698+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:29.699+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:29.702+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:29.702+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:30.703+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:30.707+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:30.707+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:31.707+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:31.711+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:31.711+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:32.712+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:32.715+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:32.715+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:33.715+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:33.718+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:33.718+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:34.718+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:34.722+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:34.722+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:35.722+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:35.727+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:35.727+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:36.728+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:36.731+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:36.731+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:37.731+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:37.734+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:37.734+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:38.735+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:38.739+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:38.739+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:39.740+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:39.743+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:39.743+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:40.743+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:40.747+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:40.747+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:41.747+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:41.750+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:41.750+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:42.750+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:42.753+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:42.753+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:43.754+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:43.757+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:43.757+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:44.757+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:44.760+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:32:44.760+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:32:45.760+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:45.763+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:32:45.763+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:32:45.774+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:32:45.777+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:32:45.777+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:33:15.836+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:15.839+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:15.842+0100 [DEBUG] statemachine: Handle event: event=event_configure state=state_start
2022-04-02T08:33:15.842+0100 [DEBUG] statemachine: Log state: event=event_configure state=state_start
2022-04-02T08:33:15.842+0100 [DEBUG] statemachine: Configure: state=state_configure
2022-04-02T08:33:15.842+0100 [DEBUG] statemachine: Log state: event=event_configure release=api state=state_configure
2022-04-02T08:33:15.842+0100 [INFO]  releaser-plugin-consul: Initializing deployment: service=api
2022-04-02T08:33:15.842+0100 [DEBUG] releaser-plugin-consul: Create service defaults: service=api
2022-04-02T08:33:16.839+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:16.842+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:17.843+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:17.845+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:18.845+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:18.848+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:18.865+0100 [DEBUG] releaser-plugin-consul: Create service resolver: service=api
2022-04-02T08:33:19.849+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:19.851+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:19.873+0100 [DEBUG] releaser-plugin-consul: Create service router: service=api
2022-04-02T08:33:20.852+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:20.855+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:20.880+0100 [DEBUG] releaser-plugin-consul: Create upstream service router: service=api
2022-04-02T08:33:21.856+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:21.858+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:21.887+0100 [DEBUG] releaser-plugin-consul: Create service intentions for the upstreams: service=api
2022-04-02T08:33:22.859+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:22.862+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:23.862+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:23.865+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:24.866+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:24.868+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:25.869+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:25.871+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:26.872+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:26.874+0100 [DEBUG] kubernetes-client: Deployment not found: name=api-deployment-primary namespace=default error=deployment_not_found
2022-04-02T08:33:26.895+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-02T08:33:26.899+0100 [DEBUG] runtime-plugin-kubernetes: Cloning deployment: name=api-deployment namespace=default
2022-04-02T08:33:26.905+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment-primary namespaces=default
2022-04-02T08:33:26.910+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:26.912+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:33:26.912+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:27.874+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:27.878+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:27.878+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:27.913+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:27.916+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:27.916+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:28.878+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:28.881+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:28.881+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:28.917+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:28.921+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:28.921+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:29.882+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:29.886+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:29.886+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:29.921+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:29.925+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:29.925+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:30.886+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:30.889+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:30.889+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:30.926+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:30.929+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:30.929+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:31.890+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:31.892+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:31.892+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:31.929+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:31.932+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:31.932+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:32.893+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:32.896+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:32.896+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:32.933+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:32.937+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:32.937+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:33.896+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:33.899+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:33.899+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:33.937+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:33.940+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:33.940+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:34.899+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:34.902+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:34.902+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:34.940+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:34.943+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:34.943+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:35.903+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:35.907+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:35.907+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:35.944+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:35.946+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:35.946+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:36.908+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:36.911+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:36.911+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:36.947+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:36.949+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:36.949+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:37.911+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:37.914+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:37.914+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:37.950+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:37.953+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:37.953+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:38.915+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:38.918+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:38.918+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:38.954+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:38.957+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:38.957+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:39.919+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:39.922+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:39.922+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:39.958+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:39.960+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:39.960+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:40.923+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:40.925+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:40.925+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:40.961+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:40.964+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:40.964+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:41.926+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:41.929+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:41.929+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:41.965+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:41.968+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=1 desired_replicas=3
2022-04-02T08:33:41.968+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.929+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.932+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:33:42.932+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.968+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.971+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:33:42.971+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.971+0100 [DEBUG] runtime-plugin-kubernetes: Successfully cloned kubernetes deployment: name=api-deployment-primary namespace=default
2022-04-02T08:33:42.971+0100 [INFO]  runtime-plugin-kubernetes: Init primary complete: name=api-deployment namespace=default
2022-04-02T08:33:42.971+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:33:42.974+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:33:47.984+0100 [INFO]  runtime-plugin-kubernetes: Remove candidate deployment: name=api-deployment namespace=default
2022-04-02T08:33:47.993+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:33:47.993+0100 [DEBUG] kubernetes-webhook: Ignore deployment, resource was modified by the controller: name=api-deployment namespace=default labels="map[app:api_v2 consul-release-controller-version:2156]"
2022-04-02T08:33:47.996+0100 [DEBUG] statemachine: Configure completed successfully
2022-04-02T08:33:47.996+0100 [DEBUG] statemachine: Handle event: event=event_configured state=state_configure
2022-04-02T08:33:47.996+0100 [DEBUG] statemachine: Log state: event=event_configured state=state_configure
2022-04-02T08:33:47.996+0100 [DEBUG] statemachine: Log state: event=event_configured release=api state=state_idle
2022-04-02T08:33:48.978+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:33:48.996+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:33:48.996+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_idle
2022-04-02T08:33:48.996+0100 [DEBUG] statemachine: Handle event: event=event_deploy state=state_idle
2022-04-02T08:33:48.996+0100 [DEBUG] statemachine: Log state: event=event_deploy state=state_idle
2022-04-02T08:33:48.996+0100 [DEBUG] statemachine: Deploy: state=state_deploy
2022-04-02T08:33:48.996+0100 [DEBUG] statemachine: Log state: event=event_deploy release=api state=state_deploy
2022-04-02T08:33:48.999+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:49.003+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:33:49.003+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:50.003+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:50.006+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:50.006+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:51.006+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:51.009+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:51.009+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:52.009+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:52.012+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:52.012+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:53.013+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:53.016+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:53.016+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:53.997+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-02T08:33:54.000+0100 [DEBUG] runtime-plugin-kubernetes: Primary deployment already exists: name=api-deployment-primary namespace=default
2022-04-02T08:33:54.000+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:33:54.002+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:33:54.006+0100 [DEBUG] statemachine: Deploy completed, executing strategy
2022-04-02T08:33:54.006+0100 [DEBUG] statemachine: Handle event: event=event_deployed state=state_deploy
2022-04-02T08:33:54.006+0100 [DEBUG] statemachine: Log state: event=event_deployed state=state_deploy
2022-04-02T08:33:54.006+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:33:54.006+0100 [DEBUG] statemachine: Log state: event=event_deployed release=api state=state_monitor
2022-04-02T08:33:54.006+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=-1
2022-04-02T08:33:54.006+0100 [DEBUG] strategy-plugin-canary: Waiting for initial grace before starting rollout: type=canary delay=30
2022-04-02T08:33:54.017+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:54.020+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:54.020+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:55.021+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:55.024+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:55.024+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:56.025+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:56.028+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:56.028+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:57.028+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:57.031+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:57.031+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:58.032+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:58.035+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:58.035+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:33:59.036+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:33:59.039+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:33:59.039+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:00.039+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:00.042+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:34:00.042+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:01.043+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:01.046+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:34:01.046+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:02.046+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:02.050+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:34:02.050+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:03.050+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:03.053+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:34:03.053+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:04.053+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:04.056+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=1 desired_replicas=3
2022-04-02T08:34:04.056+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:34:05.058+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:05.061+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:34:05.061+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:34:05.069+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-02T08:34:05.072+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:34:05.072+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-02T08:34:05.081+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:34:05.084+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:34:05.084+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:34:05.088+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:10.089+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:15.089+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:20.090+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:24.007+0100 [DEBUG] strategy-plugin-canary: Strategy setup: type=canary traffic=10
2022-04-02T08:34:24.007+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-02T08:34:24.007+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-02T08:34:24.007+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-02T08:34:24.007+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-02T08:34:24.007+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-02T08:34:24.007+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=90 traffic_canary=10
2022-04-02T08:34:24.011+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-02T08:34:24.011+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-02T08:34:24.011+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-02T08:34:24.011+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-02T08:34:24.011+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-02T08:34:24.011+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=10
2022-04-02T08:34:25.091+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:30.092+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:35.093+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:40.094+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:45.095+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:50.096+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:34:54.012+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:34:54.012+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:34:54.017+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 56.3049965043655 @[1648884894.012]"] value_type=model.Vector warnings=[]
2022-04-02T08:34:54.017+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=56
2022-04-02T08:34:54.017+0100 [DEBUG] strategy-plugin-canary: Check failed: type=canary error="check failed for query request-success using preset envoy-request-success, got value 56"
2022-04-02T08:34:55.097+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:00.099+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:05.100+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:10.101+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:15.102+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:20.102+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:24.017+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:35:24.017+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:35:24.019+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 50 @[1648884924.018]"] value_type=model.Vector warnings=[]
2022-04-02T08:35:24.019+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=50
2022-04-02T08:35:24.019+0100 [DEBUG] strategy-plugin-canary: Check failed: type=canary error="check failed for query request-success using preset envoy-request-success, got value 50"
2022-04-02T08:35:25.103+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:30.105+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:35.105+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:40.107+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:45.108+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:50.109+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:35:54.020+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:35:54.020+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:35:54.023+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 49.86816018822463 @[1648884954.021]"] value_type=model.Vector warnings=[]
2022-04-02T08:35:54.023+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=49
2022-04-02T08:35:54.023+0100 [DEBUG] strategy-plugin-canary: Check failed: type=canary error="check failed for query request-success using preset envoy-request-success, got value 49"
2022-04-02T08:35:55.109+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:00.111+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:05.111+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:10.112+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:15.113+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:20.114+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:24.024+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:36:24.024+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:36:24.026+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 50 @[1648884984.024]"] value_type=model.Vector warnings=[]
2022-04-02T08:36:24.026+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=50
2022-04-02T08:36:24.026+0100 [DEBUG] strategy-plugin-canary: Check failed: type=canary error="check failed for query request-success using preset envoy-request-success, got value 50"
2022-04-02T08:36:25.115+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:30.116+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:35.117+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:40.119+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:45.120+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:50.120+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:54.026+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-02T08:36:54.027+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-02T08:36:54.029+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 50.4132678214597 @[1648885014.027]"] value_type=model.Vector warnings=[]
2022-04-02T08:36:54.029+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=50
2022-04-02T08:36:54.029+0100 [DEBUG] strategy-plugin-canary: Check failed: type=canary error="check failed for query request-success using preset envoy-request-success, got value 50"
2022-04-02T08:36:54.029+0100 [DEBUG] statemachine: Monitor checks completed, candidate unhealthy
2022-04-02T08:36:54.029+0100 [DEBUG] statemachine: Handle event: event=event_unhealthy state=state_monitor
2022-04-02T08:36:54.029+0100 [DEBUG] statemachine: Log state: event=event_unhealthy state=state_monitor
2022-04-02T08:36:54.029+0100 [DEBUG] statemachine: Rollback: state=state_rollback
2022-04-02T08:36:54.029+0100 [DEBUG] statemachine: Log state: event=event_unhealthy release=api state=state_rollback
2022-04-02T08:36:54.029+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-02T08:36:55.121+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:36:59.037+0100 [INFO]  runtime-plugin-kubernetes: Remove candidate deployment: name=api-deployment namespace=default
2022-04-02T08:36:59.051+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:36:59.051+0100 [DEBUG] kubernetes-webhook: Ignore deployment, resource was modified by the controller: name=api-deployment namespace=default labels="map[app:api_v2 consul-release-controller-version:2620]"
2022-04-02T08:36:59.055+0100 [DEBUG] statemachine: Handle event: event=event_complete state=state_rollback
2022-04-02T08:36:59.055+0100 [DEBUG] statemachine: Log state: event=event_complete state=state_rollback
2022-04-02T08:36:59.055+0100 [DEBUG] statemachine: Log state: event=event_complete release=api state=state_idle
2022-04-02T08:37:00.122+0100 [INFO]  release_handler: Release GET handler called
2022-04-02T08:37:00.158+0100 [DEBUG] statemachine: Handle event: event=event_destroy state=state_idle
2022-04-02T08:37:00.158+0100 [DEBUG] statemachine: Log state: event=event_destroy state=state_idle
2022-04-02T08:37:00.158+0100 [DEBUG] statemachine: Destroy: state=state_destroy
2022-04-02T08:37:00.158+0100 [DEBUG] statemachine: Log state: event=event_destroy release=api state=state_destroy
2022-04-02T08:37:00.158+0100 [INFO]  runtime-plugin-kubernetes: Restore original deployment: name=api-deployment namespace=default
2022-04-02T08:37:00.161+0100 [DEBUG] runtime-plugin-kubernetes: Delete existing candidate deployment: name=api-deployment namespace=default
2022-04-02T08:37:00.168+0100 [DEBUG] runtime-plugin-kubernetes: Clone primary to create original deployment: name=api-deployment namespace=default
2022-04-02T08:37:00.179+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:37:00.186+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:00.189+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-02T08:37:00.189+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:01.190+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:01.193+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:01.193+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:02.193+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:02.196+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:02.196+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:03.197+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:03.200+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:03.200+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:04.201+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:04.204+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:04.204+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:05.205+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:05.208+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:05.208+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:06.209+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:06.212+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:06.212+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:07.212+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:07.216+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:07.216+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:08.216+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:08.219+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:08.219+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:09.220+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:09.223+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:09.223+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:10.224+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:10.227+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:10.227+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:11.227+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:11.230+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:11.230+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:12.231+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:12.235+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:12.235+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:13.236+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:13.239+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:13.239+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:14.240+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:14.244+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-02T08:37:14.244+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:15.245+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:15.250+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=1 desired_replicas=3
2022-04-02T08:37:15.250+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:16.250+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:16.253+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=2 desired_replicas=3
2022-04-02T08:37:16.253+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:17.253+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:17.257+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=2 desired_replicas=3
2022-04-02T08:37:17.257+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:18.257+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:18.260+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=2 desired_replicas=3
2022-04-02T08:37:18.260+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:19.260+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:19.263+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=2 desired_replicas=3
2022-04-02T08:37:19.263+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-02T08:37:20.264+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:20.267+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:37:20.267+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:37:20.267+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-02T08:37:20.270+0100 [DEBUG] releaser-plugin-consul: Service not healthy, retrying: name=api
2022-04-02T08:37:20.270+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=0 traffic_canary=100
2022-04-02T08:37:25.274+0100 [INFO]  runtime-plugin-kubernetes: Remove primary deployment: name=api-deployment namespace=default
2022-04-02T08:37:25.279+0100 [INFO]  releaser-plugin-consul: Remove Consul config: name=api
2022-04-02T08:37:25.279+0100 [DEBUG] releaser-plugin-consul: Delete splitter: name=api
2022-04-02T08:37:26.283+0100 [DEBUG] releaser-plugin-consul: Cleanup router: name=api
2022-04-02T08:37:27.290+0100 [DEBUG] releaser-plugin-consul: Cleanup upstream router: name=api
2022-04-02T08:37:28.301+0100 [DEBUG] releaser-plugin-consul: Cleanup resolver: name=api
2022-04-02T08:37:29.313+0100 [DEBUG] releaser-plugin-consul: Cleanup service intentions: name=api
2022-04-02T08:37:30.324+0100 [DEBUG] releaser-plugin-consul: Cleanup defaults: name=api
2022-04-02T08:37:30.326+0100 [DEBUG] statemachine: Handle event: event=event_complete state=state_destroy
2022-04-02T08:37:30.326+0100 [DEBUG] statemachine: Log state: event=event_complete state=state_destroy
2022-04-02T08:37:30.327+0100 [DEBUG] statemachine: Log state: event=event_complete release=api state=state_idle
2022-04-02T08:37:32.206+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-02T08:37:32.209+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-02T08:37:32.209+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-02T08:37:32.225+0100 [INFO]  Shutting down server gracefully
2022-04-02T08:37:32.226+0100 [INFO]  Shutting down listener
2022-04-02T08:37:32.226+0100 [INFO]  Shutting down metrics
2022-04-02T08:37:32.227+0100 [INFO]  Shutting down kubernetes controller
2022-04-02T08:37:32.227+0100 [INFO]  kubernetes-controller: Stopping Kubernetes controller
2022-04-02T08:38:16.782+0100 [ERROR] 2022-04-02T08:38:16.782+0100 [DEBUG] Generating TLS Certificates for Ingress: path=/home/nicj/.shipyard/certs
2022-04-02T08:38:18.828+0100 [ERROR] 2022-04-02T08:38:18.828+0100 [DEBUG] Starting Ingress
2022-04-02T08:38:18.828+0100 [ERROR] Running configuration from:  ./shipyard/kubernetes

2022-04-02T08:38:18.828+0100 [DEBUG] Statefile does not exist
2022-04-02T08:38:21.828+0100 [ERROR] 2022-04-02T08:38:21.828+0100 [INFO]  Creating resources from configuration: path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes
2022-04-02T08:38:21.828+0100 [DEBUG] Statefile does not exist
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=UPSTREAMS
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=GRAFANA_HTTP_ADDR
2022-04-02T08:38:26.503+0100 [INFO]  Generating template: ref=consul_values output=/home/nicj/.shipyard/data/consul_kubernetes/consul_values.yaml
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=PROMETHEUS_HTTP_ADDR
2022-04-02T08:38:26.503+0100 [DEBUG] Template content: ref=consul_values
  source=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: #{{ .Vars.datacenter }}
  | 
  |   acls:
  |     manageSystemACLs: #{{ .Vars.acl_enabled }}
  |   tls:
  |     enabled: #{{ .Vars.tls_enabled }}
  |     enableAutoEncrypt: #{{ .Vars.tls_enabled }}
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: #{{ .Vars.federation_enabled }}
  |     createFederationSecret: #{{ .Vars.create_federation_secret }}
  | 
  |   image: #{{ .Vars.consul_image }}
  |   
  |   imageK8S: #{{ .Vars.consul_k8s_image }}
  |   
  |   imageEnvoy: #{{ .Vars.consul_envoy_image }}
  | 
  |   metrics:
  |     enabled: #{{ .Vars.metrics_enabled }}
  |     enableAgentMetrics: #{{ .Vars.metrics_enabled }}
  |     enableGatewayMetrics: #{{ .Vars.metrics_enabled }}
  |   
  |   logLevel: #{{ if eq .Vars.debug true }}"debug"#{{ else }}"info"#{{ end }}
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.#{{ .Vars.monitoring_namespace }}.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: #{{ if eq .Vars.debug true }}"--log-level debug"#{{ else }}null#{{ end }}
  | 
  |   transparentProxy:
  |     defaultEnabled: #{{ .Vars.transparent_proxy_enabled }}
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: #{{ .Vars.ingress_gateway_enabled }}
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       #{{ range .Vars.ingress_gateway_ports }}
  |         - port: #{{ . }}
  |           nodePort: null
  |       #{{ end }}
  | 
  | 
  | meshGateway:
  |   enabled: #{{ .Vars.mesh_gateway_enabled }}
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: #{{ .Vars.mesh_gateway_address }}
  |     port: 30443
  | 
  |   service:
  |     enabled: #{{ .Vars.mesh_gateway_enabled }}
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=TLS_KEY
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=GRAFANA_PASSWORD
2022-04-02T08:38:26.503+0100 [INFO]  Generating template: ref=certs_script output=/home/nicj/.shipyard/data/kube_setup/fetch_certs.sh
2022-04-02T08:38:26.503+0100 [DEBUG] Template content: ref=certs_script
  source=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Generating template: ref=consul_namespace output=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [DEBUG] Template content: ref=consul_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [DEBUG] Template output: ref=certs_script
  destination=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
  
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=TLS_CERT
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Generating template: ref=consul_proxy_defaults output=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:38:26.503+0100 [DEBUG] Template content: ref=consul_proxy_defaults
  source=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.#{{ .Vars.monitoring_namespace}}.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
  
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_ADDR
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_TOKEN_FILE
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=CONSUL_CAKEY
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Generating template: ref=controller_values output=/home/nicj/.shipyard/data/kube_setup/helm-values.yaml
2022-04-02T08:38:26.503+0100 [DEBUG] Template content: ref=controller_values
  source=
  | controller:
  |   enabled: "#{{ .Vars.controller_enabled }}"
  |   container_config:
  |     image:
  |       repository: "#{{ .Vars.controller_repo }}"
  |       tag: "#{{ .Vars.controller_version }}"
  | autoencrypt:
  |   enabled: #{{ .Vars.tls_enabled }}
  | acls:
  |   enabled: #{{ .Vars.acls_enabled }}
  | #{{- if eq .Vars.controller_enabled false }}
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | #{{ end }}
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [DEBUG] Template output: ref=consul_proxy_defaults
  destination=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.monitoring.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
  
2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=TEMPO_HTTP_ADDR
2022-04-02T08:38:26.503+0100 [INFO]  Creating Network: ref=dc1
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=KUBECONFIG
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=GRAFANA_USER
2022-04-02T08:38:26.503+0100 [DEBUG] Template output: ref=consul_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [INFO]  Creating Output: ref=CONSUL_CACERT
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [DEBUG] Template output: ref=controller_values
  destination=
  | controller:
  |   enabled: "false"
  |   container_config:
  |     image:
  |       repository: "nicholasjackson/consul-release-controller"
  |       tag: ""
  | autoencrypt:
  |   enabled: true
  | acls:
  |   enabled: true
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  |
2022-04-02T08:38:26.503+0100 [ERROR] 2022-04-02T08:38:26.503+0100 [DEBUG] Template output: ref=consul_values
  destination=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: dc1
  | 
  |   acls:
  |     manageSystemACLs: true
  |   tls:
  |     enabled: true
  |     enableAutoEncrypt: true
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: false
  |     createFederationSecret: false
  | 
  |   image: hashicorp/consul:1.11.3
  |   
  |   imageK8S: hashicorp/consul-k8s-control-plane:0.40.0
  |   
  |   imageEnvoy: envoyproxy/envoy:v1.20.1
  | 
  |   metrics:
  |     enabled: true
  |     enableAgentMetrics: true
  |     enableGatewayMetrics: true
  |   
  |   logLevel: "info"
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: null
  | 
  |   transparentProxy:
  |     defaultEnabled: false
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: true
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       
  |         - port: 18080
  |           nodePort: null
  |       
  |         - port: 18443
  |           nodePort: null
  |       
  | 
  | 
  | meshGateway:
  |   enabled: false
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: dc1.k8s-cluster.shipyard.run
  |     port: 30443
  | 
  |   service:
  |     enabled: false
  |     type: NodePort
  |     nodePort: 30443
2022-04-02T08:38:26.504+0100 [ERROR] 2022-04-02T08:38:26.504+0100 [DEBUG] Attempting to create using bridge plugin: ref=dc1
2022-04-02T08:38:26.532+0100 [ERROR] 2022-04-02T08:38:26.532+0100 [INFO]  Creating ImageCache: ref=docker-cache
2022-04-02T08:38:26.534+0100 [ERROR] 2022-04-02T08:38:26.534+0100 [DEBUG] Connecting cache to network: name=network.dc1
2022-04-02T08:38:26.535+0100 [ERROR] 2022-04-02T08:38:26.535+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:38:26.554+0100 [ERROR] 2022-04-02T08:38:26.554+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:38:26.554+0100 [DEBUG] Creating Docker Container: ref=54515060-import
2022-04-02T08:38:29.265+0100 [ERROR] 2022-04-02T08:38:29.265+0100 [DEBUG] Forcefully remove: container=a8e614bd79d9e7cec397b299236664a208ab2b4c7d342773b196e8c50be2269a
2022-04-02T08:38:29.683+0100 [ERROR] 2022-04-02T08:38:29.683+0100 [DEBUG] Image exists in local cache: image=shipyardrun/docker-registry-proxy:0.6.3
2022-04-02T08:38:29.683+0100 [DEBUG] Creating Docker Container: ref=docker-cache
2022-04-02T08:38:29.739+0100 [ERROR] 2022-04-02T08:38:29.739+0100 [DEBUG] Remove container from default networks: ref=docker-cache
2022-04-02T08:38:29.743+0100 [ERROR] 2022-04-02T08:38:29.743+0100 [DEBUG] Attaching container to network: ref=f6996541b1a7da01c14e377112a71fcbee017f1b9e8d5f1ff779c90497690485 network=dc1
2022-04-02T08:38:29.753+0100 [ERROR] 2022-04-02T08:38:29.753+0100 [DEBUG] Disconnectng network: name=bridge ref=docker-cache
2022-04-02T08:38:30.257+0100 [ERROR] 2022-04-02T08:38:30.256+0100 [INFO]  dc1: Creating Cluster: ref=dc1
2022-04-02T08:38:30.279+0100 [ERROR] 2022-04-02T08:38:30.278+0100 [DEBUG] Image exists in local cache: image=shipyardrun/k3s:v1.22.4
2022-04-02T08:38:30.280+0100 [ERROR] 2022-04-02T08:38:30.280+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-02T08:38:30.280+0100 [ERROR] 2022-04-02T08:38:30.280+0100 [DEBUG] Creating Docker Container: ref=server.dc1
2022-04-02T08:38:30.334+0100 [ERROR] 2022-04-02T08:38:30.334+0100 [DEBUG] Remove container from default networks: ref=server.dc1
2022-04-02T08:38:30.337+0100 [ERROR] 2022-04-02T08:38:30.337+0100 [DEBUG] Attaching container to network: ref=37b6bbefd7b2edf86e8d376a3e650e8a42b7093c1aac72919acdc693714a10cf network=dc1
2022-04-02T08:38:30.342+0100 [ERROR] 2022-04-02T08:38:30.342+0100 [DEBUG] Disconnectng network: name=bridge ref=server.dc1
2022-04-02T08:38:32.995+0100 [ERROR] 2022-04-02T08:38:32.995+0100 [DEBUG] Copying file from: id=37b6bbefd7b2edf86e8d376a3e650e8a42b7093c1aac72919acdc693714a10cf src=/output/kubeconfig.yaml dst=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:38:33.034+0100 [ERROR] 2022-04-02T08:38:33.034+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:35.044+0100 [ERROR] 2022-04-02T08:38:35.044+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:36.829+0100 [ERROR] 2022-04-02T08:38:36.829+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 15.000826]
2022-04-02T08:38:37.047+0100 [ERROR] 2022-04-02T08:38:37.047+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:39.050+0100 [ERROR] 2022-04-02T08:38:39.050+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:41.054+0100 [ERROR] 2022-04-02T08:38:41.054+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:43.058+0100 [ERROR] 2022-04-02T08:38:43.058+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:45.062+0100 [ERROR] 2022-04-02T08:38:45.062+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:47.065+0100 [ERROR] 2022-04-02T08:38:47.065+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:49.069+0100 [ERROR] 2022-04-02T08:38:49.069+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-gwrmw namespace=kube-system status=Pending
2022-04-02T08:38:51.073+0100 [ERROR] 2022-04-02T08:38:51.073+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-02T08:38:51.073+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:38:51.829+0100 [ERROR] 2022-04-02T08:38:51.829+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 30.000979]
2022-04-02T08:38:53.077+0100 [ERROR] 2022-04-02T08:38:53.077+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-02T08:38:53.077+0100 [DEBUG] Writing docker images to volume: images=[] volume=images.volume.shipyard.run
2022-04-02T08:38:53.094+0100 [ERROR] 2022-04-02T08:38:53.094+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-02T08:38:53.094+0100 [DEBUG] Creating Docker Container: ref=94770021-import
2022-04-02T08:38:55.706+0100 [ERROR] 2022-04-02T08:38:55.706+0100 [DEBUG] Forcefully remove: container=f6d6e6617de51861365ed1334669722bcbfd67a57c6871225938d289d69907e9
2022-04-02T08:38:56.094+0100 [ERROR] 2022-04-02T08:38:56.094+0100 [DEBUG] dc1: Deploying connector
2022-04-02T08:38:57.618+0100 [ERROR] 2022-04-02T08:38:57.618+0100 [DEBUG] dc1: Writing namespace config: file=/tmp/391914446/namespace.yaml
2022-04-02T08:38:57.618+0100 [DEBUG] dc1: Writing secret config: file=/tmp/391914446/secret.yaml
2022-04-02T08:38:57.618+0100 [ERROR] 2022-04-02T08:38:57.618+0100 [DEBUG] dc1: Writing RBAC config: file=/tmp/391914446/rbac.yaml
2022-04-02T08:38:57.618+0100 [ERROR] 2022-04-02T08:38:57.618+0100 [DEBUG] dc1: Writing deployment config: file=/tmp/391914446/deployment.yaml
2022-04-02T08:38:57.618+0100 [ERROR] 2022-04-02T08:38:57.618+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/391914446/namespace.yaml
2022-04-02T08:38:58.169+0100 [ERROR] 2022-04-02T08:38:58.169+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/391914446/secret.yaml
2022-04-02T08:38:58.176+0100 [ERROR] 2022-04-02T08:38:58.176+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/391914446/rbac.yaml
2022-04-02T08:38:58.183+0100 [ERROR] 2022-04-02T08:38:58.183+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/391914446/deployment.yaml
2022-04-02T08:38:58.198+0100 [ERROR] 2022-04-02T08:38:58.198+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Applying Kubernetes configuration: ref=cert-manager-controller config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml"]
2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=upstreams-proxy
2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=web
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [DEBUG] Calling connector to expose remote service: name=upstreams-proxy local_port=28080 connector_addr=127.0.0.1:30577 local_addr=consul-release-controller.default.svc:8080
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=consul-rpc
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=controller-webhook
2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=consul-lan-serf
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Applying Kubernetes configuration: ref=consul_namespace config=["/home/nicj/.shipyard/data/consul/namespace.yaml"]
2022-04-02T08:39:00.202+0100 [DEBUG] Calling connector to expose local service: name=controller-webhook remote_port=19443 connector_addr=127.0.0.1:30577 local_addr=localhost:19443
2022-04-02T08:39:00.202+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [DEBUG] Calling connector to expose remote service: name=web local_port=9092 connector_addr=127.0.0.1:30577 local_addr=web.default.svc:9090
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [DEBUG] Calling connector to expose remote service: name=consul-rpc local_port=8300 connector_addr=127.0.0.1:30577 local_addr=consul-server.consul.svc:8300
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [DEBUG] Calling connector to expose remote service: name=consul-lan-serf local_port=8301 connector_addr=127.0.0.1:30577 local_addr=consul-server.consul.svc:8301
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-1
2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-2
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.203+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-1 local_port=18080 connector_addr=127.0.0.1:30577 local_addr=consul-ingress-gateway.consul.svc:18080
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.203+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-2 local_port=18443 connector_addr=127.0.0.1:30577 local_addr=consul-ingress-gateway.consul.svc:18443
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.202+0100 [INFO]  Create Ingress: ref=consul
2022-04-02T08:39:00.203+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.203+0100 [DEBUG] Calling connector to expose remote service: name=consul local_port=8501 connector_addr=127.0.0.1:30577 local_addr=consul-server.consul.svc:8501
2022-04-02T08:39:00.203+0100 [ERROR] 2022-04-02T08:39:00.203+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-02T08:39:00.222+0100 [ERROR] 2022-04-02T08:39:00.222+0100 [DEBUG] Successfully exposed service: id=5165c57e-37ed-4a04-9294-e8bd4997fb68
2022-04-02T08:39:00.223+0100 [ERROR] 2022-04-02T08:39:00.222+0100 [DEBUG] Successfully exposed service: id=c383235b-8767-4d83-81d9-d976fe1a3886
2022-04-02T08:39:00.222+0100 [DEBUG] Successfully exposed service: id=04a2c7c6-e60f-46d8-9003-df82f7622726
2022-04-02T08:39:00.225+0100 [ERROR] 2022-04-02T08:39:00.225+0100 [DEBUG] Successfully exposed service: id=de30d4c2-08e5-4064-acea-bdfc76c9d697
2022-04-02T08:39:00.225+0100 [DEBUG] Successfully exposed service: id=832a43de-db62-4200-a441-f0e5a712b49b
2022-04-02T08:39:00.226+0100 [ERROR] 2022-04-02T08:39:00.226+0100 [DEBUG] Successfully exposed service: id=4dc3ab26-f73b-4022-9bee-9413290abbb9
2022-04-02T08:39:00.226+0100 [DEBUG] Successfully exposed service: id=7f15ab98-a1de-403e-b519-39dd623fbadb
2022-04-02T08:39:00.227+0100 [ERROR] 2022-04-02T08:39:00.227+0100 [DEBUG] Successfully exposed service: id=341f839f-1806-409f-8463-36365d7490ec
2022-04-02T08:39:00.262+0100 [ERROR] 2022-04-02T08:39:00.262+0100 [INFO]  Creating Helm chart: ref=consul
2022-04-02T08:39:00.262+0100 [DEBUG] Updating Helm chart repository: name=hashicorp url=https://helm.releases.hashicorp.com
2022-04-02T08:39:00.392+0100 [ERROR] 2022-04-02T08:39:00.392+0100 [DEBUG] Using Kubernetes config: ref=consul path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:00.393+0100 [ERROR] 2022-04-02T08:39:00.393+0100 [DEBUG] Creating chart from config: ref=consul chart=hashicorp/consul
2022-04-02T08:39:00.489+0100 [ERROR] 2022-04-02T08:39:00.489+0100 [DEBUG] Loading chart: ref=consul path=/home/nicj/.shipyard/helm_charts/cache/consul-0.40.0.tgz
2022-04-02T08:39:00.494+0100 [ERROR] 2022-04-02T08:39:00.494+0100 [DEBUG] Using Values: ref=consul
  values=
  | map[connectInject:map[centralConfig:map[enabled:true] default:false enabled:true envoyExtraArgs:<nil> failurePolicy:Ignore replicas:1 transparentProxy:map[defaultEnabled:false]] controller:map[enabled:true] global:map[acls:map[manageSystemACLs:true] datacenter:dc1 federation:map[createFederationSecret:false enabled:false] image:hashicorp/consul:1.11.3 imageEnvoy:envoyproxy/envoy:v1.20.1 imageK8S:hashicorp/consul-k8s-control-plane:0.40.0 logLevel:info metrics:map[enableAgentMetrics:true enableGatewayMetrics:true enabled:true] name:consul tls:map[enableAutoEncrypt:true enabled:true httpsOnly:false]] ingressGateways:map[defaults:map[replicas:1 service:map[ports:[map[nodePort:<nil> port:18080] map[nodePort:<nil> port:18443]]]] enabled:true] meshGateway:map[enabled:false replicas:1 service:map[enabled:false nodePort:30443 type:NodePort] wanAddress:map[port:30443 source:Static static:dc1.k8s-cluster.shipyard.run]] server:map[bootstrapExpect:1 extraConfig:{
  |   "ui_config": {
  |     "enabled": true,
  |     "metrics_provider": "prometheus",
  |     "metrics_proxy": {
  |       "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |     }
  |   }
  | }
  |  replicas:1 storage:128Mi] ui:map[enabled:true]]
  
2022-04-02T08:39:00.494+0100 [DEBUG] Validate chart: ref=consul
2022-04-02T08:39:00.494+0100 [DEBUG] Run chart: ref=consul
2022-04-02T08:39:00.595+0100 [ERROR] 2022-04-02T08:39:00.595+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:39:01.158+0100 [ERROR] 2022-04-02T08:39:01.158+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" ServiceAccount"
2022-04-02T08:39:01.160+0100 [ERROR] 2022-04-02T08:39:01.160+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="serviceaccounts \"consul-tls-init\" not found"
2022-04-02T08:39:01.212+0100 [ERROR] 2022-04-02T08:39:01.212+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:39:01.216+0100 [ERROR] 2022-04-02T08:39:01.216+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Role"
2022-04-02T08:39:01.217+0100 [ERROR] 2022-04-02T08:39:01.217+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="roles.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:39:01.267+0100 [ERROR] 2022-04-02T08:39:01.267+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:39:01.271+0100 [ERROR] 2022-04-02T08:39:01.271+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" RoleBinding"
2022-04-02T08:39:01.274+0100 [ERROR] 2022-04-02T08:39:01.274+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="rolebindings.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-02T08:39:01.324+0100 [ERROR] 2022-04-02T08:39:01.324+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:39:01.329+0100 [ERROR] 2022-04-02T08:39:01.329+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:39:01.331+0100 [ERROR] 2022-04-02T08:39:01.331+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="jobs.batch \"consul-tls-init\" not found"
2022-04-02T08:39:01.383+0100 [ERROR] 2022-04-02T08:39:01.383+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:39:01.390+0100 [ERROR] 2022-04-02T08:39:01.389+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-tls-init with timeout of 0s"
2022-04-02T08:39:01.395+0100 [ERROR] 2022-04-02T08:39:01.394+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: ADDED"
2022-04-02T08:39:01.394+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:01.408+0100 [ERROR] 2022-04-02T08:39:01.407+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:39:01.408+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:02.600+0100 [ERROR] 2022-04-02T08:39:02.600+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-cainjector-7974c84449-bwdbb namespace=cert-manager status=Pending
2022-04-02T08:39:03.700+0100 [ERROR] 2022-04-02T08:39:03.700+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:39:03.700+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:03.709+0100 [ERROR] 2022-04-02T08:39:03.709+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-02T08:39:03.711+0100 [ERROR] 2022-04-02T08:39:03.711+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-02T08:39:03.715+0100 [ERROR] 2022-04-02T08:39:03.715+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 58 resource(s)"
2022-04-02T08:39:04.132+0100 [ERROR] 2022-04-02T08:39:04.131+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-02T08:39:04.135+0100 [ERROR] 2022-04-02T08:39:04.135+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-server-acl-init-cleanup with timeout of 0s"
2022-04-02T08:39:04.138+0100 [ERROR] 2022-04-02T08:39:04.138+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: ADDED"
2022-04-02T08:39:04.138+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:04.153+0100 [ERROR] 2022-04-02T08:39:04.153+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:39:04.153+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:04.605+0100 [ERROR] 2022-04-02T08:39:04.605+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-tv5tw namespace=cert-manager type=Ready value=False
2022-04-02T08:39:06.610+0100 [ERROR] 2022-04-02T08:39:06.610+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-tv5tw namespace=cert-manager type=Ready value=False
2022-04-02T08:39:06.829+0100 [ERROR] 2022-04-02T08:39:06.828+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 45.000112]
2022-04-02T08:39:08.616+0100 [ERROR] 2022-04-02T08:39:08.616+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-tv5tw namespace=cert-manager type=Ready value=False
2022-04-02T08:39:10.621+0100 [ERROR] 2022-04-02T08:39:10.621+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-tv5tw namespace=cert-manager type=Ready value=False
2022-04-02T08:39:12.626+0100 [ERROR] 2022-04-02T08:39:12.626+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-tv5tw namespace=cert-manager type=Ready value=False
2022-04-02T08:39:14.631+0100 [ERROR] 2022-04-02T08:39:14.631+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-02T08:39:21.829+0100 [ERROR] 2022-04-02T08:39:21.829+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 60.001048]
2022-04-02T08:39:22.792+0100 [ERROR] 2022-04-02T08:39:22.792+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:39:22.792+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:22.802+0100 [ERROR] 2022-04-02T08:39:22.802+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-02T08:39:22.804+0100 [ERROR] 2022-04-02T08:39:22.804+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-server-acl-init-cleanup\" Job"
2022-04-02T08:39:22.871+0100 [ERROR] 2022-04-02T08:39:22.871+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:39:24.876+0100 [ERROR] 2022-04-02T08:39:24.876+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-km4s4 namespace=consul status=Pending
2022-04-02T08:39:26.881+0100 [ERROR] 2022-04-02T08:39:26.881+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-02T08:39:26.881+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:39:28.886+0100 [ERROR] 2022-04-02T08:39:28.886+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-02T08:39:28.886+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:39:30.891+0100 [ERROR] 2022-04-02T08:39:30.891+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-02T08:39:30.891+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Create Ingress: ref=grafana
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Generating template: ref=fetch_consul_resources output=/home/nicj/.shipyard/data/consul_kubernetes/fetch.sh
2022-04-02T08:39:32.897+0100 [DEBUG] Template content: ref=fetch_consul_resources
  source=
  |   #!/bin/sh -e
  | 
  |   echo "Port #{{ .Vars.port }}"
  |   echo "Fetching resources from running cluster, acls_enabled: #{{ .Vars.acl_enabled }}, tls_enabled #{{ .Vars.tls_enabled }}"
  | 
  |   #{{ if eq .Vars.acl_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   #{{end}}
  |   
  |   #{{ if eq .Vars.tls_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |   #{{end}}
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Calling connector to expose remote service: name=grafana local_port=8080 connector_addr=127.0.0.1:30577 local_addr=grafana.monitoring.svc:80
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Generating template: ref=grafana_secret_template output=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:39:32.897+0100 [DEBUG] Template content: ref=grafana_secret_template
  source=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
  
2022-04-02T08:39:32.897+0100 [INFO]  Applying Kubernetes configuration: ref=consul_defaults config=["/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml"]
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Create Ingress: ref=tempo
2022-04-02T08:39:32.897+0100 [INFO]  Create Ingress: ref=zipkin
2022-04-02T08:39:32.897+0100 [INFO]  Generating template: ref=prometheus_operator_template output=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:39:32.897+0100 [DEBUG] Template content: ref=prometheus_operator_template
  source=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Template output: ref=grafana_secret_template
  destination=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: monitoring
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Calling connector to expose remote service: name=tempo local_port=3100 connector_addr=127.0.0.1:30577 local_addr=tempo.default.svc:3100
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Calling connector to expose remote service: name=zipkin local_port=9411 connector_addr=127.0.0.1:30577 local_addr=tempo.monitoring.svc:9411
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Template output: ref=fetch_consul_resources
  destination=
  |   #!/bin/sh -e
  | 
  |   echo "Port 8501"
  |   echo "Fetching resources from running cluster, acls_enabled: true, tls_enabled true"
  | 
  |   
  |   kubectl get secret -n consul -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   
  |   
  |   
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Template output: ref=prometheus_operator_template
  destination=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Remote executing command: ref=fetch_consul_resources command=sh args=["/data/fetch.sh"] image="&{shipyardrun/tools:v0.5.0  }"
2022-04-02T08:39:32.897+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Create Ingress: ref=prometheus
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [DEBUG] Calling connector to expose remote service: name=prometheus local_port=9090 connector_addr=127.0.0.1:30577 local_addr=prometheus-operated.monitoring.svc:9090
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Creating Helm chart: ref=prometheus
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.897+0100 [INFO]  Generating template: ref=monitoring_namespace output=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:39:32.898+0100 [DEBUG] Template content: ref=monitoring_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.898+0100 [DEBUG] Updating Helm chart repository: name=prometheus url=https://prometheus-community.github.io/helm-charts
2022-04-02T08:39:32.898+0100 [DEBUG] Template output: ref=monitoring_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.898+0100 [INFO]  Applying Kubernetes configuration: ref=monitoring_namespace config=["/home/nicj/.shipyard/data/monitoring/namespace.yaml"]
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.898+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-02T08:39:32.898+0100 [ERROR] 2022-04-02T08:39:32.898+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-02T08:39:32.921+0100 [ERROR] 2022-04-02T08:39:32.921+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.5.0
2022-04-02T08:39:32.921+0100 [DEBUG] Creating Docker Container: ref=fetch_consul_resources.remote_exec
2022-04-02T08:39:32.922+0100 [ERROR] 2022-04-02T08:39:32.921+0100 [DEBUG] Successfully exposed service: id=a66636be-a025-4d41-906a-b68fc9386da3
2022-04-02T08:39:32.922+0100 [ERROR] 2022-04-02T08:39:32.922+0100 [DEBUG] Successfully exposed service: id=4712ed66-bd4d-4aee-a4fe-a7a5a5032cda
2022-04-02T08:39:32.924+0100 [ERROR] 2022-04-02T08:39:32.924+0100 [DEBUG] Successfully exposed service: id=265a4e71-6c4f-49e5-8b58-340fbe41daaa
2022-04-02T08:39:32.924+0100 [ERROR] 2022-04-02T08:39:32.924+0100 [DEBUG] Successfully exposed service: id=c0bf3fb5-ca65-42ad-adf7-40e32b574b0a
2022-04-02T08:39:32.978+0100 [ERROR] 2022-04-02T08:39:32.978+0100 [DEBUG] Remove container from default networks: ref=fetch_consul_resources.remote_exec
2022-04-02T08:39:32.982+0100 [ERROR] 2022-04-02T08:39:32.982+0100 [DEBUG] Attaching container to network: ref=f01b322cf48d4b4862d9adfbb3e33a0363798a93e6a88cc82b753c7612f1ddc7 network=dc1
2022-04-02T08:39:32.989+0100 [ERROR] 2022-04-02T08:39:32.989+0100 [DEBUG] Disconnectng network: name=bridge ref=fetch_consul_resources.remote_exec
2022-04-02T08:39:33.292+0100 [ERROR] 2022-04-02T08:39:33.292+0100 [DEBUG] Using Kubernetes config: ref=prometheus path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:33.292+0100 [ERROR] 2022-04-02T08:39:33.292+0100 [DEBUG] Creating chart from config: ref=prometheus chart=prometheus/kube-prometheus-stack
2022-04-02T08:39:33.465+0100 [ERROR] 2022-04-02T08:39:33.465+0100 [INFO]  Applying Kubernetes configuration: ref=grafana_secret config=["/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml"]
2022-04-02T08:39:33.465+0100 [ERROR] 2022-04-02T08:39:33.465+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-02T08:39:33.768+0100 [ERROR] 2022-04-02T08:39:33.768+0100 [DEBUG] Port 8501
Fetching resources from running cluster, acls_enabled: true, tls_enabled true
2022-04-02T08:39:34.070+0100 [ERROR] 2022-04-02T08:39:34.070+0100 [DEBUG] Loading chart: ref=prometheus path=/home/nicj/.shipyard/helm_charts/cache/kube-prometheus-stack-32.0.0.tgz
2022-04-02T08:39:34.083+0100 [ERROR] 2022-04-02T08:39:34.083+0100 [DEBUG] Using Values: ref=prometheus values="map[alertmanager:map[enabled:false] defaultRules:map[create:false] grafana:map[enabled:false] serviceMonitor:map[enabled:false]]"
2022-04-02T08:39:34.083+0100 [DEBUG] Validate chart: ref=prometheus
2022-04-02T08:39:34.083+0100 [DEBUG] Run chart: ref=prometheus
2022-04-02T08:39:34.098+0100 [ERROR] 2022-04-02T08:39:34.098+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.148+0100 [ERROR] 2022-04-02T08:39:34.148+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.204+0100 [ERROR] 2022-04-02T08:39:34.203+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.222+0100 [ERROR] 2022-04-02T08:39:34.222+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.223+0100 [ERROR] 2022-04-02T08:39:34.223+0100 [DEBUG] Forcefully remove: container=f01b322cf48d4b4862d9adfbb3e33a0363798a93e6a88cc82b753c7612f1ddc7
2022-04-02T08:39:34.257+0100 [ERROR] 2022-04-02T08:39:34.257+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.318+0100 [ERROR] 2022-04-02T08:39:34.318+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.329+0100 [ERROR] 2022-04-02T08:39:34.329+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.367+0100 [ERROR] 2022-04-02T08:39:34.367+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:34.411+0100 [ERROR] 2022-04-02T08:39:34.411+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Clearing discovery cache"
2022-04-02T08:39:34.411+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="beginning wait for 8 resources with timeout of 1m0s"
2022-04-02T08:39:36.829+0100 [ERROR] 2022-04-02T08:39:36.829+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 75.000159]
2022-04-02T08:39:37.830+0100 [ERROR] 2022-04-02T08:39:37.829+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:38.119+0100 [ERROR] 2022-04-02T08:39:38.119+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:39:38.122+0100 [ERROR] 2022-04-02T08:39:38.122+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:38.401+0100 [ERROR] 2022-04-02T08:39:38.401+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:38.406+0100 [ERROR] 2022-04-02T08:39:38.406+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:39:38.408+0100 [ERROR] 2022-04-02T08:39:38.408+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:38.692+0100 [ERROR] 2022-04-02T08:39:38.692+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:38.698+0100 [ERROR] 2022-04-02T08:39:38.697+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:39:38.700+0100 [ERROR] 2022-04-02T08:39:38.700+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:39.002+0100 [ERROR] 2022-04-02T08:39:39.002+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:39.011+0100 [ERROR] 2022-04-02T08:39:39.011+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:39:39.014+0100 [ERROR] 2022-04-02T08:39:39.014+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:39.306+0100 [ERROR] 2022-04-02T08:39:39.306+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:39.312+0100 [ERROR] 2022-04-02T08:39:39.312+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:39:39.315+0100 [ERROR] 2022-04-02T08:39:39.315+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:39.597+0100 [ERROR] 2022-04-02T08:39:39.597+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:39.603+0100 [ERROR] 2022-04-02T08:39:39.603+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:39:39.605+0100 [ERROR] 2022-04-02T08:39:39.605+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-create\" not found"
2022-04-02T08:39:39.885+0100 [ERROR] 2022-04-02T08:39:39.885+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:39.891+0100 [ERROR] 2022-04-02T08:39:39.890+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-create with timeout of 0s"
2022-04-02T08:39:39.893+0100 [ERROR] 2022-04-02T08:39:39.893+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: ADDED"
2022-04-02T08:39:39.893+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:39.910+0100 [ERROR] 2022-04-02T08:39:39.910+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:39:39.910+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:41.706+0100 [ERROR] 2022-04-02T08:39:41.706+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:39:41.706+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:41.714+0100 [ERROR] 2022-04-02T08:39:41.714+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-02T08:39:41.716+0100 [ERROR] 2022-04-02T08:39:41.716+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:39:41.721+0100 [ERROR] 2022-04-02T08:39:41.721+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:39:41.728+0100 [ERROR] 2022-04-02T08:39:41.728+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:39:41.733+0100 [ERROR] 2022-04-02T08:39:41.733+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:39:41.738+0100 [ERROR] 2022-04-02T08:39:41.738+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:39:41.744+0100 [ERROR] 2022-04-02T08:39:41.744+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-02T08:39:41.747+0100 [ERROR] 2022-04-02T08:39:41.747+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 36 resource(s)"
2022-04-02T08:39:41.938+0100 [ERROR] 2022-04-02T08:39:41.938+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:39:41.940+0100 [ERROR] 2022-04-02T08:39:41.940+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:42.215+0100 [ERROR] 2022-04-02T08:39:42.215+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:42.219+0100 [ERROR] 2022-04-02T08:39:42.219+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:39:42.221+0100 [ERROR] 2022-04-02T08:39:42.221+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:42.530+0100 [ERROR] 2022-04-02T08:39:42.530+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:42.535+0100 [ERROR] 2022-04-02T08:39:42.534+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:39:42.537+0100 [ERROR] 2022-04-02T08:39:42.537+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:42.818+0100 [ERROR] 2022-04-02T08:39:42.818+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:42.824+0100 [ERROR] 2022-04-02T08:39:42.823+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:39:42.826+0100 [ERROR] 2022-04-02T08:39:42.826+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:43.114+0100 [ERROR] 2022-04-02T08:39:43.114+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:43.120+0100 [ERROR] 2022-04-02T08:39:43.120+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:39:43.122+0100 [ERROR] 2022-04-02T08:39:43.122+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-02T08:39:43.422+0100 [ERROR] 2022-04-02T08:39:43.422+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:43.427+0100 [ERROR] 2022-04-02T08:39:43.427+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:39:43.430+0100 [ERROR] 2022-04-02T08:39:43.430+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-patch\" not found"
2022-04-02T08:39:43.729+0100 [ERROR] 2022-04-02T08:39:43.728+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-02T08:39:43.749+0100 [ERROR] 2022-04-02T08:39:43.749+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-patch with timeout of 0s"
2022-04-02T08:39:43.751+0100 [ERROR] 2022-04-02T08:39:43.751+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: ADDED"
2022-04-02T08:39:43.751+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:43.771+0100 [ERROR] 2022-04-02T08:39:43.771+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:39:43.771+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:46.726+0100 [ERROR] 2022-04-02T08:39:46.726+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:39:46.726+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-02T08:39:46.734+0100 [ERROR] 2022-04-02T08:39:46.734+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-02T08:39:46.736+0100 [ERROR] 2022-04-02T08:39:46.736+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-02T08:39:46.742+0100 [ERROR] 2022-04-02T08:39:46.742+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-02T08:39:46.752+0100 [ERROR] 2022-04-02T08:39:46.752+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-02T08:39:46.758+0100 [ERROR] 2022-04-02T08:39:46.758+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-02T08:39:46.764+0100 [ERROR] 2022-04-02T08:39:46.764+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-02T08:39:46.769+0100 [ERROR] 2022-04-02T08:39:46.769+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-02T08:39:47.058+0100 [ERROR] 2022-04-02T08:39:47.058+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:39:49.063+0100 [ERROR] 2022-04-02T08:39:49.063+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-w6vcc namespace=monitoring type=Ready value=False
2022-04-02T08:39:51.069+0100 [ERROR] 2022-04-02T08:39:51.069+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-w6vcc namespace=monitoring type=Ready value=False
2022-04-02T08:39:51.829+0100 [ERROR] 2022-04-02T08:39:51.828+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 90.000092]
2022-04-02T08:39:53.074+0100 [ERROR] 2022-04-02T08:39:53.074+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-02T08:39:53.074+0100 [ERROR] 2022-04-02T08:39:53.074+0100 [INFO]  Applying Kubernetes configuration: ref=prometheus config=["/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml"]
2022-04-02T08:39:53.074+0100 [INFO]  Creating Helm chart: ref=loki
2022-04-02T08:39:53.074+0100 [ERROR] 2022-04-02T08:39:53.074+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:39:53.074+0100 [ERROR] 2022-04-02T08:39:53.074+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-02T08:39:53.384+0100 [ERROR] 2022-04-02T08:39:53.383+0100 [DEBUG] Using Kubernetes config: ref=loki path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:53.384+0100 [ERROR] 2022-04-02T08:39:53.384+0100 [DEBUG] Creating chart from config: ref=loki chart=grafana/loki
2022-04-02T08:39:53.811+0100 [ERROR] 2022-04-02T08:39:53.811+0100 [DEBUG] Loading chart: ref=loki path=/home/nicj/.shipyard/helm_charts/cache/loki-2.9.1.tgz
2022-04-02T08:39:53.812+0100 [ERROR] 2022-04-02T08:39:53.812+0100 [DEBUG] Using Values: ref=loki values=map[]
2022-04-02T08:39:53.812+0100 [DEBUG] Validate chart: ref=loki
2022-04-02T08:39:53.812+0100 [DEBUG] Run chart: ref=loki
2022-04-02T08:39:54.046+0100 [ERROR] W0402 08:39:54.046433   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:54.060+0100 [ERROR] 2022-04-02T08:39:54.060+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 1 resource(s)"
2022-04-02T08:39:54.068+0100 [ERROR] 2022-04-02T08:39:54.068+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 8 resource(s)"
2022-04-02T08:39:54.072+0100 [ERROR] W0402 08:39:54.072601   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:54.103+0100 [ERROR] 2022-04-02T08:39:54.103+0100 [INFO]  Creating Helm chart: ref=promtail
2022-04-02T08:39:54.103+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:39:54.103+0100 [DEBUG] Using Kubernetes config: ref=promtail path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:54.103+0100 [ERROR] 2022-04-02T08:39:54.103+0100 [DEBUG] Creating chart from config: ref=promtail chart=grafana/promtail
2022-04-02T08:39:54.778+0100 [ERROR] 2022-04-02T08:39:54.778+0100 [DEBUG] Loading chart: ref=promtail path=/home/nicj/.shipyard/helm_charts/cache/promtail-3.11.0.tgz
2022-04-02T08:39:54.780+0100 [ERROR] 2022-04-02T08:39:54.779+0100 [DEBUG] Using Values: ref=promtail values=map[config:map[lokiAddress:http://loki:3100/loki/api/v1/push]]
2022-04-02T08:39:54.780+0100 [DEBUG] Validate chart: ref=promtail
2022-04-02T08:39:54.780+0100 [DEBUG] Run chart: ref=promtail
2022-04-02T08:39:55.068+0100 [ERROR] 2022-04-02T08:39:55.068+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 1 resource(s)"
2022-04-02T08:39:55.077+0100 [ERROR] 2022-04-02T08:39:55.077+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 5 resource(s)"
2022-04-02T08:39:55.102+0100 [ERROR] 2022-04-02T08:39:55.102+0100 [INFO]  Creating Helm chart: ref=tempo
2022-04-02T08:39:55.102+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:39:55.102+0100 [ERROR] 2022-04-02T08:39:55.102+0100 [DEBUG] Using Kubernetes config: ref=tempo path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:55.103+0100 [ERROR] 2022-04-02T08:39:55.103+0100 [DEBUG] Creating chart from config: ref=tempo chart=grafana/tempo
2022-04-02T08:39:55.707+0100 [ERROR] 2022-04-02T08:39:55.707+0100 [DEBUG] Loading chart: ref=tempo path=/home/nicj/.shipyard/helm_charts/cache/tempo-0.13.1.tgz
2022-04-02T08:39:55.708+0100 [ERROR] 2022-04-02T08:39:55.708+0100 [DEBUG] Using Values: ref=tempo values="map[tempo:map[receivers:map[jaeger:map[protocols:map[grpc:map[endpoint:0.0.0.0:14250] thrift_binary:map[endpoint:0.0.0.0:6832] thrift_compact:map[endpoint:0.0.0.0:6831] thrift_http:map[endpoint:0.0.0.0:14268]]] zipkin:map[]]]]"
2022-04-02T08:39:55.708+0100 [DEBUG] Validate chart: ref=tempo
2022-04-02T08:39:55.708+0100 [DEBUG] Run chart: ref=tempo
2022-04-02T08:39:55.953+0100 [ERROR] 2022-04-02T08:39:55.953+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 1 resource(s)"
2022-04-02T08:39:55.962+0100 [ERROR] 2022-04-02T08:39:55.962+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 5 resource(s)"
2022-04-02T08:39:55.993+0100 [ERROR] 2022-04-02T08:39:55.993+0100 [INFO]  Creating Helm chart: ref=grafana
2022-04-02T08:39:55.993+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-02T08:39:55.993+0100 [ERROR] 2022-04-02T08:39:55.993+0100 [DEBUG] Using Kubernetes config: ref=grafana path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:55.993+0100 [ERROR] 2022-04-02T08:39:55.993+0100 [DEBUG] Creating chart from config: ref=grafana chart=grafana/grafana
2022-04-02T08:39:56.591+0100 [ERROR] 2022-04-02T08:39:56.591+0100 [DEBUG] Loading chart: ref=grafana path=/home/nicj/.shipyard/helm_charts/cache/grafana-6.21.2.tgz
2022-04-02T08:39:56.593+0100 [ERROR] 2022-04-02T08:39:56.593+0100 [DEBUG] Using Values: ref=grafana values="map[admin:map[existingSecret:grafana-password] datasources:map[datasources.yaml:map[apiVersion:1 datasources:[map[isDefault:true name:Prometheus type:prometheus url:http://prometheus-kube-prometheus-prometheus:9090] map[isDefault:false jsonData:map[derivedFields:[map[datasourceUid:tempo_uid matcherRegex:trace_id=(\\w+) name:trace_id url:$${__value.raw}]] maxLines:1000] name:Loki type:loki uid:loki_uid url:http://loki:3100] map[isDefault:false name:Tempo type:tempo uid:tempo_uid url:http://tempo:3100]]]] sidecar:map[dashboards:map[enabled:true]]]"
2022-04-02T08:39:56.593+0100 [DEBUG] Validate chart: ref=grafana
2022-04-02T08:39:56.593+0100 [DEBUG] Run chart: ref=grafana
2022-04-02T08:39:56.918+0100 [ERROR] W0402 08:39:56.918085   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:56.920+0100 [ERROR] W0402 08:39:56.920379   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:56.949+0100 [ERROR] 2022-04-02T08:39:56.949+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 1 resource(s)"
2022-04-02T08:39:56.968+0100 [ERROR] 2022-04-02T08:39:56.967+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 15 resource(s)"
2022-04-02T08:39:56.972+0100 [ERROR] W0402 08:39:56.971941   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:56.972+0100 [ERROR] W0402 08:39:56.972048   15802 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-02T08:39:57.059+0100 [ERROR] 2022-04-02T08:39:57.059+0100 [INFO]  Generating template: ref=monitor_ingress_gateway output=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:39:57.059+0100 [DEBUG] Template content: ref=monitor_ingress_gateway
  source=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: #{{ .Vars.consul_namespace }}
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:39:57.059+0100 [ERROR] 2022-04-02T08:39:57.059+0100 [DEBUG] Template output: ref=monitor_ingress_gateway
  destination=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: monitoring
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: consul
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-02T08:39:57.059+0100 [ERROR] 2022-04-02T08:39:57.059+0100 [INFO]  Applying Kubernetes configuration: ref=monitor_ingress_gateway config=["/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml"]
2022-04-02T08:39:57.060+0100 [ERROR] 2022-04-02T08:39:57.060+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [INFO]  Applying Kubernetes configuration: ref=application config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/../../example/kubernetes/"]
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [INFO]  Applying Kubernetes configuration: ref=upstreams-proxy config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml"]
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [INFO]  Creating Helm chart: ref=consul-release-controller
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [DEBUG] Using Kubernetes config: ref=consul-release-controller path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml
2022-04-02T08:39:57.150+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/api.yaml
2022-04-02T08:39:57.150+0100 [ERROR] 2022-04-02T08:39:57.150+0100 [DEBUG] Creating chart from config: ref=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:39:57.150+0100 [DEBUG] Loading chart: ref=consul-release-controller path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-02T08:39:57.151+0100 [ERROR] 2022-04-02T08:39:57.151+0100 [DEBUG] Using Values: ref=consul-release-controller values="map[acls:map[enabled:true] autoencrypt:map[enabled:true] controller:map[container_config:map[image:map[repository:nicholasjackson/consul-release-controller tag:]] enabled:false] webhook:map[namespace:shipyard service:controller-webhook]]"
2022-04-02T08:39:57.151+0100 [DEBUG] Validate chart: ref=consul-release-controller
2022-04-02T08:39:57.151+0100 [DEBUG] Run chart: ref=consul-release-controller
2022-04-02T08:39:57.255+0100 [ERROR] 2022-04-02T08:39:57.255+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/application-dashboard.yaml
2022-04-02T08:39:57.272+0100 [ERROR] 2022-04-02T08:39:57.272+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/consul-config.yaml
2022-04-02T08:39:57.358+0100 [ERROR] 2022-04-02T08:39:57.358+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest-dashboard.yaml
2022-04-02T08:39:57.378+0100 [ERROR] 2022-04-02T08:39:57.378+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest.yaml
2022-04-02T08:39:57.424+0100 [ERROR] 2022-04-02T08:39:57.424+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/metrics.yaml
2022-04-02T08:39:57.451+0100 [ERROR] 2022-04-02T08:39:57.450+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/web.yaml
2022-04-02T08:39:57.527+0100 [ERROR] 2022-04-02T08:39:57.527+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 1 resource(s)"
2022-04-02T08:39:57.539+0100 [ERROR] 2022-04-02T08:39:57.539+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 13 resource(s)"
2022-04-02T08:39:57.680+0100 [ERROR] 2022-04-02T08:39:57.680+0100 [INFO]  Remote executing command: ref=exec_standalone command=sh args=["/output/fetch_certs.sh"] image="&{shipyardrun/tools:v0.6.0  }"
2022-04-02T08:39:57.699+0100 [ERROR] 2022-04-02T08:39:57.698+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.6.0
2022-04-02T08:39:57.699+0100 [DEBUG] Creating Docker Container: ref=exec_standalone.remote_exec
2022-04-02T08:39:57.770+0100 [ERROR] 2022-04-02T08:39:57.770+0100 [DEBUG] Remove container from default networks: ref=exec_standalone.remote_exec
2022-04-02T08:39:57.774+0100 [ERROR] 2022-04-02T08:39:57.774+0100 [DEBUG] Attaching container to network: ref=b348885b4e875314e67b40934ba881759666f2cfd09ed8c2913726cee9b97278 network=dc1
2022-04-02T08:39:57.783+0100 [ERROR] 2022-04-02T08:39:57.783+0100 [DEBUG] Disconnectng network: name=bridge ref=exec_standalone.remote_exec
2022-04-02T08:39:58.929+0100 [ERROR] 2022-04-02T08:39:58.929+0100 [DEBUG] Forcefully remove: container=b348885b4e875314e67b40934ba881759666f2cfd09ed8c2913726cee9b97278
2022-04-02T08:39:59.637+0100 [ERROR] 2022-04-02T08:39:59.637+0100 [DEBUG] Health check urls for browser windows: count=0
2022-04-02T08:39:59.637+0100 [DEBUG] Browser windows open

########################################################

Title Development setup
Author Nic Jackson
2022-04-02T08:39:59.637+0100 [ERROR] 
• Consul: https://localhost:8501
• Grafana: https://localhost:8080
• Application: http://localhost:18080

This blueprint defines 13 output variables.

You can set output variables as environment variables for your current terminal session using the following command:

eval $(shipyard env)

To list output variables use the command:

shipyard output
2022-04-02T08:40:00.249+0100 [INFO]  Starting controller
2022-04-02T08:40:04.825+0100 [DEBUG] statemachine: Handle event: event=event_configure state=state_start
2022-04-02T08:40:04.825+0100 [DEBUG] statemachine: Log state: event=event_configure state=state_start
2022-04-02T08:40:04.825+0100 [DEBUG] statemachine: Configure: state=state_configure
2022-04-02T08:40:04.825+0100 [DEBUG] statemachine: Log state: event=event_configure release=api state=state_configure
2022-04-02T08:40:04.825+0100 [INFO]  releaser-plugin-consul: Initializing deployment: service=api
2022-04-02T08:40:04.825+0100 [DEBUG] releaser-plugin-consul: Create service defaults: service=api
2022-04-02T08:40:04.854+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-02T08:40:04.854+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_configure
2022-04-02T08:40:04.854+0100 [DEBUG] kubernetes-webhook: Reject deployment, there is currently an active release for this deployment: name=api-deployment namespace=default state=state_configure
2022-04-02T08:40:04.856+0100 [INFO]  Shutting down server gracefully
2022-04-02T08:40:04.857+0100 [INFO]  Shutting down listener
2022-04-02T08:40:04.857+0100 [INFO]  Shutting down metrics
2022-04-02T08:40:04.858+0100 [INFO]  Shutting down kubernetes controller
2022-04-02T08:40:04.858+0100 [INFO]  kubernetes-controller: Stopping Kubernetes controller
2022-04-02T08:40:05.834+0100 [ERROR] releaser-plugin-consul: Unable to create Consul ServiceDefaults: name=consul-release-controller error="Get \"https://127.0.0.1:8501/v1/config/service-defaults/consul-release-controller\": dial tcp 127.0.0.1:8501: connect: connection refused"
2022-04-02T08:40:05.834+0100 [ERROR] statemachine: Configure completed with error: error="Get \"https://127.0.0.1:8501/v1/config/service-defaults/consul-release-controller\": dial tcp 127.0.0.1:8501: connect: connection refused"
2022-04-02T08:40:05.834+0100 [DEBUG] statemachine: Handle event: event=event_fail state=state_configure
2022-04-02T08:40:05.834+0100 [DEBUG] statemachine: Log state: event=event_fail state=state_configure
2022-04-02T08:40:05.834+0100 [DEBUG] statemachine: Log state: event=event_fail release=api state=state_fail
